source_action,Comment_URL,Comment_HTML_URL,Review_Start_Line,Review_End_Line,Original_Commit_id,Merge_Commit_id,Diff_path,New_path,Body,Diff_hunk,Change_Until_Merged,Whether it contain issues or suggestions (Not Contain: 0; Contain: 1),List of issues or suggestions,Addressed Status Classification (Not Addressed: 0; Partly Addressed: 1; Fully Addressed: 2),Detail
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1210854669,https://github.com/pokt-network/pocket/pull/778#discussion_r1210854669,,209,3f7c8f63ff1d05e64f8672113a70545632501fca,a516c51f2d000d25f1469d28e6586ef7875d5a37,app/client/cli/servicer.go,nan,"I suggest changing
 ```
+	reqBytes, err := json.Marshal(relay)
```
 to
```
+	// TECHDEBT: Evaluate which fields we should and shouldn't marshall when signing the payload
+	reqBytes, err := json.Marshal(relay)
```","+		Meta:    relayMeta,
+		// DISCUSS: why is there no Proof field in v1 struct?
+	}
+	reqBytes, err := json.Marshal(relay)","--- 

+++ 

@@ -9,6 +9,7 @@

 
 	""github.com/spf13/cobra""
 
+	""github.com/pokt-network/pocket/app/client/cli/flags""
 	""github.com/pokt-network/pocket/rpc""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
@@ -43,7 +44,7 @@

 		newUnstakeCmd(cmdDef),
 		newUnpauseCmd(cmdDef),
 		{
-			// IMPROVE: allow reading the relay payload from a file
+			// IMPROVE: allow reading the relay payload from a file with the serialized protobuf via [--input_file]
 			Use:   ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
 			Short: ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
 			Long: `Sends a trustless relay using <relayPayload> as contents, to the specified active <servicerAddrHex> in the the <applicationAddrHex>'s session.
@@ -60,7 +61,7 @@

 				//	The client will: send the trustless relay and return the response (using a single function as entrypoint)
 				//	The CLI will:
 				//		1) extract the required input from the command arguments
-				//		2) call the client function that performs the trustless relay
+				//		2) call the client function (with the inputs above) that performs the trustless relay
 				pk, err := getPrivateKeyFromKeybase(applicationAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting application's private key: %w"", err)
@@ -72,7 +73,7 @@

 					return fmt.Errorf(""Error getting current session: %w"", err)
 				}
 
-				servicer, err := validateServicer(cmd.Context(), session, servicerAddr)
+				servicer, err := validateServicer(session, servicerAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting servicer for the relay: %w"", err)
 				}
@@ -82,7 +83,7 @@

 					return fmt.Errorf(""error building relay from payload: %w"", err)
 				}
 
-				fmt.Printf(""sending trustless relay for %s to %v with payload: %s\n"", applicationAddr, servicer, relayPayload)
+				fmt.Printf(""sending trustless relay for %s to %s with payload: %s\n"", applicationAddr, servicerAddr, relayPayload)
 
 				resp, err := sendTrustlessRelay(cmd.Context(), servicer.ServiceUrl, relay)
 				if err != nil {
@@ -100,13 +101,13 @@

 	return cmds
 }
 
-// DECIDE: do we need/want a cli subcommand for fetching sessions?
+// TODO: add a cli command for fetching sessions
 // validateServicer returns the servicer specified by the <servicer> argument.
-// It validates that the <servicer> is the address of a servicer that is active in the passed session.
-func validateServicer(ctx context.Context, session *rpc.Session, servicerAddress string) (*rpc.ProtocolActor, error) {
-	for _, s := range session.Servicers {
-		if s.Address == servicerAddress {
-			return &s, nil
+// It validates that the <servicer> is the address of a servicer that is active in the current session.
+func validateServicer(session *rpc.Session, servicerAddress string) (*rpc.ProtocolActor, error) {
+	for i := range session.Servicers {
+		if session.Servicers[i].Address == servicerAddress {
+			return &session.Servicers[i], nil
 		}
 	}
 
@@ -128,7 +129,7 @@

 		SessionHeight: currentHeight,
 	}
 
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session for app/chain/height: %s/%s/%d: %w"", appAddress, chain, currentHeight, err)
 	}
@@ -137,7 +138,8 @@

 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session with request %v: %w"", req, err)
 	}
-	// TODO: refactor boiler-plate code
+
+	// CLEANUP: move the HTTP response processing code to a separate function to enable reuse.
 	if resp.HTTPResponse.StatusCode != http.StatusOK {
 		return nil, fmt.Errorf(""Error getting current session: Unexpected status code %d for request %v"", resp.HTTPResponse.StatusCode, req)
 	}
@@ -149,9 +151,9 @@

 	return resp.JSON200, nil
 }
 
-// TODO: reuse this function in the query commands
+// REFACTOR: reuse this function in all the query commands
 func getCurrentHeight(ctx context.Context) (int64, error) {
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return 0, fmt.Errorf(""Error getting current height: %w"", err)
 	}
@@ -170,8 +172,8 @@

 	return resp.JSON200.Height, nil
 }
 
-// IMPROVE: [K8s][LocalNet] Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
-// REFACTOR: move package-level variables (e.g. remoteCLIURL) to a cli object.
+// IMPROVE(#823): [K8s][LocalNet] Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
+// CONSIDERATION: move package-level variables (e.g. RemoteCLIURL) to a cli object and consider storing it in the context
 func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay *rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
 	client, err := rpc.NewClientWithResponses(servicerUrl)
 	if err != nil {
@@ -184,16 +186,15 @@

 func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session *rpc.Session, servicer *rpc.ProtocolActor) (*rpc.RelayRequest, error) {
 	// TECHDEBT: This is mostly COPIED from pocket-go: we should refactor pocket-go code and import this functionality from there instead.
 	relayPayload := rpc.Payload{
-		// INCOMPLETE: need to unmarshal into JSONRPC (and other supported relay formats), using a custom unmarshaller
+		// INCOMPLETE(#803): need to unmarshal into JSONRPC and other supported relay formats once proto-generated custom types are added.
 		Jsonrpc: ""2.0"",
 		Method:  payload,
-		// TODO: Path: load Path field from the corresponding Blockchain (e.g. database)
-		// TODO: set Headers
+		// INCOMPLETE: set Headers for HTTP relays
 	}
 
 	relayMeta := rpc.RelayRequestMeta{
 		BlockHeight: session.SessionHeight,
-		// TODO: use Identifiable for Chain in Session (or string for Chain in Relay Meta)
+		// TODO: Make Chain Identifier type consistent in Session and Meta use Identifiable for Chain in Session (or string for Chain in Relay Meta)
 		Chain: rpc.Identifiable{
 			Id: session.Chain,
 		},
@@ -204,8 +205,8 @@

 	relay := &rpc.RelayRequest{
 		Payload: relayPayload,
 		Meta:    relayMeta,
-		// DISCUSS: why is there no Proof field in v1 struct?
-	}
+	}
+	// TECHDEBT: Evaluate which fields we should and shouldn't marshall when signing the payload
 	reqBytes, err := json.Marshal(relay)
 	if err != nil {
 		return nil, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
@@ -220,14 +221,14 @@

 	return relay, nil
 }
 
-// TECHDEBT: remove use of package-level variables
+// TECHDEBT: remove use of package-level variables, such as NonInteractive, RemoteCLIURL, pwd, etc.
 func getPrivateKeyFromKeybase(address string) (crypto.PrivateKey, error) {
 	kb, err := keybaseForCLI()
 	if err != nil {
 		return nil, err
 	}
 
-	if !nonInteractive {
+	if !flags.NonInteractive {
 		pwd = readPassphrase(pwd)
 	}
 ",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/QuickSwap/interface-v2/pulls/comments/1723026159,https://github.com/QuickSwap/interface-v2/pull/1504#discussion_r1723026159,,1790,d2c0a5ef57dc153a4ec110127636bbb5a2aaa709,83c269b439febb5297d9f29688695434245996a4,src/constants/v3/addresses.ts,nan,@brianshattuck this is not WETH but MATIC on zkevm,"+  137: '0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270', // Polygon Mainnet (WMATIC)
+  80001: '0x9c3c9283d3e44854697cd22d3faa240cfb032889', // Mumbai Testnet (WMATIC)
+  43114: '0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7', // Avalanche Mainnet (WAVAX)
+  1101: '0xa2036f0538221a77A3937F1379699f44945018d0', // Polygon zkEVM Mainnet (WETH)","--- 

+++ 

@@ -1627,7 +1627,7 @@

     //USDC[ChainId.MATIC],
     //USDT[ChainId.MATIC],
     //OLD_QUICK[ChainId.MATIC],
-    //NEW_QUICK[ChainId.MATIC],
+    NEW_QUICK[ChainId.MATIC],
     ETHER[ChainId.MATIC],
     //WBTC[ChainId.MATIC],
     USDCE[ChainId.MATIC],
@@ -1776,33 +1776,42 @@

   }
 }
 
-export const wrappedTokenAddresses = {
-  1: '0xC02aaa39b223FE8D0A0e5C4F27eAD9083C756Cc2', // Ethereum Mainnet (WETH)
-  3: '0xc778417E063141139Fce010982780140Aa0cD5Ab', // Ropsten Testnet (WETH)
-  4: '0xc778417E063141139Fce010982780140Aa0cD5Ab', // Rinkeby Testnet (WETH)
-  5: '0xc778417E063141139Fce010982780140Aa0cD5Ab', // Goerli Testnet (WETH)
-  42: '0xd0A1E359811322d97991E03f863a0C30C2cF029C', // Kovan Testnet (WETH)
-  56: '0xBB4CdB9CBd36B01bD1cBaEBF2De08d9173bc095c', // Binance Smart Chain Mainnet (WBNB)
-  97: '0xae13d989dac2f0debff460ac112a837c89baa7cd', // Binance Smart Chain Testnet (WBNB)
-  137: '0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270', // Polygon Mainnet (WMATIC)
-  80001: '0x9c3c9283d3e44854697cd22d3faa240cfb032889', // Mumbai Testnet (WMATIC)
-  43114: '0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7', // Avalanche Mainnet (WAVAX)
-  1101: '0xa2036f0538221a77A3937F1379699f44945018d0', // Polygon zkEVM Mainnet (WETH)
-  344: '0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7', // Manta Pacific L2 Rollup (WETH)
-  132: '0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7', // Immutable zkEVM Mainnet (WETH)
-  592: '0x9fB83c0635De2E815fd1c21b3a292277540C2e8d', // Astar zkEVM (WASTR)
-  2000: '0x5c21a9226A2E5cd2c30bDB3A5D53E1c2325Ff55a', // DogeChain (wDOGE)
-  2010: '0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7', // X Layer (WETH)
-  2222: '0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', // Kava - Kinetix (WKAVA)
-};
-
-export const nativeTokenSymbols = {
-  137: 'MATIC', // Polygon Mainnet
-  1101: 'ETH', // Polygon zkEVM
-  344: 'MANTA', // Manta Pacific L2 Rollup (Assumed)
-  132: 'IMX', // Immutable zkEVM Mainnet (Assumed)
-  592: 'ASTR', // Astar zkEVM
-  2000: 'DOGE', // DogeChain
-  2010: 'XLAYER', // X Layer (Assumed)
-  2222: 'KAVA', // Kava - Kinetix
-};
+export const wrappedTokenAddresses: {
+  readonly [chainId in ChainId]: string;
+} = {
+  [ChainId.MATIC]: WETH[ChainId.MATIC].address,
+  [ChainId.ZKEVM]: WETH[ChainId.ZKEVM].address,
+  [ChainId.MANTA]: WETH[ChainId.MANTA].address,
+  [ChainId.KAVA]: WETH[ChainId.KAVA].address,
+  [ChainId.IMX]: WETH[ChainId.IMX].address,
+  [ChainId.ASTARZKEVM]: WETH[ChainId.ASTARZKEVM].address,
+  [ChainId.DOGECHAIN]: WETH[ChainId.DOGECHAIN].address,
+  [ChainId.LAYERX]: WETH[ChainId.LAYERX].address,
+  [ChainId.ZKATANA]: WETH[ChainId.ZKATANA].address,
+  [ChainId.BTTC]: WETH[ChainId.BTTC].address,
+  [ChainId.X1]: WETH[ChainId.X1].address,
+  [ChainId.TIMX]: WETH[ChainId.TIMX].address,
+  [ChainId.ZKTESTNET]: WETH[ChainId.ZKTESTNET].address,
+  [ChainId.MUMBAI]: WETH[ChainId.MUMBAI].address,
+  [ChainId.DOEGCHAIN_TESTNET]: WETH[ChainId.DOEGCHAIN_TESTNET].address,
+};
+
+export const nativeTokenSymbols: {
+  readonly [chainId in ChainId]: string | undefined;
+} = {
+  [ChainId.MATIC]: 'MATIC',
+  [ChainId.ZKEVM]: 'ETH',
+  [ChainId.MANTA]: 'ETH',
+  [ChainId.KAVA]: 'KAVA',
+  [ChainId.IMX]: 'IMX',
+  [ChainId.ASTARZKEVM]: 'ETH',
+  [ChainId.DOGECHAIN]: 'DOGE',
+  [ChainId.LAYERX]: 'OKB',
+  [ChainId.ZKATANA]: 'ETH',
+  [ChainId.BTTC]: 'BTT',
+  [ChainId.X1]: 'OKB',
+  [ChainId.TIMX]: 'IMX',
+  [ChainId.ZKTESTNET]: 'ETH',
+  [ChainId.MUMBAI]: 'MATIC',
+  [ChainId.DOEGCHAIN_TESTNET]: 'DOGE',
+};",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1181998181,https://github.com/pokt-network/pocket/pull/684#discussion_r1181998181,,51,964333fcec0ccb79708af5ce07dbab1cf5efb3a5,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/handlers_query.go,nan,Move `upokt` into a constant and use everywhere,"+
+	return ctx.JSON(http.StatusOK, Account{
+		Address: body.Address,
+		Coins:   []Coin{{Amount: amount, Denom: ""upokt""}},","--- 

+++ 

@@ -14,23 +14,21 @@

 	""github.com/pokt-network/pocket/shared/utils""
 )
 
-// Queries
+// This file contains the handlers for the v1/query path in the RPC specification
+// It pertains to all the user facing/public RPC endpoints to query the pocket
+// network, its actors and state.
+
+const (
+	denom = ""upokt""
+)
 
 func (s *rpcServer) PostV1QueryAccount(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -48,7 +46,7 @@

 
 	return ctx.JSON(http.StatusOK, Account{
 		Address: body.Address,
-		Coins:   []Coin{{Amount: amount, Denom: ""upokt""}},
+		Coins:   []Coin{{Amount: amount, Denom: denom}},
 	})
 }
 
@@ -58,15 +56,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -90,7 +80,7 @@

 	for _, account := range allAccounts[start : end+1] {
 		accounts = append(accounts, Account{
 			Address: account.Address,
-			Coins:   []Coin{{Amount: account.Amount, Denom: ""upokt""}},
+			Coins:   []Coin{{Amount: account.Amount, Denom: denom}},
 		})
 	}
 
@@ -102,19 +92,19 @@

 }
 
 func (s *rpcServer) PostV1QueryAccountTxs(ctx echo.Context) error {
-	var body QueryAddressPaginated
+	var body QueryAccountPaginated
 	if err := ctx.Bind(&body); err != nil {
 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 	sortDesc := checkSortDesc(*body.Sort)
 
 	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResults, err := txIndexer.GetBySender(body.Address, sortDesc)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(txResults), int(body.Page), int(body.PerPage))
+	idxTxs, err := txIndexer.GetBySender(body.Address, sortDesc)
+	if err != nil {
+		return ctx.String(http.StatusInternalServerError, err.Error())
+	}
+
+	start, end, totalPages, err := getPageIndexes(len(idxTxs), int(body.Page), int(body.PerPage))
 	if err != nil && !errors.Is(err, errNoItems) {
 		return ctx.String(http.StatusBadRequest, err.Error())
 	}
@@ -122,9 +112,9 @@

 		return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{})
 	}
 
-	pageTxs := make([]Transaction, 0)
-	for _, txResult := range txResults[start : end+1] {
-		rpcTx, err := s.txResultToRPCTransaction(txResult)
+	pageTxs := make([]IndexedTransaction, 0)
+	for _, idxTx := range idxTxs[start : end+1] {
+		rpcTx, err := s.idxTxToRPCIdxTx(idxTx)
 		if err != nil {
 			return ctx.String(http.StatusInternalServerError, err.Error())
 		}
@@ -134,52 +124,40 @@

 	return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{
 		Txs:        pageTxs,
 		Page:       body.Page,
-		TotalTxs:   int64(len(txResults)),
+		TotalTxs:   int64(len(idxTxs)),
 		TotalPages: int64(totalPages),
 	})
 }
 
 func (s *rpcServer) GetV1QueryAllChainParams(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
-
-	paramSlice, err := readCtx.GetAllParams()
+	height := s.getQueryHeight(0)
+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
+	if err != nil {
+		return ctx.String(http.StatusInternalServerError, err.Error())
+	}
+	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
+
+	paramsSlice, err := readCtx.GetAllParams()
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
 	resp := make([]Parameter, 0)
-	for i := 0; i < len(paramSlice); i++ {
+	for i := 0; i < len(paramsSlice); i++ {
 		resp = append(resp, Parameter{
-			ParameterName:  paramSlice[i][0],
-			ParameterValue: paramSlice[i][1],
+			ParameterName:  paramsSlice[i][0],
+			ParameterValue: paramsSlice[i][1],
 		})
 	}
 	return ctx.JSON(200, resp)
 }
 
 func (s *rpcServer) PostV1QueryApp(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -205,15 +183,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -233,11 +203,7 @@

 		return ctx.JSON(http.StatusOK, QueryAppsResponse{})
 	}
 
-	rpcApps := make([]ProtocolActor, 0)
-	for _, app := range allApps[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(app)
-		rpcApps = append(rpcApps, actor)
-	}
+	rpcApps := protocolActorToRPCProtocolActors(allApps[start : end+1])
 
 	return ctx.JSON(http.StatusOK, QueryAppsResponse{
 		Apps:       rpcApps,
@@ -248,20 +214,12 @@

 }
 
 func (s *rpcServer) PostV1QueryBalance(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -292,16 +250,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := uint64(body.Height)
-	if height == 0 {
-		currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
-
+	height := uint64(s.getQueryHeight(body.Height))
 	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
 	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
 	if err != nil {
@@ -327,16 +276,7 @@

 	}
 	sortDesc := checkSortDesc(*body.Sort)
 
-	// Get latest stored block height
-	height := uint64(body.Height)
-	if height == 0 {
-		currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
-
+	height := uint64(s.getQueryHeight(body.Height))
 	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
 	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
 	if err != nil {
@@ -375,20 +315,12 @@

 }
 
 func (s *rpcServer) PostV1QueryFisherman(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -414,15 +346,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -442,11 +366,7 @@

 		return ctx.JSON(http.StatusOK, QueryFishermenResponse{})
 	}
 
-	rpcFishermen := make([]ProtocolActor, 0)
-	for _, fisher := range allFishermen[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(fisher)
-		rpcFishermen = append(rpcFishermen, actor)
-	}
+	rpcFishermen := protocolActorToRPCProtocolActors(allFishermen[start : end+1])
 
 	return ctx.JSON(http.StatusOK, QueryFishermenResponse{
 		Fishermen:      rpcFishermen,
@@ -457,14 +377,8 @@

 }
 
 func (s *rpcServer) GetV1QueryHeight(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-
 	return ctx.JSON(http.StatusOK, QueryHeight{
-		Height: int64(currentHeight),
+		Height: s.getQueryHeight(0),
 	})
 }
 
@@ -474,16 +388,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -502,20 +407,12 @@

 }
 
 func (s *rpcServer) PostV1QueryServicer(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -541,15 +438,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -569,11 +458,7 @@

 		return ctx.JSON(http.StatusOK, QueryServicersResponse{})
 	}
 
-	rpcServicers := make([]ProtocolActor, 0)
-	for _, servicer := range allServicers[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(servicer)
-		rpcServicers = append(rpcServicers, actor)
-	}
+	rpcServicers := protocolActorToRPCProtocolActors(allServicers[start : end+1])
 
 	return ctx.JSON(http.StatusOK, QueryServicersResponse{
 		Servicers:      rpcServicers,
@@ -589,15 +474,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -622,7 +499,7 @@

 			Address: pool.Address,
 			Name:    name,
 			Amount:  pool.Amount,
-			Denom:   ""upokt"",
+			Denom:   denom,
 		})
 	}
 
@@ -630,7 +507,7 @@

 		Pools: rpcPools,
 		Total: Coin{
 			Amount: total.String(),
-			Denom:  ""upokt"",
+			Denom:  denom,
 		},
 	})
 }
@@ -641,15 +518,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -677,12 +546,12 @@

 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
 	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResult, err := txIndexer.GetByHash(hashBz)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	rpcTx, err := s.txResultToRPCTransaction(txResult)
+	idxTx, err := txIndexer.GetByHash(hashBz)
+	if err != nil {
+		return ctx.String(http.StatusInternalServerError, err.Error())
+	}
+
+	rpcTx, err := s.idxTxToRPCIdxTx(idxTx)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -702,7 +571,7 @@

 		return ctx.String(http.StatusBadRequest, fmt.Sprintf(""hash not found in mempool: %s"", body.Hash))
 	}
 
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions([][]byte{uncTx})
+	rpcUncTxs, err := s.txProtoBytesToRPCIdxTxs([][]byte{uncTx})
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -727,7 +596,7 @@

 		return ctx.JSON(http.StatusOK, QueryTxsResponse{})
 	}
 
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions(uncTxs[start : end+1])
+	rpcUncTxs, err := s.txProtoBytesToRPCIdxTxs(uncTxs[start : end+1])
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -746,15 +615,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	reatCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -772,20 +633,12 @@

 }
 
 func (s *rpcServer) PostV1QueryValidator(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	var body QueryAccountHeight
+	if err := ctx.Bind(&body); err != nil {
+		return ctx.String(http.StatusBadRequest, ""bad request"")
+	}
+
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -811,15 +664,7 @@

 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
 
-	// Get latest stored block height
-	height := body.Height
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		height = currentHeight
-	}
+	height := s.getQueryHeight(body.Height)
 	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
@@ -839,11 +684,7 @@

 		return ctx.JSON(http.StatusOK, QueryValidatorsResponse{})
 	}
 
-	rpcValidators := make([]ProtocolActor, 0)
-	for _, val := range allValidators[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(val)
-		rpcValidators = append(rpcValidators, actor)
-	}
+	rpcValidators := protocolActorToRPCProtocolActors(allValidators[start : end+1])
 
 	return ctx.JSON(http.StatusOK, QueryValidatorsResponse{
 		Validators:      rpcValidators,",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828153409,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1828153409,,246,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,nan,"I suggest changing
 ```
+    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
```
 to
```
+    """"""Compute the distance between the position and the target.
+    
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
```"," 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    assert len(target) == 2
+    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1171827832,https://github.com/pokt-network/pocket/pull/683#discussion_r1171827832,,191,cc15ba1ea64bab29113ec8247bc952b2cc657856,fb245bc027ae55aaee936daf9f85ecb2d9ad8b40,utility/session.go,nan,"""number of _fishermen_""","-	return nil, nil
+// hydrateSessionFishermen finds the fishermen that are staked at the session height and populates the session with them
+func (s *sessionHydrator) hydrateSessionFishermen() error {
+	// number of fisherman per session at this height","--- 

+++ 

@@ -7,32 +7,17 @@

 	""math""
 	""math/rand""
 
+	""golang.org/x/exp/slices""
+
 	""github.com/pokt-network/pocket/logger""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/utility/types""
-	""golang.org/x/exp/slices""
 )
 
-// sessionHydrator is an internal structure used to prepare a Session returned by `GetSession` below
-type sessionHydrator struct {
-	logger modules.Logger
-
-	// The height of the request for which the session is being hydrated
-	blockHeight int64
-
-	// The session being hydrated and returned
-	session *coreTypes.Session
-
-	// Caches a readCtx to avoid draining too many connections to the database
-	readCtx modules.PersistenceReadContext
-
-	// A redundant helper that maintains a hex decoded copy of `session.Id` used for session hydration
-	sessionIdBz []byte
-}
-
-// GetSession is an implementation of the exposed `UtilityModule.GetSession` function
+// GetSession implements of the exposed `UtilityModule.GetSession` function
+// TECHDEBT(#519): Add custom error types depending on the type of issue that occurred and assert on them in the unit tests.
 func (m *utilityModule) GetSession(appAddr string, height int64, relayChain, geoZone string) (*coreTypes.Session, error) {
 	persistenceModule := m.GetBus().GetPersistenceModule()
 	readCtx, err := persistenceModule.NewReadContext(height)
@@ -48,56 +33,71 @@

 
 	sessionHydrator := &sessionHydrator{
 		logger:      m.logger.With().Str(""source"", ""sessionHydrator"").Logger(),
+		session:     session,
 		blockHeight: height,
-		session:     session,
 		readCtx:     readCtx,
 	}
 
-	if err := sessionHydrator.hydrateSessionHeight(height); err != nil {
-		return nil, err
+	if err := sessionHydrator.hydrateSessionMetadata(); err != nil {
+		return nil, fmt.Errorf(""failed to hydrate session metadata: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionApplication(appAddr); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session application: %w"", err)
 	}
 
 	if err := sessionHydrator.validateApplicationSession(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to validate application session: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionID(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session ID: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionServicers(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session servicers: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionFishermen(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session fishermen: %w"", err)
 	}
 
 	return sessionHydrator.session, nil
 }
 
-// hydrateSessionHeight hydrates the height at which the session started given the current block height
-func (s *sessionHydrator) hydrateSessionHeight(blockHeight int64) error {
-	numBlocksPerSession, err := s.readCtx.GetIntParam(types.BlocksPerSessionParamName, blockHeight)
-	if err != nil {
-		return err
-	}
-	numBlocksAheadOfSession := blockHeight % int64(numBlocksPerSession)
+type sessionHydrator struct {
+	logger modules.Logger
+
+	// The session being hydrated and returned
+	session *coreTypes.Session
+
+	// The height at which the request is being made to get session information
+	blockHeight int64
+
+	// Caches a readCtx to avoid draining too many connections to the database
+	readCtx modules.PersistenceReadContext
+
+	// A redundant helper that maintains a hex decoded copy of `session.Id` used for session hydration
+	sessionIdBz []byte
+}
+
+// hydrateSessionMetadata hydrates the height at which the session started, its number, and the number of blocks per session
+func (s *sessionHydrator) hydrateSessionMetadata() error {
+	numBlocksPerSession, err := s.readCtx.GetIntParam(types.BlocksPerSessionParamName, s.blockHeight)
+	if err != nil {
+		return err
+	}
+	numBlocksAheadOfSession := s.blockHeight % int64(numBlocksPerSession)
 
 	s.session.NumSessionBlocks = int64(numBlocksPerSession)
-	s.session.SessionNumber = int64(blockHeight / int64(numBlocksPerSession))
-	s.session.SessionHeight = blockHeight - numBlocksAheadOfSession
-	return nil
-}
-
-// hydrateSessionApplication hydrates the full Application actor based on the address the session is being
-// dispatched for.
+	s.session.SessionNumber = int64(s.blockHeight / int64(numBlocksPerSession))
+	s.session.SessionHeight = s.blockHeight - numBlocksAheadOfSession
+	return nil
+}
+
+// hydrateSessionApplication hydrates the full Application actor based on the address provided
 func (s *sessionHydrator) hydrateSessionApplication(appAddr string) error {
-	// TECHDEBT: We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
+	// TECHDEBT(#706): We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
 	addr, err := hex.DecodeString(appAddr)
 	if err != nil {
 		return err
@@ -106,20 +106,22 @@

 	return err
 }
 
-// validateApplicationSession validates that the application can dispatch a session at the requested geo zone and for the request relay chain
+// validateApplicationSession validates that the application can have a valid session for the provided relay chain and geo zone
 func (s *sessionHydrator) validateApplicationSession() error {
-	// TODO(#XXX): Filter by geo-zone
 	app := s.session.Application
 
 	if !slices.Contains(app.Chains, s.session.RelayChain) {
 		return fmt.Errorf(""application %s does not stake for relay chain %s"", app.Address, s.session.RelayChain)
 	}
 
-	if !(app.PausedHeight == -1 && app.UnstakingHeight == -1) {
+	if app.PausedHeight != -1 || app.UnstakingHeight != -1 {
 		return fmt.Errorf(""application %s is either unstaked or paused"", app.Address)
 	}
 
-	// TODO: Consider what else we should validate for here
+	// TODO(#697): Filter by geo-zone
+
+	// INVESTIGATE: Consider what else we should validate for here (e.g. Application stake amount, etc.)
+
 	return nil
 }
 
@@ -135,6 +137,9 @@

 	}
 	prevHashBz, err := hex.DecodeString(prevHash)
 
+	if err != nil {
+		return err
+	}
 	appPubKeyBz := []byte(s.session.Application.PublicKey)
 	relayChainBz := []byte(string(s.session.RelayChain))
 	geoZoneBz := []byte(s.session.GeoZone)
@@ -163,21 +168,14 @@

 	candidateServicers := make([]*coreTypes.Actor, 0)
 	for _, servicer := range servicers {
 		// Sanity check the servicer is not paused, jailed or unstaking
-		if !(servicer.PausedHeight == -1 && servicer.UnstakingHeight == -1) {
+		if servicer.PausedHeight != -1 || servicer.UnstakingHeight != -1 {
 			return fmt.Errorf(""hydrateSessionServicers should not have encountered a paused or unstaking servicer: %s"", servicer.Address)
 		}
 
-		// TODO(#XXX): Filter by geo-zone
-
-		// OPTIMIZE: If this was a map[string]struct{}, we could have avoided the loop
-		var chain string
-		for _, chain = range servicer.Chains {
-			if chain != s.session.RelayChain {
-				chain = """"
-				continue
-			}
-		}
-		if chain != """" {
+		// TECHDEBT(#697): Filter by geo-zone
+
+		// OPTIMIZE: If `servicer.Chains` was a map[string]struct{}, we could eliminate `slices.Contains()`'s loop
+		if slices.Contains(servicer.Chains, s.session.RelayChain) {
 			candidateServicers = append(candidateServicers, servicer)
 		}
 	}
@@ -188,13 +186,13 @@

 
 // hydrateSessionFishermen finds the fishermen that are staked at the session height and populates the session with them
 func (s *sessionHydrator) hydrateSessionFishermen() error {
-	// number of fisherman per session at this height
+	// number of fishermen per session at this height
 	numFishermen, err := s.readCtx.GetIntParam(types.FishermanPerSessionParamName, s.session.SessionHeight)
 	if err != nil {
 		return err
 	}
 
-	// returns all the staked fisherman at this session height
+	// returns all the staked fishermen at this session height
 	fishermen, err := s.readCtx.GetAllFishermen(s.session.SessionHeight)
 	if err != nil {
 		return err
@@ -202,24 +200,17 @@

 
 	// OPTIMIZE: Consider updating the persistence module so a single SQL query can retrieve all of the actors at once.
 	candidateFishermen := make([]*coreTypes.Actor, 0)
-	for _, fisherman := range fishermen {
-		// Sanity check the fisherman is not paused, jailed or unstaking
-		if !(fisherman.PausedHeight == -1 && fisherman.UnstakingHeight == -1) {
-			return fmt.Errorf(""hydrateSessionFishermen should not have encountered a paused or unstaking fisherman: %s"", fisherman.Address)
-		}
-
-		// TODO(#XXX): Filter by geo-zone
+	for _, fisher := range fishermen {
+		// Sanity check the fisher is not paused, jailed or unstaking
+		if fisher.PausedHeight != -1 || fisher.UnstakingHeight != -1 {
+			return fmt.Errorf(""hydrateSessionFishermen should not have encountered a paused or unstaking fisherman: %s"", fisher.Address)
+		}
+
+		// TODO(#697): Filter by geo-zone
 
 		// OPTIMIZE: If this was a map[string]struct{}, we could have avoided the loop
-		var chain string
-		for _, chain = range fisherman.Chains {
-			if chain != s.session.RelayChain {
-				chain = """"
-				continue
-			}
-		}
-		if chain != """" {
-			candidateFishermen = append(candidateFishermen, fisherman)
+		if slices.Contains(fisher.Chains, s.session.RelayChain) {
+			candidateFishermen = append(candidateFishermen, fisher)
 		}
 	}
 
@@ -228,8 +219,8 @@

 }
 
 // pseudoRandomSelection returns a random subset of the candidates.
-// TECHDEBT: We are using a `Go` native implementation for a pseudo-random number generator. In order
-// for it to be language agnostic, a general purpose algorithm needs ot be used.
+// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
+// for it to be language agnostic, a general purpose algorithm MUST be used.
 func pseudoRandomSelection(candidates []*coreTypes.Actor, numTarget int, sessionId []byte) []*coreTypes.Actor {
 	// If there aren't enough candidates, return all of them
 	if numTarget > len(candidates) {
@@ -238,6 +229,7 @@

 	}
 
 	// Take the first 8 bytes of sessionId to use as the seed
+	// NB: There is specific reason why `BigEndian` was chosen over `LittleEndian` in this specific context.
 	seed := int64(binary.BigEndian.Uint64(crypto.SHA3Hash(sessionId)[:8]))
 
 	// Retrieve the indices for the candidates
@@ -252,10 +244,11 @@

 
 // OPTIMIZE: Postgres uses a `Twisted Mersenne Twister (TMT)` randomness algorithm.
 // We could potentially look into changing everything into a single SQL query but
-// would nee dto verify that it can be implemented in a platform agnostic way.
+// would need to verify that it can be implemented in a platform agnostic way.
 
 // uniqueRandomIndices returns a map of `numIndices` unique random numbers less than `maxIndex`
 // seeded by `seed`.
+// panics if `numIndicies > maxIndex` since that code path SHOULD never be executed.
 // NB: A map pointing to empty structs is used to simulate set behaviour.
 func uniqueRandomIndices(seed, maxIndex, numIndices int64) map[int64]struct{} {
 	// This should never happen",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160295772,https://github.com/pokt-network/pocket/pull/622#discussion_r1160295772,,85,010c045d060cf8114d09066846c1f7d1fe5f7318,6b9b2db0f25b5fb605c1cb6d454b47708ff4eacd,utility/unit_of_work/gov.go,nan,"Thoughts on making this a receiver of uow so you don't have to pass it in?
I suggest changing
 ```
+func getGovParam[T *big.Int | int | int64 | []byte | string](u *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {
```
 to
```
+func (uow *baseUtilityUnitOfWork) getGovParam[T *big.Int | int | int64 | []byte | string](paramName string) (i T, err typesUtil.Error) {
```","+	}
+}
+
+func getGovParam[T *big.Int | int | int64 | []byte | string](u *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {","--- 

+++ 

@@ -2,7 +2,9 @@

 
 import (
 	""math/big""
-
+	""strings""
+
+	""github.com/pokt-network/pocket/logger""
 	""github.com/pokt-network/pocket/persistence""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/utils""
@@ -11,11 +13,19 @@

 )
 
 func init() {
-	GovParams = generateParamConfigs()
+	govParamTypes = prepareGovParamParamTypesMap()
+	for _, key := range utils.GovParamMetadataKeys {
+		if isOwner := strings.Contains(key, ""_owner""); isOwner {
+			continue
+		}
+		if _, ok := govParamTypes[key]; !ok {
+			logger.Global.Fatal().Msgf(""govParamTypes map does not contain: %s"", key)
+		}
+	}
 }
 
 var (
-	GovParams map[string]int
+	govParamTypes map[string]int
 )
 
 const (
@@ -26,7 +36,7 @@

 	STRING
 )
 
-func generateParamConfigs() map[string]int {
+func prepareGovParamParamTypesMap() map[string]int {
 	return map[string]int{
 		typesUtil.AppMinimumStakeParamName:                 BIGINT,
 		typesUtil.AppMaxChainsParamName:                    INT,
@@ -59,6 +69,8 @@

 		typesUtil.MessageSendFee:                           BIGINT,
 		typesUtil.MessageStakeFishermanFee:                 BIGINT,
 		typesUtil.MessageEditStakeFishermanFee:             BIGINT,
+		typesUtil.MessageUnstakeFishermanFee:               BIGINT,
+		typesUtil.MessagePauseFishermanFee:                 BIGINT,
 		typesUtil.MessageUnpauseFishermanFee:               BIGINT,
 		typesUtil.MessageFishermanPauseServicerFee:         BIGINT,
 		typesUtil.MessageTestScoreFee:                      BIGINT,
@@ -82,25 +94,25 @@

 	}
 }
 
-func getGovParam[T *big.Int | int | int64 | []byte | string](u *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {
+func getGovParam[T *big.Int | int | int64 | []byte | string](uow *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {
 	switch tp := any(i).(type) {
 	case *big.Int:
-		v, er := u.getBigIntParam(paramName)
+		v, er := uow.getBigIntParam(paramName)
 		return any(v).(T), er
 	case int:
-		v, er := u.getIntParam(paramName)
+		v, er := uow.getIntParam(paramName)
 		return any(v).(T), er
 	case int64:
-		v, er := u.getInt64Param(paramName)
+		v, er := uow.getInt64Param(paramName)
 		return any(v).(T), er
 	case []byte:
-		v, er := u.getByteArrayParam(paramName)
+		v, er := uow.getByteArrayParam(paramName)
 		return any(v).(T), er
 	case string:
-		v, er := u.getStringParam(paramName)
+		v, er := uow.getStringParam(paramName)
 		return any(v).(T), er
 	default:
-		u.logger.Fatal().Msgf(""unhandled parameter type: %T"", tp)
+		uow.logger.Fatal().Msgf(""unhandled parameter type: %T"", tp)
 	}
 	return
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1253759436,https://github.com/pokt-network/pocket/pull/732#discussion_r1253759436,,149,4f879217cbd947cb98258e447397b7cc8f063884,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/README.md,nan,"I suggest changing
 ```
+
```
 to
```
+
+_tl;dr Deleting an instance does not necessarily delete instances it depends on_
+
+
```","+_(see: [UML Shared composition](https://www.uml-diagrams.org/composition.html))_
+
+#### Aggregation
+","--- 

+++ 

@@ -7,6 +7,8 @@

 - [Definitions](#definitions)
 - [Interface & Integration](#interface--integration)
 - [Module Architecture](#module-architecture)
+  - [Architecture Design Language](#architecture-design-language)
+  - [Legends](#legends)
   - [P2P Module / Router Decoupling](#p2p-module--router-decoupling)
   - [Message Propagation & Handling](#message-propagation--handling)
   - [Message Deduplication](#message-deduplication)
@@ -71,6 +73,8 @@

 
 ## Module Architecture
 
+_(TODO: move ""arch. design lang."" & ""legends"" sections into `shared` to support common usage)_
+
 ### Architecture Design Language
 
 The architecture design language expressed in this documentation is based on [UML](https://www.uml-diagrams.org/).
@@ -123,6 +127,8 @@

 
 #### Interface Realization
 
+_TL;DR An instance (i.e. client) implements the associated interface (i.e. supplierl)._
+
 > Realization is a specialized abstraction relationship between two sets of model elements, one representing a specification (the supplier) and the other represents an implementation of the latter (the client).
 
 > Realization can be used to model stepwise refinement, optimizations, transformations, templates, model synthesis, framework composition, etc.
@@ -131,6 +137,8 @@

 
 #### Direct Usage
 
+_TL;DR one instance (i.e. client) is dependent the associated instance(s) (i.e. supplier) to function properly._
+
 > Dependency is a directed relationship which is used to show that some UML element or a set of elements requires, needs or depends on other model elements for specification or implementation. Because of this, dependency is called a supplier - client relationship, where supplier provides something to the client, and thus the client is in some sense incomplete while semantically or structurally dependent on the supplier element(s). Modification of the supplier may impact the client elements.
 
 > Usage is a dependency in which one named element (client) requires another named element (supplier) for its full definition or implementation.
@@ -139,6 +147,8 @@

 
 #### Composition
 
+_TL;DR deleting an instance also deletes the associated instance(s)._
+
 > A ""strong"" form of aggregation
 
 > If a composite (whole) is deleted, all of its composite parts are ""normally"" deleted with it.
@@ -147,7 +157,10 @@

 
 #### Aggregation
 
-> A ""weak"" form of composition
+
+_TL;DR deleting an instance does not necessarily delete the associated instance(s)._
+
+> A ""weak"" form of aggregation
 
 > Shared part could be included in several composites, and if some or all of the composites are deleted, shared part may still exist.
 
@@ -155,7 +168,11 @@

 
 #### Cardinality
 
-Cardinality indicates the number or range of simultaneous instances of the classifier at the ""cardinality-side"" association end that are associated with the classifier at the other end of the given association type.
+_TL;DR indicates a number, or range of instances associated (i.e. supplier(s))_
+
+Cardinality indicates the number or range of simultaneous instances of supplier that are associated with the client.
+Applicable to multiple association types.
+Can be expressed arbitrarily (e.g. wildcards, variable, equation, etc.)
 
 _(see: [UML Association](https://www.uml-diagrams.org/association.html#association-end))_
 
@@ -169,19 +186,21 @@

 
 **Unicast**
 
-| Sender         | Receiver       | Router          | Example Usage                                        |
-|----------------|----------------|-----------------|------------------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree only   | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Staked Actor   | Background only | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Unstaked Actor | Background only | Consensus (state sync) & Debug (CLI) messages        |
+| Sender         | Receiver       | Router          | Example Usage                                                        |
+|----------------|----------------|-----------------|----------------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree only   | Consensus hotstuff messages (validators only) & state sync responses |
+| Staked Actor   | Untaked Actor  | Background only | Consensus state sync responses                                       |
+| Unstaked Actor | Staked Actor   | Background only | Consensus state sync responses, debug messages                       |
+| Unstaked Actor | Unstaked Actor | Background only | Consensus state sync responses, debug messages                       |
 
 **Broadcast**
 
-| Broadcaster    | Receiver       | Router                | Example Usage                              |
-|----------------|----------------|-----------------------|--------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages                        |
-| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (gossipsub redundancy) |
-| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages                        |
+| Broadcaster    | Receiver       | Router                | Example Usage                                                   |
+|----------------|----------------|-----------------------|-----------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages, consensus state sync requests              |
+| Staked Actor   | Untaked Actor  | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages, consensus state sync requests              |
 
 Both router submodule implementations embed a `UnicastRouter` which enables them to send and receive messages directly to/from a single peer.
 
@@ -486,7 +505,7 @@

 This pairing always has an associated TTL (time-to-live), near the end of which it must
 be refreshed.
 
-In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves 7/8th through their TTL.
+In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves every 3 hours through their TTL (see: [`RoutingDiscovery#Advertise()`](https://github.com/libp2p/go-libp2p/blob/87c2561238cb0340ddb182c61be8dbbc7a12a780/p2p/discovery/routing/routing.go#L34) and [`ProviderManager#AddProvider()`](https://github.com/libp2p/go-libp2p-kad-dht/blob/v0.24.2/providers/providers_manager.go#L255)).
 This refreshes the libp2p peerstore automatically.
 
 In the raintree gossip overlay network (`raintreeRouter`), the libp2p peerstore is **NOT** currently refreshed _(TODO: [#859](https://github.com/pokt-network/network/isues/859))_.",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184482817,https://github.com/pokt-network/pocket/pull/710#discussion_r1184482817,,119,b6be2a930185debeb449f91b51d6c70a44a35401,dbc0deb6a7aec39359745e9be952a4992689052a,utility/module.go,nan,Can you move this into a helper function call `validateActorModuleExclusivity` with a `TODO` comment to figure out all the rules we want to enable?," 
-	return m, nil
+	actors := m.GetActorModuleNames()
+	if len(m.actorModules) > 1 {","--- 

+++ 

@@ -9,11 +9,9 @@

 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 	""github.com/pokt-network/pocket/utility/fisherman""
-	fisherman_module ""github.com/pokt-network/pocket/utility/fisherman""
-	portal_module ""github.com/pokt-network/pocket/utility/portal""
-	servicer_module ""github.com/pokt-network/pocket/utility/servicer""
+	""github.com/pokt-network/pocket/utility/servicer""
 	""github.com/pokt-network/pocket/utility/types""
-	validator_module ""github.com/pokt-network/pocket/utility/validator""
+	""github.com/pokt-network/pocket/utility/validator""
 )
 
 const (
@@ -33,12 +31,7 @@

 
 	mempool mempool.TXMempool
 
-	actorModules []modules.Module
-
-	validator *validator_module.ValidatorModule
-	servicer  *servicer_module.ServicerModule
-	fisherman *fisherman_module.FishermanModule
-	portal    *portal_module.PortalModule
+	actorModules map[string]modules.Module
 }
 
 func Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
@@ -47,7 +40,7 @@

 
 func (*utilityModule) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
 	m := &utilityModule{
-		actorModules: []modules.Module{},
+		actorModules: map[string]modules.Module{},
 	}
 
 	for _, option := range options {
@@ -65,66 +58,48 @@

 	m.mempool = types.NewTxFIFOMempool(utilityCfg.MaxMempoolTransactionBytes, utilityCfg.MaxMempoolTransactions)
 	m.logger = logger.Global.CreateLoggerForModule(m.GetModuleName())
 
-	return m, enableActorModules(cfg, m, bus)
+	if err := m.enableActorModules(cfg); err != nil {
+		return m, err
+	}
+
+	return m, nil
 }
 
 // enableActorModules enables the actor-specific modules and adds them to the utility module's actorModules to be started later.
-func enableActorModules(cfg *configs.Config, m *utilityModule, bus modules.Bus) error {
+func (u *utilityModule) enableActorModules(cfg *configs.Config) error {
 	fishermanCfg := cfg.Fisherman
 	servicerCfg := cfg.Servicer
 	validatorCfg := cfg.Validator
-	portalCfg := cfg.Portal
 
 	if servicerCfg.Enabled {
-		servicer, err := servicer_module.CreateServicer(bus)
+		s, err := servicer.CreateServicer(u.GetBus())
 		if err != nil {
-			m.logger.Error().Err(err).Msg(""failed to create servicer module"")
+			u.logger.Error().Err(err).Msg(""failed to create servicer module"")
 			return err
 		}
-		m.servicer = &servicer
-		m.actorModules = append(m.actorModules, servicer)
+		u.actorModules[s.GetModuleName()] = s
 	}
 
 	if fishermanCfg.Enabled {
-		fisherman, err := fisherman.CreateFisherman(bus)
+		f, err := fisherman.CreateFisherman(u.GetBus())
 		if err != nil {
-			m.logger.Error().Err(err).Msg(""failed to create fisherman module"")
+			u.logger.Error().Err(err).Msg(""failed to create fisherman module"")
 			return err
 		}
-		m.fisherman = &fisherman
-		m.actorModules = append(m.actorModules, fisherman)
+		u.actorModules[f.GetModuleName()] = f
 	}
 
 	if validatorCfg.Enabled {
-		validator, err := validator_module.CreateValidator(bus)
+		v, err := validator.CreateValidator(u.GetBus())
 		if err != nil {
-			m.logger.Error().Err(err).Msg(""failed to create validator module"")
+			u.logger.Error().Err(err).Msg(""failed to create validator module"")
 			return err
 		}
-		m.validator = &validator
-		m.actorModules = append(m.actorModules, validator)
+		u.actorModules[v.GetModuleName()] = v
 	}
 
-	if portalCfg.Enabled {
-		portal, err := portal_module.CreatePortal(bus)
-		if err != nil {
-			m.logger.Error().Err(err).Msg(""failed to create portal module"")
-			return err
-		}
-		m.portal = &portal
-		m.actorModules = append(m.actorModules, portal)
-	}
-
-	actors := m.GetActorModuleNames()
-	if len(m.actorModules) > 1 {
-		// only case where this is allowed is if the node is a validator and a servicer
-		if !(validatorCfg.Enabled && servicerCfg.Enabled) {
-			m.logger.Error().Strs(""actors"", actors).Msg(ErrInvalidActorsEnabled)
-			m.actorModules = []modules.Module{} // reset the actorModules
-			return errors.New(ErrInvalidActorsEnabled)
-		}
-	} else {
-		m.logger.Info().Strs(""actors"", actors).Msg(""Node actors enabled"")
+	if err := u.validateActorModuleExclusivity(cfg); err != nil {
+		return err
 	}
 
 	return nil
@@ -162,18 +137,62 @@

 	return u.mempool
 }
 
-func (u *utilityModule) GetLogger() *modules.Logger {
-	return u.logger
-}
-
-func (u *utilityModule) GetActorModules() []modules.Module {
+func (u *utilityModule) GetActorModules() map[string]modules.Module {
 	return u.actorModules
 }
 
-func (u *utilityModule) GetActorModuleNames() []string {
-	names := []string{}
+func (u *utilityModule) GetServicerModule() (modules.ServicerModule, error) {
+	if u.actorModules[servicer.ServicerModuleName] == nil {
+		return nil, errors.New(""servicer module not enabled"")
+	}
+	if m, ok := u.actorModules[servicer.ServicerModuleName].(modules.ServicerModule); ok {
+		return m, nil
+	}
+	return nil, errors.New(""failed to cast servicer module"")
+}
+
+func (u *utilityModule) GetFishermanModule() (modules.FishermanModule, error) {
+	if u.actorModules[fisherman.FishermanModuleName] == nil {
+		return nil, errors.New(""fisherman module not enabled"")
+	}
+	if m, ok := u.actorModules[fisherman.FishermanModuleName].(modules.FishermanModule); ok {
+		return m, nil
+	}
+	return nil, errors.New(""failed to cast fisherman module"")
+}
+
+func (u *utilityModule) GetValidatorModule() (modules.ValidatorModule, error) {
+	if u.actorModules[validator.ValidatorModuleName] == nil {
+		return nil, errors.New(""validator module not enabled"")
+	}
+	if m, ok := u.actorModules[validator.ValidatorModuleName].(modules.ValidatorModule); ok {
+		return m, nil
+	}
+	return nil, errors.New(""failed to cast validator module"")
+}
+
+// validateActorModuleExclusivity validates that the actor modules are enabled in a valid combination.
+// TODO: There are probably more rules that need to be added here.
+func (u *utilityModule) validateActorModuleExclusivity(cfg *configs.Config) error {
+	servicerCfg := cfg.Servicer
+	validatorCfg := cfg.Validator
+	actors := []string{}
 	for _, submodule := range u.actorModules {
-		names = append(names, submodule.GetModuleName())
+		actors = append(actors, submodule.GetModuleName())
 	}
-	return names
+
+	if len(u.actorModules) > 1 {
+		// only case where this is allowed is if the node is a validator and a servicer
+		isVal := (validatorCfg != nil && validatorCfg.Enabled)
+		isServ := (servicerCfg != nil && servicerCfg.Enabled)
+		if !isVal || !isServ {
+			u.logger.Error().Strs(""actors"", actors).Msg(ErrInvalidActorsEnabled)
+			u.actorModules = map[string]modules.Module{}
+			return errors.New(ErrInvalidActorsEnabled)
+		}
+	}
+
+	u.logger.Info().Strs(""actors"", actors).Msg(""Node actors enabled"")
+
+	return nil
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1182022834,https://github.com/pokt-network/pocket/pull/684#discussion_r1182022834,,264,964333fcec0ccb79708af5ce07dbab1cf5efb3a5,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/v1/openapi.yaml,nan,s/QUeryHeightPaginated/QueryAddressHeightPaginated/,"+        content:
+          application/json:
+            schema:
+              $ref: ""#/components/schemas/QueryHeightPaginated""","--- 

+++ 

@@ -99,7 +99,7 @@

         - client
       summary: Sends a session request to the network and get the nodes that will be servicing your requests for the session
       requestBody:
-        description: Request the nodes that will be servicing the requests in your session
+        description: Retrieve the list of actors involved in servicing and verifying a session
         content:
           application/json:
             schema:
@@ -107,7 +107,7 @@

         required: true
       responses:
         ""200"":
-          description: Session servicer response
+          description: Session response
           content:
             application/json:
               schema:
@@ -122,11 +122,12 @@

           content:
             text/plain:
               example: ""description of failure""
+  # TODO: Update this handler and its schemas when the HandleRelay function has been implemented
   /v1/client/relay:
     post:
       tags:
         - client
-      summary: Sends a challenge request to the network to service the RPC request
+      summary: Sends a relay to the servicer to receive a response
       requestBody:
         description: Request a relay to be sent on behalf of your application
         content:
@@ -151,11 +152,12 @@

           content:
             text/plain:
               example: ""description of failure""
+  # TODO: Update this handler and its schemas when the HandleChallenge function has been implemented
   /v1/client/challenge:
     post:
       tags:
         - client
-      summary: Sends a relay request to the network to for invalid data returned from an RPC request
+      summary: Sends a challenge request to the network
       requestBody:
         description: Request a challenge for invalid data returned from an RPC request
         content:
@@ -229,7 +231,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -294,7 +296,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressPaginated""
+              $ref: ""#/components/schemas/QueryAccountPaginated""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               page: 1
@@ -322,7 +324,7 @@

     get:
       tags:
         - query
-      summary: Returns the current values of all governance parameters
+      summary: Returns the current values of all on-chain governance parameters
       responses:
         ""200"":
           description: Returns all the chain parameters
@@ -345,7 +347,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -367,7 +369,9 @@

           content:
             text/plain:
               example: ""description of failure""
-  # TODO: (h5law) Think of an equivalent to staking status
+  # TODO: (h5law) Determine a parameter to give the request that differentiates
+  # the staking status of an app and returns only that type of staked application
+  # This will be equivalent to the `staking_status` field from the V0 RPC spec
   /v1/query/apps:
     post:
       tags:
@@ -411,7 +415,7 @@

         content:
           application/json:
             schema:
-              $ref: '#/components/schemas/QueryAddressHeight'
+              $ref: '#/components/schemas/QueryAccountHeight'
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 99
@@ -508,7 +512,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -621,7 +625,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -758,7 +762,7 @@

           content:
             application/json:
               schema:
-                $ref: ""#/components/schemas/Transaction""
+                $ref: ""#/components/schemas/IndexedTransaction""
         ""400"":
           description: Bad request
           content:
@@ -789,7 +793,7 @@

           content:
             application/json:
               schema:
-                $ref: ""#/components/schemas/Transaction""
+                $ref: ""#/components/schemas/IndexedTransaction""
         ""400"":
           description: Bad request
           content:
@@ -874,7 +878,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -957,7 +961,7 @@

           type: string
         session_id:
           type: string
-    QueryAddressHeight:
+    QueryAccountHeight:
       type: object
       required:
         - height
@@ -968,7 +972,7 @@

           format: int64
         address:
           type: string
-    QueryAddressPaginated:
+    QueryAccountPaginated:
       type: object
       required:
         - address
@@ -1197,7 +1201,7 @@

         txs:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
         total_txs:
           type: integer
           format: int64
@@ -1317,7 +1321,7 @@

         transactions:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
         total_txs:
           type: integer
           format: int64
@@ -1459,7 +1463,7 @@

         transactions:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
     BlockHeader:
       type: object
       required:
@@ -1753,7 +1757,7 @@

           type: string
         signature:
           type: string
-    StdTx:
+    TxMessage:
       type: object
       required:
         - fee
@@ -1781,8 +1785,7 @@

         - hash
         - height
         - index
-        - txResult
-        - stdTx
+        - txMsg
       properties:
         hash:
           type: string
@@ -1792,14 +1795,11 @@

         index:
           type: integer
           format: int32
-        txResult:
-          $ref: ""#/components/schemas/TxResult""
-        stdTx:
-          $ref: ""#/components/schemas/StdTx""
-    TxResult:
-      type: object
-      required:
-        - tx
+        txMsg:
+          $ref: ""#/components/schemas/TxMessage""
+    IndexedTransaction:
+      type: object
+      required:
         - height
         - index
         - result_code
@@ -1807,9 +1807,8 @@

         - signer_addr
         - recipient_addr
         - message_type
-      properties:
-        tx:
-          type: string
+        - tx
+      properties:
         height:
           type: integer
           format: int64
@@ -1825,6 +1824,8 @@

           type: string
         message_type:
           type: string
+        tx:
+          $ref: ""#/components/schemas/Transaction""
     ThresholdSignature:
       type: object
       required:",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1230110454,https://github.com/pokt-network/pocket/pull/756#discussion_r1230110454,,123,64586305e2aaeb40b5380dde8c922368e4ec8464,70a1a0e2fe2c238fce8c2019e13e9167629a2639,persistence/trees/trees.go,nan,"Can you move `sha256.New` into a local constant called `smtPathHasher` so it's not ""hidden"" in the code","+			return nil, err
+		}
+		stateTrees.nodeStores[tree] = nodeStore
+		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, sha256.New())","--- 

+++ 

@@ -6,22 +6,24 @@

 
 import (
 	""bytes""
-	""context""
 	""crypto/sha256""
 	""encoding/hex""
 	""fmt""
+	""hash""
 
 	""github.com/jackc/pgx/v5""
 
 	""github.com/pokt-network/pocket/persistence/indexer""
 	""github.com/pokt-network/pocket/persistence/kvstore""
-	ptypes ""github.com/pokt-network/pocket/persistence/types""
+	""github.com/pokt-network/pocket/persistence/sql""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/smt""
 )
 
+var smtTreeHasher hash.Hash = sha256.New()
+
 var merkleTreeToString = map[merkleTree]string{
 	appMerkleTree:      ""app"",
 	valMerkleTree:      ""val"",
@@ -43,13 +45,6 @@

 	coreTypes.ActorType_ACTOR_TYPE_SERVICER: servicerMerkleTree,
 }
 
-var actorTypeToSchemaName = map[coreTypes.ActorType]ptypes.ProtocolActorSchema{
-	coreTypes.ActorType_ACTOR_TYPE_APP:      ptypes.ApplicationActor,
-	coreTypes.ActorType_ACTOR_TYPE_VAL:      ptypes.ValidatorActor,
-	coreTypes.ActorType_ACTOR_TYPE_FISH:     ptypes.FishermanActor,
-	coreTypes.ActorType_ACTOR_TYPE_SERVICER: ptypes.ServicerActor,
-}
-
 var merkleTreeToActorTypeName = map[merkleTree]coreTypes.ActorType{
 	appMerkleTree:      coreTypes.ActorType_ACTOR_TYPE_APP,
 	valMerkleTree:      coreTypes.ActorType_ACTOR_TYPE_VAL,
@@ -65,6 +60,8 @@

 	// defines the index of the root hash each independent as they are concatenated together
 	// to generate the state hash.
 
+	// TECHDEBT(#834): Remove the need for enforced ordering
+
 	// Actor Merkle Trees
 	appMerkleTree merkleTree = iota
 	valMerkleTree
@@ -86,7 +83,7 @@

 
 // treeStore stores a set of merkle trees that
 // it manages. It fulfills the modules.TreeStore interface.
-// * It is responsible for commit or rollback behavior
+// * It is responsible for atomic commit or rollback behavior
 // of the underlying trees by utilizing the lazy loading
 // functionality provided by the underlying smt library.
 type treeStore struct {
@@ -95,12 +92,12 @@

 	nodeStores   map[merkleTree]kvstore.KVStore
 }
 
-// Update takes a transaction and a height and updates
-// all of the trees in the treeStore for that height.
 func (t *treeStore) Update(pgtx pgx.Tx, txi indexer.TxIndexer, height uint64) (string, error) {
 	return t.updateMerkleTrees(pgtx, txi, height)
 }
 
+// NewStateTrees is the constructor object for a treeStore and initializes and configures a new
+// tree for the appropriate type of store, i.e. in-memory vs file system storage.
 func NewStateTrees(treesStoreDir string) (*treeStore, error) {
 	if treesStoreDir == "":memory:"" {
 		return newMemStateTrees()
@@ -118,22 +115,22 @@

 			return nil, err
 		}
 		stateTrees.nodeStores[tree] = nodeStore
-		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
+		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
 	}
 	return stateTrees, nil
 }
 
 // DebugClearAll is used by the debug cli to completely reset all merkle trees.
 // This should only be called by the debug CLI.
+// TECHDEBT: Move this into a separate file with a debug build flag to avoid accidental usage in prod
 func (t *treeStore) DebugClearAll() error {
 	for treeType := merkleTree(0); treeType < numMerkleTrees; treeType++ {
 		nodeStore := t.nodeStores[treeType]
 		if err := nodeStore.ClearAll(); err != nil {
 			return fmt.Errorf(""failed to clear %s node store: %w"", merkleTreeToString[treeType], err)
 		}
-		t.merkleTrees[treeType] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
-	}
-
+		t.merkleTrees[treeType] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
+	}
 	return nil
 }
 
@@ -146,13 +143,13 @@

 	for tree := merkleTree(0); tree < numMerkleTrees; tree++ {
 		nodeStore := kvstore.NewMemKVStore() // For testing, `smt.NewSimpleMap()` can be used as well
 		stateTrees.nodeStores[tree] = nodeStore
-		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
+		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
 	}
 	return stateTrees, nil
 }
 
-// updateMerkleTrees updates all of the merkle trees that TreeStore manages.
-// * it returns an hash of the output or an error.
+// updateMerkleTrees updates all of the merkle trees in order defined by `numMerkleTrees`
+// * it returns the new state hash capturing the state of all the trees or an error if one occurred
 func (t *treeStore) updateMerkleTrees(pgtx pgx.Tx, txi indexer.TxIndexer, height uint64) (string, error) {
 	for treeType := merkleTree(0); treeType < numMerkleTrees; treeType++ {
 		switch treeType {
@@ -163,7 +160,7 @@

 				return """", fmt.Errorf(""no actor type found for merkle tree: %v"", treeType)
 			}
 
-			actors, err := t.getActorsUpdated(pgtx, actorType, height)
+			actors, err := sql.GetActors(pgtx, actorType, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get actors at height: %w"", err)
 			}
@@ -174,7 +171,7 @@

 
 		// Account Merkle Trees
 		case accountMerkleTree:
-			accounts, err := t.getAccounts(pgtx, height)
+			accounts, err := sql.GetAccounts(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get accounts: %w"", err)
 			}
@@ -182,7 +179,7 @@

 				return """", fmt.Errorf(""failed to update account trees: %w"", err)
 			}
 		case poolMerkleTree:
-			pools, err := t.getPools(pgtx, height)
+			pools, err := sql.GetPools(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get transactions: %w"", err)
 			}
@@ -192,7 +189,7 @@

 
 		// Data Merkle Trees
 		case transactionsMerkleTree:
-			indexedTxs, err := t.getTransactions(txi, height)
+			indexedTxs, err := sql.GetTransactions(txi, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get transactions: %w"", err)
 			}
@@ -200,7 +197,7 @@

 				return """", fmt.Errorf(""failed to update transactions: %w"", err)
 			}
 		case paramsMerkleTree:
-			params, err := t.getParams(pgtx, height)
+			params, err := sql.GetParams(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get params: %w"", err)
 			}
@@ -208,7 +205,7 @@

 				return """", fmt.Errorf(""failed to update params tree: %w"", err)
 			}
 		case flagsMerkleTree:
-			flags, err := t.getFlags(pgtx, height)
+			flags, err := sql.GetFlags(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get flags from transaction: %w"", err)
 			}
@@ -369,206 +366,3 @@

 
 	return nil
 }
-
-// getActorsUpdated is responsible for fetching the actors that have been updated at a given height.
-func (t *treeStore) getActorsUpdated(
-	pgtx pgx.Tx,
-	actorType coreTypes.ActorType,
-	height uint64,
-) ([]*coreTypes.Actor, error) {
-	actorSchema, ok := actorTypeToSchemaName[actorType]
-	if !ok {
-		return nil, fmt.Errorf(""no schema found for actor type: %s"", actorType)
-	}
-
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	query := actorSchema.GetUpdatedAtHeightQuery(int64(height))
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	addrs := make([][]byte, 0)
-	for rows.Next() {
-		var addr string
-		if err := rows.Scan(&addr); err != nil {
-			return nil, err
-		}
-		addrBz, err := hex.DecodeString(addr)
-		if err != nil {
-			return nil, err
-		}
-		addrs = append(addrs, addrBz)
-	}
-
-	actors := make([]*coreTypes.Actor, len(addrs))
-	for i, addr := range addrs {
-		// TECHDEBT #XXX: Avoid this cast to int64
-		actor, err := t.getActor(pgtx, actorSchema, addr, int64(height))
-		if err != nil {
-			return nil, err
-		}
-		actors[i] = actor
-	}
-	rows.Close()
-
-	return actors, nil
-}
-
-func (t *treeStore) getAccountsUpdated(
-	pgtx pgx.Tx,
-	acctType ptypes.ProtocolAccountSchema,
-	height uint64,
-) ([]*coreTypes.Account, error) {
-	accounts := []*coreTypes.Account{}
-
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	query := acctType.GetAccountsUpdatedAtHeightQuery(int64(height))
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	for rows.Next() {
-		acc := new(coreTypes.Account)
-		if err := rows.Scan(&acc.Address, &acc.Amount); err != nil {
-			return nil, err
-		}
-		accounts = append(accounts, acc)
-	}
-
-	return accounts, nil
-}
-
-func (t *treeStore) getTransactions(txi indexer.TxIndexer, height uint64) ([]*coreTypes.IndexedTransaction, error) {
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	indexedTxs, err := txi.GetByHeight(int64(height), false)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get transactions by height: %w"", err)
-	}
-	return indexedTxs, nil
-}
-
-// getPools returns the pools updated at the given height
-func (t *treeStore) getPools(pgtx pgx.Tx, height uint64) ([]*coreTypes.Account, error) {
-	pools, err := t.getAccountsUpdated(pgtx, ptypes.Pool, height)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get pools: %w"", err)
-	}
-	return pools, nil
-}
-
-// getAccounts returns the list of accounts updated at the provided height
-func (t *treeStore) getAccounts(pgtx pgx.Tx, height uint64) ([]*coreTypes.Account, error) {
-	accounts, err := t.getAccountsUpdated(pgtx, ptypes.Account, height)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get accounts: %w"", err)
-	}
-	return accounts, nil
-}
-
-func (t *treeStore) getFlags(pgtx pgx.Tx, height uint64) ([]*coreTypes.Flag, error) {
-	fields := ""name,value,enabled""
-	query := fmt.Sprintf(""SELECT %s FROM %s WHERE height=%d ORDER BY name ASC"", fields, ptypes.FlagsTableName, height)
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get flags: %w"", err)
-	}
-	defer rows.Close()
-
-	flagSlice := []*coreTypes.Flag{}
-	for rows.Next() {
-		flag := new(coreTypes.Flag)
-		if err := rows.Scan(&flag.Name, &flag.Value, &flag.Enabled); err != nil {
-			return nil, err
-		}
-		flag.Height = int64(height)
-		flagSlice = append(flagSlice, flag)
-	}
-
-	return flagSlice, nil
-}
-
-func (t *treeStore) getParams(pgtx pgx.Tx, height uint64) ([]*coreTypes.Param, error) {
-	fields := ""name,value""
-	query := fmt.Sprintf(""SELECT %s FROM %s WHERE height=%d ORDER BY name ASC"", fields, ptypes.ParamsTableName, height)
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	var paramSlice []*coreTypes.Param
-	for rows.Next() {
-		param := new(coreTypes.Param)
-		if err := rows.Scan(&param.Name, &param.Value); err != nil {
-			return nil, err
-		}
-		param.Height = int64(height)
-		paramSlice = append(paramSlice, param)
-	}
-
-	return paramSlice, nil
-}
-
-func (t *treeStore) getActor(tx pgx.Tx, actorSchema ptypes.ProtocolActorSchema, address []byte, height int64) (actor *coreTypes.Actor, err error) {
-	ctx := context.TODO()
-	actor, height, err = t.getActorFromRow(actorSchema.GetActorType(), tx.QueryRow(ctx, actorSchema.GetQuery(hex.EncodeToString(address), height)))
-	if err != nil {
-		return
-	}
-	return t.getChainsForActor(ctx, tx, actorSchema, actor, height)
-}
-
-func (t *treeStore) getActorFromRow(actorType coreTypes.ActorType, row pgx.Row) (actor *coreTypes.Actor, height int64, err error) {
-	actor = &coreTypes.Actor{
-		ActorType: actorType,
-	}
-	err = row.Scan(
-		&actor.Address,
-		&actor.PublicKey,
-		&actor.StakedAmount,
-		&actor.ServiceUrl,
-		&actor.Output,
-		&actor.PausedHeight,
-		&actor.UnstakingHeight,
-		&height)
-	return
-}
-
-func (t *treeStore) getChainsForActor(
-	ctx context.Context,
-	tx pgx.Tx,
-	actorSchema ptypes.ProtocolActorSchema,
-	actor *coreTypes.Actor,
-	height int64,
-) (a *coreTypes.Actor, err error) {
-	if actorSchema.GetChainsTableName() == """" {
-		return actor, nil
-	}
-	rows, err := tx.Query(ctx, actorSchema.GetChainsQuery(actor.Address, height))
-	if err != nil {
-		return actor, err
-	}
-	defer rows.Close()
-
-	var chainAddr string
-	var chainID string
-	var chainEndHeight int64 // DISCUSS: why is this commented as ""unused""?
-	for rows.Next() {
-		err = rows.Scan(&chainAddr, &chainID, &chainEndHeight)
-		if err != nil {
-			return
-		}
-		if chainAddr != actor.Address {
-			return actor, fmt.Errorf(""unexpected address %s, expected %s when reading chains"", chainAddr, actor.Address)
-		}
-		actor.Chains = append(actor.Chains, chainID)
-	}
-	return actor, nil
-}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1189208273,https://github.com/pokt-network/pocket/pull/727#discussion_r1189208273,,159,35eb743feb0e74e2fb39399abc6efdc0155840de,a06d875ed5964c806a6fe1ed4f4989f82797c07a,build/debug_keybase/main.go,nan,"I suggest changing
 ```
+	keysList := []string{}
```
 to
```
+	keysList := make([]string, 0)
```","-	}
-	validatorKeysMap := make(map[string]string)
+	decoder := yaml.NewDecoder(bytes.NewReader(privateKeysYamlBytes))
+	keysList := []string{}","--- 

+++ 

@@ -156,13 +156,12 @@

 func parsePrivateKeysFromEmbeddedYaml(privateKeysYamlBytes []byte) ([]string, error) {
 	// Parse the YAML file and load into the config struct
 	decoder := yaml.NewDecoder(bytes.NewReader(privateKeysYamlBytes))
-	keysList := []string{}
+	keysList := make([]string, 0)
 
 	for {
 		var secret K8sSecret
 
-		err := decoder.Decode(&secret)
-		if err != nil {
+		if err := decoder.Decode(&secret); err != nil {
 			if err == io.EOF {
 				break
 			}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220554141,https://github.com/pokt-network/pocket/pull/803#discussion_r1220554141,,270,f37b394454f45c27d058dac20af974a6ed273903,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/service/service.go,utility/servicer/module.go,"I suggest changing
 ```
+// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
```
 to
```
+// calculateServicerAppSessionTokens return the number of tokens the servicer has remaining for the Application in the session provided. If nothing is cached, the maximum number of session tokens is computed.
```","+	return s.totalTokens[session.Application.PublicKey]
+}
+
+// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session","--- 

+++ 

@@ -1,9 +1,8 @@

-package service
+package servicer
 
 import (
 	""bytes""
 	""encoding/hex""
-	""encoding/json""
 	""errors""
 	""fmt""
 	""io""
@@ -13,33 +12,39 @@

 	""sync""
 	""time""
 
-	""golang.org/x/exp/slices""
-
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
+	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 	""github.com/pokt-network/pocket/shared/utils""
 	typesUtil ""github.com/pokt-network/pocket/utility/types""
+	""golang.org/x/exp/slices""
 )
 
-// DISCUSS: where should the RelayAccracyParameter be defined?
-const RelayAccuracyParameter = 0.2
-
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
-	errValidateServicer    = errors.New(""relay does not match the servicer"")
-	errValidateApplication = errors.New(""relay failed application validation"")
-
-	_ modules.Servicer = &servicer{}
+	errValidateServicer    = errors.New(""relay failed servicer validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
+
+	_ modules.ServicerModule = &servicer{}
 )
 
+const (
+	ServicerModuleName = ""servicer""
+)
+
+// sessionTokens is used to cache the starting number of tokens available
+// during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	SessionNumber int64
-	Count         *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -49,19 +54,29 @@

 	logger *modules.Logger
 	config *configs.ServicerConfig
 
+	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
-	// totalTokens holds the total number of tokens assigned to this servicer for the app in the current session
-	// DISCUSS: considering the computational complexity, should we skip caching this value?
+	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
-func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return new(servicer).Create(bus, options...)
+var (
+	_ modules.ServicerModule = &servicer{}
+)
+
+func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.ServicerModule, error) {
+	m, err := new(servicer).Create(bus, options...)
+	if err != nil {
+		return nil, err
+	}
+	return m.(modules.ServicerModule), nil
 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
 	s := &servicer{
-		logger: logger.Global.CreateLoggerForModule(servicerModuleName),
+		totalTokens: make(map[string]*sessionTokens),
 	}
 
 	for _, option := range options {
@@ -70,19 +85,27 @@

 
 	bus.RegisterModule(s)
 
+	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
+
 	cfg := bus.GetRuntimeMgr().GetConfig()
-	s.config = cfg.Utility.ServicerConfig
+	s.config = cfg.Servicer
 
 	return s, nil
 }
 
+// TODO: implement this function
 func (s *servicer) Start() error {
-	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
-	return nil
-}
-
-func (*servicer) GetModuleName() string {
-	return servicerModuleName
+	s.logger.Info().Msg(""_ Servicer module started _"")
+	return nil
+}
+
+func (s *servicer) Stop() error {
+	s.logger.Info().Msg(""_ Servicer module stopped _"")
+	return nil
+}
+
+func (s *servicer) GetModuleName() string {
+	return ServicerModuleName
 }
 
 // HandleRelay processes a relay after performing validation.
@@ -101,8 +124,14 @@

 		return nil, fmt.Errorf(""Error executing relay: %w"", err)
 	}
 
-	// DISCUSS: should we validate the response from the node?
-	relayDigest, shouldStore, err := s.hasCollision(relay, response)
+	// TODO(M6): Look into data integrity checks and response validation.
+
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -110,83 +139,63 @@

 		return response, nil
 	}
 
-	height := s.GetBus().GetConsensusModule().CurrentHeight()
-	writeCtx, err := s.GetBus().GetPersistenceModule().NewRWContext(int64(height))
-	if err != nil {
-		return nil, fmt.Errorf(""Error getting a write context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	defer writeCtx.Release()
-
-	// DISCUSS: should we extend/use UnitOfWork for updating/retrieving token usage?
-	if err := writeCtx.RecordRelayService(relay.Meta.ApplicationAddress, relayDigest, relay, response); err != nil {
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
+	}
+
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
 	return response, nil
 }
 
-// hasCollision returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) hasCollision(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest []byte, collides bool, err error) {
-	relayBytes, err := marshal(relay, response)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
-	}
-
-	relayDigest := crypto.SHA3Hash(relayBytes)
-
+// isRelayVolumeApplicable returns:
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
+	}
+
+	relayDigest := crypto.SHA3Hash(relayReqResBytes)
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.hasCollisionOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
-	}
-
-	return signedDigest, collision, nil
-}
-
-// INCOMPLETE: implement this
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
+	}
+
+	return signedDigest, relayReqResBytes, collision, nil
+}
+
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) hasCollisionOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
-func marshal(request *coreTypes.Relay, response *coreTypes.RelayResponse) ([]byte, error) {
-	if request == nil || response == nil {
-		return nil, fmt.Errorf(""error marshalling: got nil value as input"")
-	}
-
-	s := struct {
-		*coreTypes.Relay
-		*coreTypes.RelayResponse
-	}{
-		request,
-		response,
-	}
-	return json.Marshal(s)
-}
-
-// executeRelay performs the passed relay using an HTTP request to the chain-specific target URL
+// executeRelay performs the passed relay using the correct method depending on the relay payload type.
 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
-	if relay.Meta == nil || relay.Meta.RelayChain == nil || relay.Meta.RelayChain.Id == """" {
-		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", relay.Meta.ApplicationAddress)
-	}
-
-	chainConfig, ok := s.config.Chains[relay.Meta.RelayChain.Id]
-	if !ok {
-		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", relay.Meta.RelayChain.Id, errValidateRelayMeta)
-	}
-
-	res, err := executeHTTPRequest(chainConfig, relay.Payload)
-	if err != nil {
-		return nil, fmt.Errorf(""Error executing HTTP request for relay on application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	return res, nil
+	switch payload := relay.RelayPayload.(type) {
+	case *coreTypes.Relay_JsonRpcPayload:
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
+	case *coreTypes.Relay_RestPayload:
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
+	default:
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+	}
 }
 
 // validateRelayMeta ensures the relay metadata is valid for being handled by the servicer
@@ -208,8 +217,8 @@

 }
 
 func (s *servicer) validateRelayChainSupport(relayChain *coreTypes.Identifiable, currentHeight int64) error {
-	if _, ok := s.config.Chains[relayChain.Id]; !ok {
-		return fmt.Errorf(""chain %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
+	if _, ok := s.config.Services[relayChain.Id]; !ok {
+		return fmt.Errorf(""service %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
 	}
 
 	// DISCUSS: either update NewReadContext to take a uint64, or the GetCurrentHeight to return an int64.
@@ -232,26 +241,27 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session, currentHeight int64) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session, currentHeight)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return fmt.Errorf(""Error getting read context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
-	}
-
-	usedAppSessionTokens, err := readCtx.GetServicerTokenUsage(session)
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
+	}
+
+	usedAppSessionTokens, err := localCtx.GetSessionTokensUsed(session)
 	if err != nil {
 		return fmt.Errorf(""Error getting servicer token usage: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -260,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -267,18 +280,21 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session, currentHeight int64) (*big.Int, error) {
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
+//
+//	If nothing is cached, the maximum number of session tokens is computed.
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.Count != nil && tokens.SessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.Count), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
 	//	This is distributed rate limiting (DRL): no need to know how many requests have
 	//		been performed for this application by other servicers. Instead, simply enforce
 	//		this servicer's share of the application's tokens for this session.
-	appSessionTokens, err := s.calculateAppSessionTokens(session.Application.StakedAmount, currentHeight)
+	appSessionTokens, err := s.calculateAppSessionTokens(session)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating application %s total tokens for session %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -290,7 +306,7 @@

 	// This multiplication is performed to minimize the chance of under-utilization of application's tokens,
 	//	while removing the overhead of communication between servicers which would be necessary otherwise.
 	// see https://arxiv.org/abs/2305.10672 for details on application and servicer distributed rate-limiting
-	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+RelayAccuracyParameter))
+	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+s.config.RelayMiningVolumeAccuracy))
 	roundedTokens, _ := adjustedTokens.Int(big.NewInt(1))
 
 	s.setAppSessionTokens(session, &sessionTokens{session.SessionNumber, roundedTokens})
@@ -300,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -329,6 +341,17 @@

 	return nil
 }
 
+// getSession returns a session for the current height and the passed relay
+func (s *servicer) getSession(relay *coreTypes.Relay) (*coreTypes.Session, error) {
+	height := s.GetBus().GetConsensusModule().CurrentHeight()
+	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
+	if err != nil {
+		return nil, fmt.Errorf(""failed to get a session for height %d for relay meta %s: %w"", height, relay.Meta, err)
+	}
+
+	return session, nil
+}
+
 // admitRelay decides whether the relay should be served
 func (s *servicer) admitRelay(relay *coreTypes.Relay) error {
 	// TODO: utility module should initialize the servicer (if this module instance is a servicer)
@@ -343,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
-	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
-	if err != nil {
-		return fmt.Errorf(""%s: failed to get a session for height %d for relay meta %s: %w"", errPrefix, height, relay.Meta, err)
-	}
-
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
+	session, err := s.getSession(relay)
+	if err != nil {
+		return err
+	}
+
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -358,35 +379,105 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session, int64(height)); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
-// of every session. Each servicer will serve a maximum of (Session Tokens / Number of Servicers in the Session) relays for the application
-func (s *servicer) calculateAppSessionTokens(appStakeStr string, currentHeight int64) (*big.Int, error) {
-	appStake, err := utils.StringToBigInt(appStakeStr)
-	if err != nil {
-		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", appStakeStr, coreTypes.ErrStringToBigInt(err))
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", currentHeight, err)
+// of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
+func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+	appStake, err := utils.StringToBigInt(session.Application.StakedAmount)
+	if err != nil {
+		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
+	}
+
+	// TODO(M5): find the right document to explain the following:
+	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
+	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
+	//		matches the beginning of the session.
+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", session.SessionHeight, err)
 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, currentHeight, err)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
 
 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
+}
+
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
+	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
+		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
+	}
+
+	serviceConfig, ok := s.config.Services[meta.RelayChain.Id]
+	if !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
+	if err != nil {
+		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
+	}
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
+	if err != nil {
+		return nil, err
+	}
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
+		req.Header.Set(k, v)
+	}
+	if req.Header.Get(""Content-Type"") == """" {
+		req.Header.Set(""Content-Type"", ""application/json"")
+	}
+
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
+	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
+	if err != nil {
+		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
+	}
+	defer resp.Body.Close()
+
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return nil, fmt.Errorf(""Error reading response body: %w"", err)
+	}
+
+	return &coreTypes.RelayResponse{Payload: string(body)}, nil
 }
 
 // IMPROVE: Add session height tolerance to account for session rollovers
@@ -404,188 +495,3 @@

 		sessionStartingBlock,
 		sessionLastBlock)
 }
-
-// TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1
-// TODO: remove: use coreTypes.Relay instead
-type Relay interface {
-	RelayPayload
-	RelayMeta
-}
-
-type RelayPayload interface {
-	GetData() string               // the actual data string for the external chain
-	GetMethod() string             // the http CRUD method
-	GetHTTPPath() string           // the HTTP Path
-	GetHeaders() map[string]string // http headers
-}
-
-type RelayMeta interface {
-	GetBlockHeight() int64 // the block height when the request is made
-	GetServicerPublicKey() crypto.PublicKey
-	GetRelayChain() RelayChain
-	GetGeoZone() GeoZone
-	GetToken() AAT
-	GetSignature() string
-}
-
-type RelayResponse interface {
-	Payload() string
-	ServicerSignature() string
-}
-
-type RelayChain Identifiable
-type GeoZone Identifiable
-
-type AAT interface {
-	GetVersion() string              // confirm a valid AAT version
-	GetApplicationPublicKey() string // confirm the identity/signature of the app
-	GetClientPublicKey() string      // confirm the identity/signature of the client
-	GetApplicationSignature() string // confirm the application signed the token
-}
-
-type Identifiable interface {
-	Name() string
-	ID() string
-}
-
-var _ Relay = &relay{}
-
-type relay struct{}
-
-// Validate a submitted relay by a client before servicing
-func (r *relay) Validate() coreTypes.Error {
-
-	// validate payload
-
-	// validate the metadata
-
-	// ensure the RelayChain is supported locally
-
-	// ensure session block height is current
-
-	// get the session context
-
-	// get the application object from the r.AAT()
-
-	// get session node count from that session height
-
-	// get maximum possible relays for the application
-
-	// ensure not over serviced
-
-	// generate the session from seed data
-
-	// validate self against the session
-
-	return nil
-}
-
-// Store a submitted relay by a client for volume tracking
-func (r *relay) Store() coreTypes.Error {
-
-	// marshal relay object into protoBytes
-
-	// calculate the hashOf(protoBytes) <needed for volume tracking>
-
-	// persist relay object, indexing under session
-
-	return nil
-}
-
-// Execute a submitted relay by a client after validation
-func (r *relay) Execute() (RelayResponse, coreTypes.Error) {
-
-	// retrieve the RelayChain url from the servicer's local configuration file
-
-	// execute http request with the relay payload
-
-	// format and digitally sign the response
-
-	return nil, nil
-}
-
-// Get volume metric applicable relays from store
-func (r *relay) ReapStoreForHashCollision(sessionBlockHeight int64, hashEndWith string) ([]Relay, coreTypes.Error) {
-
-	// Pull all relays whose hash collides with the revealed secret key
-	// It's important to note, the secret key isn't revealed by the network until the session is over
-	// to prevent volume based bias. The secret key is usually a pseudorandom selection using the block hash as a seed.
-	// (See the session protocol)
-	//
-	// Demonstrable pseudocode below:
-	//   `SELECT * from RELAY where HashOf(relay) ends with hashEndWith AND sessionBlockHeight=sessionBlockHeight`
-
-	// This function also signifies deleting the non-volume-applicable Relays
-
-	return nil, nil
-}
-
-// Report volume metric applicable relays to Fisherman
-func (r *relay) ReportVolumeMetrics(fishermanServiceURL string, volumeRelays []Relay) coreTypes.Error {
-
-	// Send all volume applicable relays to the assigned trusted Fisherman for
-	// a proper verification of the volume completed. Send volumeRelays to fishermanServiceURL
-	// through http.
-
-	// NOTE: an alternative design is a 2 step, claim - proof lifecycle where the individual servicers
-	// build a merkle sum index tree from all the relays, submits a root and subsequent merkle proof to the
-	// network.
-	//
-	// Pros: Can report volume metrics directly to the chain in a trustless fashion
-	// Cons: Large chain bloat, non-trivial compute requirement for creation of claim/proof transactions and trees,
-	//       non-trivial compute requirement to process claim / proofs during ApplyBlock()
-
-	return nil
-}
-
-func (r *relay) GetData() string                        { return """" }
-func (r *relay) GetMethod() string                      { return """" }
-func (r *relay) GetHTTPPath() string                    { return """" }
-func (r *relay) GetHeaders() map[string]string          { return nil }
-func (r *relay) GetBlockHeight() int64                  { return 0 }
-func (r *relay) GetServicerPublicKey() crypto.PublicKey { return nil }
-func (r *relay) GetRelayChain() RelayChain              { return nil }
-func (r *relay) GetGeoZone() GeoZone                    { return nil }
-func (r *relay) GetToken() AAT                          { return nil }
-func (r *relay) GetSignature() string                   { return """" }
-
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func executeHTTPRequest(cfg *configs.ChainConfig, relay *coreTypes.RelayPayload) (*coreTypes.RelayResponse, error) {
-	chainUrl, err := url.Parse(cfg.Url)
-	if err != nil {
-		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", cfg.Url, err)
-	}
-	targetUrl := chainUrl.JoinPath(relay.HttpPath)
-
-	req, err := http.NewRequest(relay.Method, targetUrl.String(), bytes.NewBuffer([]byte(relay.Data)))
-	if err != nil {
-		return nil, err
-	}
-	if cfg.BasicAuth != nil && cfg.BasicAuth.UserName != """" {
-		req.SetBasicAuth(cfg.BasicAuth.UserName, cfg.BasicAuth.Password)
-	}
-	if cfg.UserAgent != """" {
-		req.Header.Set(""User-Agent"", cfg.UserAgent)
-	}
-
-	for k, v := range relay.Headers {
-		req.Header.Set(k, v)
-	}
-	if len(relay.Headers) == 0 {
-		req.Header.Set(""Content-Type"", ""application/json"")
-	}
-
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
-	resp, err := (&http.Client{Timeout: time.Duration(cfg.TimeoutMilliseconds) * time.Millisecond}).Do(req)
-	if err != nil {
-		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
-	}
-	defer resp.Body.Close()
-
-	body, err := io.ReadAll(resp.Body)
-	if err != nil {
-		return nil, fmt.Errorf(""Error reading response body: %w"", err)
-	}
-
-	return &coreTypes.RelayResponse{Payload: string(body)}, nil
-}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1162133053,https://github.com/pokt-network/pocket/pull/659#discussion_r1162133053,,85,6a35868ee75f6c99df368e65221af283178b1cb6,82d931822979d5286c0cd811cf3f7f31ebb5897b,consensus/fsm_handler.go,nan,"I suggest changing
 ```
+	m.logger.Debug().Msg(""Node is in Sync Mode, starting syncing..."")
```
 to
```
+	m.logger.Debug().Msg(""Node is in Sync Mode, starting to sync..."")
```"," // In Sync mode node (validator or non-validator) starts syncing with the rest of the network.
 func (m *consensusModule) HandleSyncMode(msg *messaging.StateMachineTransitionEvent) error {
-	m.logger.Debug().Msg(""Node is in Sync Mode, start syncing..."")
+	m.logger.Debug().Msg(""Node is in Sync Mode, starting syncing..."")","--- 

+++ 

@@ -82,7 +82,7 @@

 // HandleSyncMode handles FSM event Consensus_IsSyncing, and SyncMode is the destination state.
 // In Sync mode node (validator or non-validator) starts syncing with the rest of the network.
 func (m *consensusModule) HandleSyncMode(msg *messaging.StateMachineTransitionEvent) error {
-	m.logger.Debug().Msg(""Node is in Sync Mode, starting syncing..."")
+	m.logger.Debug().Msg(""Node is in Sync Mode, starting to sync..."")
 
 	return m.stateSync.Start()
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160227465,https://github.com/pokt-network/pocket/pull/652#discussion_r1160227465,,122,f092dcd17501dfcb1376f815c09a480a63b1f6d2,c9f18a38f7eaa521dc9be5e477fe860fea301fe8,utility/unit_of_work/module.go,nan,"I suggest changing
 ```
+// It also removes the transactions from the mempool if they are already present.
```
 to
```
+// It also removes the transactions from the mempool if they are also present.
```","+}
+
+// processTransactionsFromProposalBlock processes the transactions from the proposal block.
+// It also removes the transactions from the mempool if they are already present.","--- 

+++ 

@@ -1,6 +1,8 @@

 package unit_of_work
 
 import (
+	""fmt""
+
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/mempool""
 	""github.com/pokt-network/pocket/shared/modules""
@@ -36,6 +38,8 @@

 	proposalStateHash    string
 	proposalProposerAddr []byte
 	proposalBlockTxs     [][]byte
+
+	stateHash string
 }
 
 func (uow *baseUtilityUnitOfWork) SetProposalBlock(blockHash string, proposerAddr []byte, txs [][]byte) error {
@@ -45,46 +49,58 @@

 	return nil
 }
 
-// CLEANUP: code re-use ApplyBlock() for CreateAndApplyBlock()
-func (uow *baseUtilityUnitOfWork) ApplyBlock() (stateHash string, txs [][]byte, err error) {
+func (uow *baseUtilityUnitOfWork) ApplyBlock() error {
 	log := uow.logger.With().Fields(map[string]interface{}{
 		""source"": ""ApplyBlock"",
 	}).Logger()
 
 	log.Debug().Msg(""checking if proposal block has been set"")
 	if !uow.isProposalBlockSet() {
-		return """", nil, utilTypes.ErrProposalBlockNotSet()
+		return utilTypes.ErrProposalBlockNotSet()
 	}
 
 	// begin block lifecycle phase
 	log.Debug().Msg(""calling beginBlock"")
 	if err := uow.beginBlock(); err != nil {
-		return """", nil, err
+		return err
 	}
 
 	log.Debug().Msg(""processing transactions from proposal block"")
 	txMempool := uow.GetBus().GetUtilityModule().GetMempool()
-	if err := uow.processTransactionsFromProposalBlock(txMempool, uow.proposalBlockTxs); err != nil {
-		return """", nil, err
+	if err := uow.processTransactionsFromProposalBlock(txMempool); err != nil {
+		return err
 	}
 
 	// end block lifecycle phase
 	log.Debug().Msg(""calling endBlock"")
 	if err := uow.endBlock(uow.proposalProposerAddr); err != nil {
-		return """", nil, err
+		return err
 	}
-	// TODO(@deblasis): this should be from a ReadContext (the ephemeral/staging one)
 	// return the app hash (consensus module will get the validator set directly)
 	log.Debug().Msg(""computing state hash"")
-	stateHash, err = uow.persistenceRWContext.ComputeStateHash()
+	stateHash, err := uow.persistenceRWContext.ComputeStateHash()
 	if err != nil {
-		log.Fatal().Err(err).Msg(""Updating the app hash failed. TODO: Look into roll-backing the entire commit..."")
-		return """", nil, utilTypes.ErrAppHash(err)
+		log.Fatal().Err(err).Bool(""TODO"", true).Msg(""Updating the app hash failed. TODO: Look into roll-backing the entire commit..."")
+		return utilTypes.ErrAppHash(err)
 	}
+
+	// IMPROVE(#655): this acts as a feature flag to allow tests to ignore the check if needed, ideally the tests should have a way to determine
+	// the hash and set it into the proposal block it's currently hard to do because the state is different at every test run (non-determinism)
+	if uow.proposalStateHash != IgnoreProposalBlockCheckHash {
+		if uow.proposalStateHash != stateHash {
+			log.Fatal().Bool(""TODO"", true).
+				Str(""proposalStateHash"", uow.proposalStateHash).
+				Str(""stateHash"", stateHash).
+				Msg(""State hash mismatch. TODO: Look into roll-backing the entire commit..."")
+			return utilTypes.ErrAppHash(fmt.Errorf(""state hash mismatch: expected %s from the proposal, got %s"", uow.proposalStateHash, stateHash))
+		}
+	}
+
 	log.Info().Str(""state_hash"", stateHash).Msgf(""ApplyBlock succeeded!"")
 
-	// return the app hash; consensus module will get the validator set directly
-	return stateHash, nil, nil
+	uow.stateHash = stateHash
+
+	return nil
 }
 
 // TODO(@deblasis): change tracking here
@@ -114,14 +130,16 @@

 	return nil
 }
 
+// isProposalBlockSet returns true if the proposal block has been set.
+// TODO: it should also check that uow.proposalBlockTxs is not empty but if we do, tests fail.
 func (uow *baseUtilityUnitOfWork) isProposalBlockSet() bool {
 	return uow.proposalStateHash != """" && uow.proposalProposerAddr != nil
 }
 
 // processTransactionsFromProposalBlock processes the transactions from the proposal block.
-// It also removes the transactions from the mempool if they are already present.
-func (uow *baseUtilityUnitOfWork) processTransactionsFromProposalBlock(txMempool mempool.TXMempool, txsBytes [][]byte) (err error) {
-	for index, txProtoBytes := range txsBytes {
+// It also removes the transactions from the mempool if they are also present.
+func (uow *baseUtilityUnitOfWork) processTransactionsFromProposalBlock(txMempool mempool.TXMempool) (err error) {
+	for index, txProtoBytes := range uow.proposalBlockTxs {
 		tx, err := coreTypes.TxFromBytes(txProtoBytes)
 		if err != nil {
 			return err
@@ -151,8 +169,13 @@

 
 		// TODO(#564): make sure that indexing is reversible in case of a rollback
 		if err := uow.persistenceRWContext.IndexTransaction(txResult); err != nil {
-			uow.logger.Fatal().Err(err).Msgf(""TODO(#327): We can apply the transaction but not index it. Crash the process for now: %v\n"", err)
+			uow.logger.Fatal().Err(err).Msg(""TODO(#327): We can apply the transaction but not index it. Crash the process for now"")
 		}
 	}
 	return nil
 }
+
+// GetStateHash returns the state hash of the unit of work. It is only available after the block has been applied.
+func (uow *baseUtilityUnitOfWork) GetStateHash() string {
+	return uow.stateHash
+}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1230111375,https://github.com/pokt-network/pocket/pull/756#discussion_r1230111375,,126,64586305e2aaeb40b5380dde8c922368e4ec8464,70a1a0e2fe2c238fce8c2019e13e9167629a2639,persistence/trees/trees.go,nan,"I suggest changing
 ```
+func (t *treeStore) DebugClearAll() error {
```
 to
```
+// TECHDEBT: Move this into a separate file with a debug build flag to avoid accidental usage in prod
+func (t *treeStore) DebugClearAll() error {
```","+
+// DebugClearAll is used by the debug cli to completely reset all merkle trees.
+// This should only be called by the debug CLI.
+func (t *treeStore) DebugClearAll() error {","--- 

+++ 

@@ -6,22 +6,24 @@

 
 import (
 	""bytes""
-	""context""
 	""crypto/sha256""
 	""encoding/hex""
 	""fmt""
+	""hash""
 
 	""github.com/jackc/pgx/v5""
 
 	""github.com/pokt-network/pocket/persistence/indexer""
 	""github.com/pokt-network/pocket/persistence/kvstore""
-	ptypes ""github.com/pokt-network/pocket/persistence/types""
+	""github.com/pokt-network/pocket/persistence/sql""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/smt""
 )
 
+var smtTreeHasher hash.Hash = sha256.New()
+
 var merkleTreeToString = map[merkleTree]string{
 	appMerkleTree:      ""app"",
 	valMerkleTree:      ""val"",
@@ -43,13 +45,6 @@

 	coreTypes.ActorType_ACTOR_TYPE_SERVICER: servicerMerkleTree,
 }
 
-var actorTypeToSchemaName = map[coreTypes.ActorType]ptypes.ProtocolActorSchema{
-	coreTypes.ActorType_ACTOR_TYPE_APP:      ptypes.ApplicationActor,
-	coreTypes.ActorType_ACTOR_TYPE_VAL:      ptypes.ValidatorActor,
-	coreTypes.ActorType_ACTOR_TYPE_FISH:     ptypes.FishermanActor,
-	coreTypes.ActorType_ACTOR_TYPE_SERVICER: ptypes.ServicerActor,
-}
-
 var merkleTreeToActorTypeName = map[merkleTree]coreTypes.ActorType{
 	appMerkleTree:      coreTypes.ActorType_ACTOR_TYPE_APP,
 	valMerkleTree:      coreTypes.ActorType_ACTOR_TYPE_VAL,
@@ -65,6 +60,8 @@

 	// defines the index of the root hash each independent as they are concatenated together
 	// to generate the state hash.
 
+	// TECHDEBT(#834): Remove the need for enforced ordering
+
 	// Actor Merkle Trees
 	appMerkleTree merkleTree = iota
 	valMerkleTree
@@ -86,7 +83,7 @@

 
 // treeStore stores a set of merkle trees that
 // it manages. It fulfills the modules.TreeStore interface.
-// * It is responsible for commit or rollback behavior
+// * It is responsible for atomic commit or rollback behavior
 // of the underlying trees by utilizing the lazy loading
 // functionality provided by the underlying smt library.
 type treeStore struct {
@@ -95,12 +92,12 @@

 	nodeStores   map[merkleTree]kvstore.KVStore
 }
 
-// Update takes a transaction and a height and updates
-// all of the trees in the treeStore for that height.
 func (t *treeStore) Update(pgtx pgx.Tx, txi indexer.TxIndexer, height uint64) (string, error) {
 	return t.updateMerkleTrees(pgtx, txi, height)
 }
 
+// NewStateTrees is the constructor object for a treeStore and initializes and configures a new
+// tree for the appropriate type of store, i.e. in-memory vs file system storage.
 func NewStateTrees(treesStoreDir string) (*treeStore, error) {
 	if treesStoreDir == "":memory:"" {
 		return newMemStateTrees()
@@ -118,22 +115,22 @@

 			return nil, err
 		}
 		stateTrees.nodeStores[tree] = nodeStore
-		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
+		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
 	}
 	return stateTrees, nil
 }
 
 // DebugClearAll is used by the debug cli to completely reset all merkle trees.
 // This should only be called by the debug CLI.
+// TECHDEBT: Move this into a separate file with a debug build flag to avoid accidental usage in prod
 func (t *treeStore) DebugClearAll() error {
 	for treeType := merkleTree(0); treeType < numMerkleTrees; treeType++ {
 		nodeStore := t.nodeStores[treeType]
 		if err := nodeStore.ClearAll(); err != nil {
 			return fmt.Errorf(""failed to clear %s node store: %w"", merkleTreeToString[treeType], err)
 		}
-		t.merkleTrees[treeType] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
-	}
-
+		t.merkleTrees[treeType] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
+	}
 	return nil
 }
 
@@ -146,13 +143,13 @@

 	for tree := merkleTree(0); tree < numMerkleTrees; tree++ {
 		nodeStore := kvstore.NewMemKVStore() // For testing, `smt.NewSimpleMap()` can be used as well
 		stateTrees.nodeStores[tree] = nodeStore
-		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, sha256.New())
+		stateTrees.merkleTrees[tree] = smt.NewSparseMerkleTree(nodeStore, smtTreeHasher)
 	}
 	return stateTrees, nil
 }
 
-// updateMerkleTrees updates all of the merkle trees that TreeStore manages.
-// * it returns an hash of the output or an error.
+// updateMerkleTrees updates all of the merkle trees in order defined by `numMerkleTrees`
+// * it returns the new state hash capturing the state of all the trees or an error if one occurred
 func (t *treeStore) updateMerkleTrees(pgtx pgx.Tx, txi indexer.TxIndexer, height uint64) (string, error) {
 	for treeType := merkleTree(0); treeType < numMerkleTrees; treeType++ {
 		switch treeType {
@@ -163,7 +160,7 @@

 				return """", fmt.Errorf(""no actor type found for merkle tree: %v"", treeType)
 			}
 
-			actors, err := t.getActorsUpdated(pgtx, actorType, height)
+			actors, err := sql.GetActors(pgtx, actorType, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get actors at height: %w"", err)
 			}
@@ -174,7 +171,7 @@

 
 		// Account Merkle Trees
 		case accountMerkleTree:
-			accounts, err := t.getAccounts(pgtx, height)
+			accounts, err := sql.GetAccounts(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get accounts: %w"", err)
 			}
@@ -182,7 +179,7 @@

 				return """", fmt.Errorf(""failed to update account trees: %w"", err)
 			}
 		case poolMerkleTree:
-			pools, err := t.getPools(pgtx, height)
+			pools, err := sql.GetPools(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get transactions: %w"", err)
 			}
@@ -192,7 +189,7 @@

 
 		// Data Merkle Trees
 		case transactionsMerkleTree:
-			indexedTxs, err := t.getTransactions(txi, height)
+			indexedTxs, err := sql.GetTransactions(txi, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get transactions: %w"", err)
 			}
@@ -200,7 +197,7 @@

 				return """", fmt.Errorf(""failed to update transactions: %w"", err)
 			}
 		case paramsMerkleTree:
-			params, err := t.getParams(pgtx, height)
+			params, err := sql.GetParams(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get params: %w"", err)
 			}
@@ -208,7 +205,7 @@

 				return """", fmt.Errorf(""failed to update params tree: %w"", err)
 			}
 		case flagsMerkleTree:
-			flags, err := t.getFlags(pgtx, height)
+			flags, err := sql.GetFlags(pgtx, height)
 			if err != nil {
 				return """", fmt.Errorf(""failed to get flags from transaction: %w"", err)
 			}
@@ -369,206 +366,3 @@

 
 	return nil
 }
-
-// getActorsUpdated is responsible for fetching the actors that have been updated at a given height.
-func (t *treeStore) getActorsUpdated(
-	pgtx pgx.Tx,
-	actorType coreTypes.ActorType,
-	height uint64,
-) ([]*coreTypes.Actor, error) {
-	actorSchema, ok := actorTypeToSchemaName[actorType]
-	if !ok {
-		return nil, fmt.Errorf(""no schema found for actor type: %s"", actorType)
-	}
-
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	query := actorSchema.GetUpdatedAtHeightQuery(int64(height))
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	addrs := make([][]byte, 0)
-	for rows.Next() {
-		var addr string
-		if err := rows.Scan(&addr); err != nil {
-			return nil, err
-		}
-		addrBz, err := hex.DecodeString(addr)
-		if err != nil {
-			return nil, err
-		}
-		addrs = append(addrs, addrBz)
-	}
-
-	actors := make([]*coreTypes.Actor, len(addrs))
-	for i, addr := range addrs {
-		// TECHDEBT #XXX: Avoid this cast to int64
-		actor, err := t.getActor(pgtx, actorSchema, addr, int64(height))
-		if err != nil {
-			return nil, err
-		}
-		actors[i] = actor
-	}
-	rows.Close()
-
-	return actors, nil
-}
-
-func (t *treeStore) getAccountsUpdated(
-	pgtx pgx.Tx,
-	acctType ptypes.ProtocolAccountSchema,
-	height uint64,
-) ([]*coreTypes.Account, error) {
-	accounts := []*coreTypes.Account{}
-
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	query := acctType.GetAccountsUpdatedAtHeightQuery(int64(height))
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	for rows.Next() {
-		acc := new(coreTypes.Account)
-		if err := rows.Scan(&acc.Address, &acc.Amount); err != nil {
-			return nil, err
-		}
-		accounts = append(accounts, acc)
-	}
-
-	return accounts, nil
-}
-
-func (t *treeStore) getTransactions(txi indexer.TxIndexer, height uint64) ([]*coreTypes.IndexedTransaction, error) {
-	// TECHDEBT (ISSUE #813): Avoid this cast to int64
-	// https://github.com/pokt-network/pocket/issues/813
-	indexedTxs, err := txi.GetByHeight(int64(height), false)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get transactions by height: %w"", err)
-	}
-	return indexedTxs, nil
-}
-
-// getPools returns the pools updated at the given height
-func (t *treeStore) getPools(pgtx pgx.Tx, height uint64) ([]*coreTypes.Account, error) {
-	pools, err := t.getAccountsUpdated(pgtx, ptypes.Pool, height)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get pools: %w"", err)
-	}
-	return pools, nil
-}
-
-// getAccounts returns the list of accounts updated at the provided height
-func (t *treeStore) getAccounts(pgtx pgx.Tx, height uint64) ([]*coreTypes.Account, error) {
-	accounts, err := t.getAccountsUpdated(pgtx, ptypes.Account, height)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get accounts: %w"", err)
-	}
-	return accounts, nil
-}
-
-func (t *treeStore) getFlags(pgtx pgx.Tx, height uint64) ([]*coreTypes.Flag, error) {
-	fields := ""name,value,enabled""
-	query := fmt.Sprintf(""SELECT %s FROM %s WHERE height=%d ORDER BY name ASC"", fields, ptypes.FlagsTableName, height)
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, fmt.Errorf(""failed to get flags: %w"", err)
-	}
-	defer rows.Close()
-
-	flagSlice := []*coreTypes.Flag{}
-	for rows.Next() {
-		flag := new(coreTypes.Flag)
-		if err := rows.Scan(&flag.Name, &flag.Value, &flag.Enabled); err != nil {
-			return nil, err
-		}
-		flag.Height = int64(height)
-		flagSlice = append(flagSlice, flag)
-	}
-
-	return flagSlice, nil
-}
-
-func (t *treeStore) getParams(pgtx pgx.Tx, height uint64) ([]*coreTypes.Param, error) {
-	fields := ""name,value""
-	query := fmt.Sprintf(""SELECT %s FROM %s WHERE height=%d ORDER BY name ASC"", fields, ptypes.ParamsTableName, height)
-	rows, err := pgtx.Query(context.TODO(), query)
-	if err != nil {
-		return nil, err
-	}
-	defer rows.Close()
-
-	var paramSlice []*coreTypes.Param
-	for rows.Next() {
-		param := new(coreTypes.Param)
-		if err := rows.Scan(&param.Name, &param.Value); err != nil {
-			return nil, err
-		}
-		param.Height = int64(height)
-		paramSlice = append(paramSlice, param)
-	}
-
-	return paramSlice, nil
-}
-
-func (t *treeStore) getActor(tx pgx.Tx, actorSchema ptypes.ProtocolActorSchema, address []byte, height int64) (actor *coreTypes.Actor, err error) {
-	ctx := context.TODO()
-	actor, height, err = t.getActorFromRow(actorSchema.GetActorType(), tx.QueryRow(ctx, actorSchema.GetQuery(hex.EncodeToString(address), height)))
-	if err != nil {
-		return
-	}
-	return t.getChainsForActor(ctx, tx, actorSchema, actor, height)
-}
-
-func (t *treeStore) getActorFromRow(actorType coreTypes.ActorType, row pgx.Row) (actor *coreTypes.Actor, height int64, err error) {
-	actor = &coreTypes.Actor{
-		ActorType: actorType,
-	}
-	err = row.Scan(
-		&actor.Address,
-		&actor.PublicKey,
-		&actor.StakedAmount,
-		&actor.ServiceUrl,
-		&actor.Output,
-		&actor.PausedHeight,
-		&actor.UnstakingHeight,
-		&height)
-	return
-}
-
-func (t *treeStore) getChainsForActor(
-	ctx context.Context,
-	tx pgx.Tx,
-	actorSchema ptypes.ProtocolActorSchema,
-	actor *coreTypes.Actor,
-	height int64,
-) (a *coreTypes.Actor, err error) {
-	if actorSchema.GetChainsTableName() == """" {
-		return actor, nil
-	}
-	rows, err := tx.Query(ctx, actorSchema.GetChainsQuery(actor.Address, height))
-	if err != nil {
-		return actor, err
-	}
-	defer rows.Close()
-
-	var chainAddr string
-	var chainID string
-	var chainEndHeight int64 // DISCUSS: why is this commented as ""unused""?
-	for rows.Next() {
-		err = rows.Scan(&chainAddr, &chainID, &chainEndHeight)
-		if err != nil {
-			return
-		}
-		if chainAddr != actor.Address {
-			return actor, fmt.Errorf(""unexpected address %s, expected %s when reading chains"", chainAddr, actor.Address)
-		}
-		actor.Chains = append(actor.Chains, chainID)
-	}
-	return actor, nil
-}",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829318550,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1829318550,405,406,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,nan,"I suggest changing
 ```
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
+                    axis=1,
```
 to
```
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities}, axis=1
```
Revert black","+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
+                    axis=1,","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -74,19 +73,18 @@

         ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
             data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -171,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -186,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -201,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -213,46 +214,30 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
@@ -264,7 +249,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -288,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -310,8 +297,9 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
@@ -333,27 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -388,7 +396,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
@@ -407,13 +415,7 @@

                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -452,18 +454,12 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820596662,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1820596662,,71,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/__init__.py,nan,bare-except (E722) fixed," 
         streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
-    except:
+    except ImportError:","--- 

+++ 

@@ -1,10 +1,13 @@

 """"""DataJoint pipeline for Aeon.""""""
 
 import hashlib
+import logging
 import os
 import uuid
 
 import datajoint as dj
+
+logger = dj.logger
 
 _default_database_prefix = os.getenv(""DJ_DB_PREFIX"") or ""aeon_""
 _default_repository_config = {""ceph_aeon"": ""/ceph/aeon""}
@@ -15,9 +18,7 @@

 
 db_prefix = dj.config[""custom""].get(""database.prefix"", _default_database_prefix)
 
-repository_config = dj.config[""custom""].get(
-    ""repository_config"", _default_repository_config
-)
+repository_config = dj.config[""custom""].get(""repository_config"", _default_repository_config)
 
 
 def get_schema_name(name) -> str:
@@ -27,24 +28,28 @@

 
 def dict_to_uuid(key) -> uuid.UUID:
     """"""Given a dictionary `key`, returns a hash string as UUID.""""""
-    hashed = hashlib.sha256()
+    hashed = hashlib.md5()
     for k, v in sorted(key.items()):
         hashed.update(str(k).encode())
         hashed.update(str(v).encode())
-    return uuid.UUID(hex=hashed.hexdigest()[:32])
+    return uuid.UUID(hex=hashed.hexdigest())
 
 
-def fetch_stream(query, drop_pk=True):
+def fetch_stream(query, drop_pk=True, round_microseconds=True):
     """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
 
     Provided a query containing data from a Stream table,
     fetch and aggregate the data into one DataFrame indexed by ""time""
+
+    Args:
+        query (datajoint.Query): A query object containing data from a Stream table
+        drop_pk (bool, optional): Drop primary key columns. Defaults to True.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to True.
+            (this is important as timestamps in mysql is only accurate to microseconds)
     """"""
     df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
     cols2explode = [
-        c
-        for c in query.heading.secondary_attributes
-        if query.heading.attributes[c].type == ""longblob""
+        c for c in query.heading.secondary_attributes if query.heading.attributes[c].type == ""longblob""
     ]
     df = df.explode(column=cols2explode)
     cols2drop = [""sample_count""] + (query.primary_key if drop_pk else [])
@@ -56,8 +61,12 @@

         convert_string=False,
         convert_integer=False,
         convert_boolean=False,
-        convert_floating=False,
+        convert_floating=False
     )
+    if not df.empty and round_microseconds:
+        logging.warning(""Rounding timestamps to microseconds is now enabled by default.""
+                        "" To disable, set round_microseconds=False."")
+        df.index = df.index.round(""us"")
     return df
 
 
@@ -68,5 +77,5 @@

         from .utils import streams_maker
 
         streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
-    except ImportError:
-        pass
+    except Exception as e:
+        logger.debug(f""Could not import streams module: {e}"")",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1766872761,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/402#discussion_r1766872761,,47,6bacc43e93826f9a3ffa8e1c8c9189abc0bf4c14,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/populate/worker.py,nan,Remove also https://github.com/SainsburyWellcomeCentre/aeon_mecha/blob/6bacc43e93826f9a3ffa8e1c8c9189abc0bf4c14/aeon/dj_pipeline/populate/worker.py#L59,"         acquisition.Chunk.ingest_chunks(experiment_name)
 
 
-def ingest_environment_visits():","--- 

+++ 

@@ -56,7 +56,6 @@

 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
-# acquisition_worker(ingest_environment_visits)
 acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228625570,https://github.com/pokt-network/pocket/pull/803#discussion_r1228625570,,10,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,consensus/doc/CHANGELOG.md,nan,"This is unrelated to your changes, please fix/revert."," ## [Unreleased]
 
-## [0.0.0.53] - 2023-06-08
+## [0.0.0.53] - 2023-06-12","--- 

+++ 

@@ -7,7 +7,11 @@

 
 ## [Unreleased]
 
-## [0.0.0.53] - 2023-06-12
+## [0.0.0.54] - 2023-06-13
+
+- Fix tests
+
+## [0.0.0.53] - 2023-06-08
 
 - Add consensus README
 ",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829153616,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1829153616,,37,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,nan,"I suggest changing
 ```
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
```
 to
```
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
```
To get rid of noqa: E501","+    definition = """""" # Catalog of all stream types used across Project Aeon
     stream_type          : varchar(20)
     ---
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1179898995,https://github.com/pokt-network/pocket/pull/699#discussion_r1179898995,,45,3f3609683964db22399882fbba4fcc0c3fcb3a09,37bc0a72df29bb6524cbd6b8274a82ad366e84f8,shared/k8s/debug.go,nan,Move into a constant," }
+
+func getNamespace() (string, error) {
+	nsFile := ""/var/run/secrets/kubernetes.io/serviceaccount/namespace""","--- 

+++ 

@@ -1,3 +1,4 @@

+// // CONSIDERATION: Add a debug tag
 package k8s
 
 import (
@@ -12,6 +13,8 @@

 
 //nolint:gosec // G101 Not a credential
 const privateKeysSecretResourceName = ""validators-private-keys""
+const kubernetesServiceAccountNamespaceFile = ""/var/run/secrets/kubernetes.io/serviceaccount/namespace""
+const defaultNamespace = ""default""
 
 var CurrentNamespace = """"
 
@@ -19,11 +22,11 @@

 	var err error
 	CurrentNamespace, err = getNamespace()
 	if err != nil {
-		logger.Global.Err(err).Msg(""could not get namespace, using default"")
-		CurrentNamespace = ""default""
+		logger.Global.Err(err).Msg(""could not get namespace, using \"""" + defaultNamespace + ""\"""")
+		CurrentNamespace = defaultNamespace
 	}
 
-	logger.Global.Info().Str(""namespace"", CurrentNamespace).Msg(""using namespace"")
+	logger.Global.Info().Str(""namespace"", CurrentNamespace).Msg(""got new namespace"")
 }
 
 func FetchValidatorPrivateKeys(clientset *kubernetes.Clientset) (map[string]string, error) {
@@ -42,15 +45,15 @@

 }
 
 func getNamespace() (string, error) {
-	nsFile := ""/var/run/secrets/kubernetes.io/serviceaccount/namespace""
-
-	if _, err := os.Stat(nsFile); err == nil {
-		nsBytes, err := os.ReadFile(nsFile)
-		if err != nil {
-			return """", fmt.Errorf(""could not read namespace file: %v"", err)
-		}
-		return string(nsBytes), nil
+	_, err := os.Stat(kubernetesServiceAccountNamespaceFile)
+	if err != nil {
+		return defaultNamespace, nil
 	}
 
-	return ""default"", nil
+	nsBytes, err := os.ReadFile(kubernetesServiceAccountNamespaceFile)
+	if err != nil {
+		return """", fmt.Errorf(""could not read namespace file: %v"", err)
+	}
+
+	return string(nsBytes), nil
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228638984,https://github.com/pokt-network/pocket/pull/803#discussion_r1228638984,,247,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,shared/modules/persistence_module.go,nan,"Field #7 of `Session` is the application, so we don't need to pass in the `appAddr` separately.","+	//	b) prepare claim/proof messages once the session is over
+	// The ""relayDigest"" and ""relayReqResBytes"" parameters will be used as key and leaf contents in the constructed SMT, respectively.
+	// OPTIMIZE: both the relay and the response can be large structures: we may need to truncate the stored values
+	StoreServiceRelay(session *coreTypes.Session, appAddr string, relayDigest, relayReqResBytes []byte) error","--- 

+++ 

@@ -20,27 +20,31 @@

 
 	// Context operations
 	NewRWContext(height int64) (PersistenceRWContext, error)
-	// DISCUSS: removing height from ""NewReadContext"" input and passing it to specific methods seems a better choice.
+	// TODO(#406): removing height from ""NewReadContext"" input and passing it to specific methods seems a better choice.
 	//	This could prevent confusion when retrieving the value of a parameter for a height less than the current height,
 	//		e.g. when getting the App Token sessions multiplier for the starting height of a session.
 	NewReadContext(height int64) (PersistenceReadContext, error)
 	ReleaseWriteContext() error // The module can maintain many read contexts, but only one write context can exist at a time
 
-	// BlockStore operations
+	// BlockStore maps a block height to an *coreTypes.IndexedTransaction
 	GetBlockStore() blockstore.BlockStore
+
 	NewWriteContext() PersistenceRWContext
 
 	// Indexer operations
 	GetTxIndexer() indexer.TxIndexer
 	TransactionExists(transactionHash string) (bool, error)
 
+	// TreeStore operations
+	GetTreeStore() TreeStoreModule
+
 	// Debugging / development only
 	HandleDebugMessage(*messaging.DebugMessage) error
 
-	// NewLocalContext returns a local persistence context that can be used to store/retrieve node-specific, i.e. off-chain, data
-	// The module can maintain a single local context for both read and write operations: subsequent calls to NewLocalContext return
+	// GetLocalContext returns a local persistence context that can be used to store/retrieve node-specific, i.e. off-chain, data
+	// The module can maintain a single (i.e. a singleton) local context for both read and write operations: subsequent calls to GetLocalContext return
 	// the same local context.
-	NewLocalContext() (PersistenceLocalContext, error)
+	GetLocalContext() (PersistenceLocalContext, error)
 }
 
 // Interface defining the context within which the node can operate with the persistence layer.
@@ -238,17 +242,14 @@

 //	This context should be used for node-specific data, e.g. records of served relays.
 //	This is in contrast to PersistenceRWContext which should be used to store on-chain data.
 type PersistenceLocalContext interface {
-	// StoreServiceRelay stores record of a serviced relay and its response in the local context.
+	// StoreServicedRelay stores record of a serviced relay and its response in the local context.
 	// The stored service relays will be used to:
 	//	a) check the number of tokens used per session, and
 	//	b) prepare claim/proof messages once the session is over
 	// The ""relayDigest"" and ""relayReqResBytes"" parameters will be used as key and leaf contents in the constructed SMT, respectively.
-	// OPTIMIZE: both the relay and the response can be large structures: we may need to truncate the stored values
-	StoreServiceRelay(session *coreTypes.Session, appAddr string, relayDigest, relayReqResBytes []byte) error
-	// GetSessionTokensUsed returns the number of tokens that have been used for the passed session.
-	//    This gets calculated from local storage, so it returns the count of tokens used by the servicer instance
+	StoreServicedRelay(session *coreTypes.Session, relayDigest, relayReqResBytes []byte) error
+	// GetSessionTokensUsed returns the number of tokens that have been used for the provided session.
+	//    It returns the count of tokens used by the servicer instance
 	//	for the application associated with the session
 	GetSessionTokensUsed(*coreTypes.Session) (*big.Int, error)
-	// Release the allocated local context.
-	Release() error
-}
+}",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828170826,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1828170826,,255,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,nan,"I suggest changing
 ```
+    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
```
 to
```
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
```"," def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
+    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1773764875,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/398#discussion_r1773764875,,184,c69b78ea3a84abff51985f2f506670f854571b04,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,nan,Should we exclude Dummy patches here or is this already handled elsewhere and we can assume dummy patches will never be fetched?,"@@ -188,37 +183,14 @@ def make(self, key):
         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")","--- 

+++ 

@@ -256,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,8 +319,7 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
 
         self.insert1(
             {
@@ -540,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -722,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: : {patch_mean}"",
                         )
                     )
@@ -744,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -762,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -803,12 +801,10 @@

         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
-
     Args:
         patch_key (dict): primary key for the patch
         start (datetime): start timestamp
         end (datetime): end timestamp
-
     Returns:
         pd.DataFrame: DataFrame with the following columns:
         - threshold_update_timestamp (index)
@@ -819,42 +815,40 @@

     """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
-    # pellet delivery and beam break data
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
-    beam_break_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
         start:end
     ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
     manual_delivery_df = fetch_stream(
         streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
     )[start:end]
-
-    # exclude ManualDelivery from the pellet delivery (take the not intersecting part)
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    if delivered_pellet_df.empty or beam_break_df.empty:
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
         return acquisition.io_api._empty(
             [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
         )
 
-    # patch threshold data
-    depletion_state_df = fetch_stream(
-        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
-    )[start:end]
-    # remove NaNs from threshold column
-    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # remove invalid rows where the time difference is less than 1 second
-    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
-    depletion_state_df = depletion_state_df[~invalid_rows]
-
-    # find pellet times with matching beam break times (within 1.2s after pellet times)
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
     pellet_beam_break_df = (
         pd.merge_asof(
             delivered_pellet_df.reset_index(),
-            beam_break_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
             tolerance=pd.Timedelta(""1.2s""),
@@ -865,7 +859,17 @@

     )
     pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
-    # find pellet times approximately coincide with each threshold update
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
@@ -879,13 +883,11 @@

         .set_index(""time"")
         .dropna(subset=[""pellet_timestamp""])
     )
+
+    # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
     pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
-        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
-    )
-
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
     return pellet_ts_threshold_df",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1205997356,https://github.com/pokt-network/pocket/pull/778#discussion_r1205997356,,255,b93c50312e4967daff683ac9fd577e4e5edc2a3b,a516c51f2d000d25f1469d28e6586ef7875d5a37,app/client/cli/servicer.go,nan,We have a helper in `shared/crypto/sha3.go` you might be able to use. Can you see if it can be leveraged?,"+	return relay, nil
+}
+
+func hash(data []byte) ([]byte, error) {","--- 

+++ 

@@ -2,7 +2,6 @@

 
 import (
 	""context""
-	sha ""crypto""
 	""encoding/hex""
 	""encoding/json""
 	""fmt""
@@ -10,6 +9,7 @@

 
 	""github.com/spf13/cobra""
 
+	""github.com/pokt-network/pocket/app/client/cli/flags""
 	""github.com/pokt-network/pocket/rpc""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
@@ -19,7 +19,6 @@

 	rootCmd.AddCommand(NewServicerCommand())
 }
 
-// TECHDEBT: (unittest) unit test the command: e.g. on number of arguments
 func NewServicerCommand() *cobra.Command {
 	cmd := &cobra.Command{
 		Use:     ""Servicer"",
@@ -45,25 +44,36 @@

 		newUnstakeCmd(cmdDef),
 		newUnpauseCmd(cmdDef),
 		{
-			Use:   ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Short: ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Long: `Sends a trustless relay using <payload> as contents, to the specified active <servicer> in the the <application>'s session.
+			// IMPROVE: allow reading the relay payload from a file with the serialized protobuf via [--input_file]
+			Use:   ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Short: ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Long: `Sends a trustless relay using <relayPayload> as contents, to the specified active <servicerAddrHex> in the the <applicationAddrHex>'s session.
 Will prompt the user for the *application* account passphrase`,
 			Aliases: []string{},
 			Args:    cobra.ExactArgs(4),
 			RunE: func(cmd *cobra.Command, args []string) error {
-				servicerAddr := args[0]
-				applicationAddr := args[1]
+				applicationAddr := args[0]
+				servicerAddr := args[1]
 				chain := args[2]
 				relayPayload := args[3]
 
-				// TODO: (SUGGESTION) refactor to decouple the client logic from the CLI/command
-				pk, err := getPrivateKey(applicationAddr)
+				// REFACTOR: decouple the client logic from the CLI
+				//	The client will: send the trustless relay and return the response (using a single function as entrypoint)
+				//	The CLI will:
+				//		1) extract the required input from the command arguments
+				//		2) call the client function (with the inputs above) that performs the trustless relay
+				pk, err := getPrivateKeyFromKeybase(applicationAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting application's private key: %w"", err)
 				}
 
-				session, servicer, err := fetchServicer(cmd.Context(), applicationAddr, chain, servicerAddr)
+				// TECHDEBT(#791): cache session data
+				session, err := getCurrentSession(cmd.Context(), applicationAddr, chain)
+				if err != nil {
+					return fmt.Errorf(""Error getting current session: %w"", err)
+				}
+
+				servicer, err := validateServicer(session, servicerAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting servicer for the relay: %w"", err)
 				}
@@ -73,7 +83,7 @@

 					return fmt.Errorf(""error building relay from payload: %w"", err)
 				}
 
-				fmt.Printf(""sending trustless relay for %s to %v with payload: %s\n"", applicationAddr, servicer, relayPayload)
+				fmt.Printf(""sending trustless relay for %s to %s with payload: %s\n"", applicationAddr, servicerAddr, relayPayload)
 
 				resp, err := sendTrustlessRelay(cmd.Context(), servicer.ServiceUrl, relay)
 				if err != nil {
@@ -91,59 +101,22 @@

 	return cmds
 }
 
-// TODO: (QUESTION): do we need/want a cli subcommand for fetching servicers?
-
-// fetchServicer returns the servicer specified by the <servicer> argument.
-// It validates the following conditions:
-//
-//	A. The <application> argument is the address of an active application
-//	B. The <servicer> is the address of a servicer that is active in the application's current session.
-//
-// TODO: (SUGGESTION) use a package-internal interface for servicer and application?
-// TODO: (SUGGESTION) use a struct as input to combine all fields (same for output)
-func fetchServicer(ctx context.Context, appAddress, chain, servicerAddress string) (rpc.Session, rpc.ProtocolActor, error) {
-	// TECHDEBT: cache session data
-	session, err := getCurrentSession(ctx, appAddress, chain)
-	if err != nil {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: %w"", err)
-	}
-
-	var (
-		servicer rpc.ProtocolActor
-		found    bool
-	)
-	// TODO: a map may be a better choice for storing servicers
-	for _, s := range session.Servicers {
-		if s.Address == servicerAddress {
-			servicer = s
-			found = true
-			break
+// TODO: add a cli command for fetching sessions
+// validateServicer returns the servicer specified by the <servicer> argument.
+// It validates that the <servicer> is the address of a servicer that is active in the current session.
+func validateServicer(session *rpc.Session, servicerAddress string) (*rpc.ProtocolActor, error) {
+	for i := range session.Servicers {
+		if session.Servicers[i].Address == servicerAddress {
+			return &session.Servicers[i], nil
 		}
 	}
 
-	// TODO: cover with unit tests
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session"", servicerAddress)
-	}
-
-	// TODO: cover with unit tests
-	found = false
-	for _, ch := range servicer.Chains {
-		if ch == chain {
-			found = true
-			break
-		}
-	}
-
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: service %s does not support chain %s"", servicerAddress, chain)
-	}
-
-	return *session, servicer, nil
+	// ADDTEST: cover with gherkin tests
+	return nil, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session %d"", servicerAddress, session.SessionNumber)
 }
 
 func getCurrentSession(ctx context.Context, appAddress, chain string) (*rpc.Session, error) {
-	// TODO: passing 0 as the height value to get the current session seems more optimal than this.
+	// CONSIDERATION: passing 0 as the height value to get the current session seems more optimal than this.
 	currentHeight, err := getCurrentHeight(ctx)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session: %w"", err)
@@ -152,11 +125,11 @@

 	req := rpc.SessionRequest{
 		AppAddress: appAddress,
 		Chain:      chain,
-		// TODO: Geozone
+		// TODO(#697): Geozone
 		SessionHeight: currentHeight,
 	}
 
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session for app/chain/height: %s/%s/%d: %w"", appAddress, chain, currentHeight, err)
 	}
@@ -165,7 +138,8 @@

 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session with request %v: %w"", req, err)
 	}
-	// TODO: refactor boiler-plate code
+
+	// CLEANUP: move the HTTP response processing code to a separate function to enable reuse.
 	if resp.HTTPResponse.StatusCode != http.StatusOK {
 		return nil, fmt.Errorf(""Error getting current session: Unexpected status code %d for request %v"", resp.HTTPResponse.StatusCode, req)
 	}
@@ -177,9 +151,9 @@

 	return resp.JSON200, nil
 }
 
-// TODO: reuse this function in the query commands
+// REFACTOR: reuse this function in all the query commands
 func getCurrentHeight(ctx context.Context) (int64, error) {
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return 0, fmt.Errorf(""Error getting current height: %w"", err)
 	}
@@ -198,51 +172,46 @@

 	return resp.JSON200.Height, nil
 }
 
-// TODO: (localnet) Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
-// TODO: (REFACTOR) should we move package-level variables (e.g. remoteCLIURL) to a cli object?
-func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
+// IMPROVE(#823): [K8s][LocalNet] Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
+// CONSIDERATION: move package-level variables (e.g. RemoteCLIURL) to a cli object and consider storing it in the context
+func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay *rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
 	client, err := rpc.NewClientWithResponses(servicerUrl)
 	if err != nil {
 		return nil, err
 	}
 
-	return client.PostV1ClientRelayWithResponse(ctx, relay)
-}
-
-// TODO: (NICE) allow reading the relay request from the command line arguments AND from a file
-func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session rpc.Session, servicer rpc.ProtocolActor) (rpc.RelayRequest, error) {
+	return client.PostV1ClientRelayWithResponse(ctx, *relay)
+}
+
+func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session *rpc.Session, servicer *rpc.ProtocolActor) (*rpc.RelayRequest, error) {
 	// TECHDEBT: This is mostly COPIED from pocket-go: we should refactor pocket-go code and import this functionality from there instead.
 	relayPayload := rpc.Payload{
-		Data:   payload,
-		Method: ""POST"",
-		// TODO: Path: load Path field from the corresponding Blockchain (e.g. database)
-		// TODO: set Headers
+		// INCOMPLETE(#803): need to unmarshal into JSONRPC and other supported relay formats once proto-generated custom types are added.
+		Jsonrpc: ""2.0"",
+		Method:  payload,
+		// INCOMPLETE: set Headers for HTTP relays
 	}
 
 	relayMeta := rpc.RelayRequestMeta{
 		BlockHeight: session.SessionHeight,
-		// TODO: use Identifiable for Chain in Session (or string for Chain in Relay Meta)
+		// TODO: Make Chain Identifier type consistent in Session and Meta use Identifiable for Chain in Session (or string for Chain in Relay Meta)
 		Chain: rpc.Identifiable{
 			Id: session.Chain,
 		},
 		ServicerPubKey: servicer.PublicKey,
-		// TODO: Geozone
-		// TODO: Token
-	}
-
-	relay := rpc.RelayRequest{
+		// TODO(#697): Geozone
+	}
+
+	relay := &rpc.RelayRequest{
 		Payload: relayPayload,
 		Meta:    relayMeta,
-		// TODO: (QUESTION) why is there no Proof field in v1 struct?
-	}
+	}
+	// TECHDEBT: Evaluate which fields we should and shouldn't marshall when signing the payload
 	reqBytes, err := json.Marshal(relay)
 	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
-	}
-	hashedReq, err := hash(reqBytes)
-	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error hashing relay request bytes %s: %w"", string(reqBytes), err)
-	}
+		return nil, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
+	}
+	hashedReq := crypto.SHA3Hash(reqBytes)
 	signature, err := appPrivateKey.Sign(hashedReq)
 	if err != nil {
 		return relay, fmt.Errorf(""Error signing relay: %w"", err)
@@ -252,23 +221,14 @@

 	return relay, nil
 }
 
-func hash(data []byte) ([]byte, error) {
-	hasher := sha.SHA3_256.New()
-	if _, err := hasher.Write(data); err != nil {
-		return nil, fmt.Errorf(""Error hashing data: %w"", err)
-	}
-
-	return hasher.Sum(nil), nil
-}
-
-// TODO: remove use of package-level variables
-func getPrivateKey(address string) (crypto.PrivateKey, error) {
+// TECHDEBT: remove use of package-level variables, such as NonInteractive, RemoteCLIURL, pwd, etc.
+func getPrivateKeyFromKeybase(address string) (crypto.PrivateKey, error) {
 	kb, err := keybaseForCLI()
 	if err != nil {
 		return nil, err
 	}
 
-	if !nonInteractive {
+	if !flags.NonInteractive {
 		pwd = readPassphrase(pwd)
 	}
 ",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1243077644,https://github.com/pokt-network/pocket/pull/732#discussion_r1243077644,,19,8467f3a832ae26b48ed9ec481a8b99355c849c09,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/testutil.go,nan,"I really like this debug comment approach. Worthwhile adding elsewhere, just in case ;)","+		mod, ok := m.(*p2pModule)
+		if ok {
+			mod.host = host
+			mod.logger.Debug().Msg(""using host provided via `WithHost`"")","--- 

+++ 

@@ -21,20 +21,28 @@

 	}
 }
 
+// WithUnstakedActorRouter assigns the given router to the P2P modules
+// `#unstakedActor` field, used to communicate between unstaked actors
+// and the rest of the network, plus as a redundancy to the staked actor
+// router when broadcasting.
 func WithUnstakedActorRouter(router typesP2P.Router) modules.ModuleOption {
 	return func(m modules.InitializableModule) {
 		mod, ok := m.(*p2pModule)
 		if ok {
 			mod.unstakedActorRouter = router
+			mod.logger.Debug().Msg(""using unstaked actor router provided via `WithUnstakeActorRouter`"")
 		}
 	}
 }
 
+// WithStakedActorRouter assigns the given router to the P2P modules'
+// `#stakedActor` field, exclusively used to communicate between staked actors.
 func WithStakedActorRouter(router typesP2P.Router) modules.ModuleOption {
 	return func(m modules.InitializableModule) {
 		mod, ok := m.(*p2pModule)
 		if ok {
 			mod.stakedActorRouter = router
+			mod.logger.Debug().Msg(""using staked actor router provided via `WithStakeActorRouter`"")
 		}
 	}
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160316336,https://github.com/pokt-network/pocket/pull/622#discussion_r1160316336,,49,010c045d060cf8114d09066846c1f7d1fe5f7318,6b9b2db0f25b5fb605c1cb6d454b47708ff4eacd,shared/utils/gov_utils.go,nan,Does this exist?,"+		poktValType := extractStructTag(poktTag, ""val_type="")
+		poktOwner := extractStructTag(poktTag, ""owner="")
+		golangType := field.Type.Name() // Get string version of field's Golang type
+		protoName := extractStructTag(protoTag, ""name="")","--- 

+++ 

@@ -7,7 +7,7 @@

 	""github.com/pokt-network/pocket/runtime/genesis""
 )
 
-// init initializes a map that contains the metadata extracted from `gov.proto`.
+// init initializes a map that contains the metadata extracted from genesis.proto.
 // Since protobuf files do not change at runtime, it seems efficient to do it here.
 func init() {
 	GovParamMetadataMap = parseGovProto()
@@ -20,13 +20,12 @@

 
 type GovParamMetadata struct {
 	PropertyName string
-	ParamName    string
 	ParamOwner   string
 	PoktType     string
 	GoType       string
 }
 
-// parseGovProto parses genesis.Params{} (generated from gov.proto) in order to extract metadata about its fields.
+// parseGovProto parses genesis.Params{} (generated from genesis.proto) in order to extract metadata about its fields.
 //
 // The metadata comes in the form of struct tags that we attached to gov.proto and also from the tags that protoc injects automatically.
 // Since currently we need to specify a mapping between the fields and a custom enum in the database (and potentially other things as well in the future),
@@ -45,14 +44,11 @@

 		protoTag := field.Tag.Get(""protobuf"")
 		poktValType := extractStructTag(poktTag, ""val_type="")
 		poktOwner := extractStructTag(poktTag, ""owner="")
-		golangType := field.Type.Name() // Get string version of field's Golang type
 		protoName := extractStructTag(protoTag, ""name="")
 		govParamMetadataMap[protoName] = GovParamMetadata{
 			PropertyName: field.Name,
-			ParamName:    protoName,
 			ParamOwner:   poktOwner,
 			PoktType:     poktValType,
-			GoType:       golangType,
 		}
 		GovParamMetadataKeys = append(GovParamMetadataKeys, protoName)
 	}
@@ -71,5 +67,5 @@

 		}
 		structTag = strings.TrimPrefix(structTag[i:], "","")
 	}
-	return """"
+	return """" // key not found
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184439797,https://github.com/pokt-network/pocket/pull/707#discussion_r1184439797,,144,4159d7a40f8d99835d17a00a052b6782cb3f795d,d7cc85d553cf9b0257f16c3a213ed4de96b8363a,p2p/background/router.go,nan,Looking forward to the day that we run into this in testing __ ,"+// AddPeer implements the respective `typesP2P.Router` interface  method.
+func (rtr *backgroundRouter) AddPeer(peer typesP2P.Peer) error {
+	// Noop if peer with the pokt address already exists in the peerstore.
+	// TECHDEBT: add method(s) to update peers.","--- 

+++ 

@@ -11,6 +11,7 @@

 	libp2pHost ""github.com/libp2p/go-libp2p/core/host""
 
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/p2p/config""
 	""github.com/pokt-network/pocket/p2p/protocol""
 	typesP2P ""github.com/pokt-network/pocket/p2p/types""
 	""github.com/pokt-network/pocket/p2p/utils""
@@ -38,23 +39,27 @@

 	// (i.e. multiple, unidentified receivers)
 	// TECHDEBT: investigate diff between randomSub and gossipSub
 	gossipSub *pubsub.PubSub
-	// topic similar to pubsub but received messages are filtered by a ""topic"" string.
+	// topic is similar to pubsub but received messages are filtered by a ""topic"" string.
 	// Published messages are also given the respective topic before broadcast.
 	topic *pubsub.Topic
 	// subscription provides an interface to continuously read messages from.
 	subscription *pubsub.Subscription
-	kadDHT       *dht.IpfsDHT
-	pstore       typesP2P.Peerstore
+	// kadDHT is a kademlia distributed hash table used for routing and peer discovery.
+	kadDHT *dht.IpfsDHT
+	// TECHDEBT: `pstore` will likely be removed in future refactoring / simplification
+	// of the `Router` interface.
+	// pstore is the background router's peerstore.
+	pstore typesP2P.Peerstore
 }
 
 // NewBackgroundRouter returns a `backgroundRouter` as a `typesP2P.Router`
 // interface using the given configuration.
-func NewBackgroundRouter(bus modules.Bus, cfg *utils.RouterConfig) (typesP2P.Router, error) {
+func NewBackgroundRouter(bus modules.Bus, cfg *config.BackgroundConfig) (typesP2P.Router, error) {
 	// TECHDEBT(#595): add ctx to interface methods and propagate down.
 	ctx := context.TODO()
 
 	networkLogger := logger.Global.CreateLoggerForModule(""backgroundRouter"")
-	networkLogger.Info().Msg(""Initializing background"")
+	networkLogger.Info().Msg(""Initializing background router"")
 
 	// seed initial peerstore with current on-chain peer info (i.e. staked actors)
 	pstore, err := cfg.PeerstoreProvider.GetStakedPeerstoreAtHeight(
@@ -64,7 +69,7 @@

 		return nil, err
 	}
 
-	// NOTE_TO_SELF: `pubsub.NewRandomSub` requires a `size` arg.
+	// CONSIDERATION: If switching to `NewRandomSub`, there will be a max size
 	gossipSub, err := pubsub.NewGossipSub(ctx, cfg.Host)
 	if err != nil {
 		return nil, fmt.Errorf(""creating gossip pubsub: %w"", err)",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/QuickSwap/interface-v2/pulls/comments/1188377629,https://github.com/QuickSwap/interface-v2/pull/798#discussion_r1188377629,,14,699e03aa920964b9a20f3af9a5be2f7646d61fdb,3a854bec2ebd84162ad6fe1be7b4fbc567dd0682,src/lib/src/swapRouter.ts,nan,remove it," import { Trade } from './trade';
 import { PermitOptions, SelfPermit } from './selfPermit';
 import { MethodParameters, toHex } from './utils/calldata';
-// import { abi } from '@uniswap/v3-periphery/artifacts/contracts/SwapRouter.sol/SwapRouter.json'","--- 

+++ 

@@ -14,8 +14,6 @@

 import abi from 'constants/abis/v3/swap-router.json';
 import { ADDRESS_ZERO } from 'v3lib/utils/v3constants';
 import { encodeRouteToPath } from './utils/encodeRouteToPath';
-
-// import abi from './swapRouterTestABI.json'
 
 export interface FeeOptions {
   /**",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1206026649,https://github.com/pokt-network/pocket/pull/789#discussion_r1206026649,,27,81e6c24ebc25c21c24f5997eaa8570347f0f5d47,682912b958986467465e21b1ae2710cac98aff55,utility/module.go,nan,"See #710 by @0xBigBoss  who is working on this.

@0xBigBoss do you think we could merge in your configs work (even though it's not fully done) to unblock soon to come work?"," 	logger  *modules.Logger
 	mempool mempool.TXMempool
+
+	// TODO: initialize","--- 

+++ 

@@ -6,7 +6,6 @@

 	""github.com/pokt-network/pocket/shared/mempool""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
-	""github.com/pokt-network/pocket/utility/service""
 	""github.com/pokt-network/pocket/utility/types""
 )
 
@@ -25,7 +24,7 @@

 	mempool mempool.TXMempool
 
 	// TODO: initialize
-	servicer service.Servicer
+	servicer modules.Servicer
 }
 
 func Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1218655296,https://github.com/pokt-network/pocket/pull/804#discussion_r1218655296,,66,094a95e15311c61707e1e16a9311ac44deae6faa,9cb0ee93d1dc34449b90a9c8487366a3a2abbe38,p2p/providers/peerstore_provider/persistence/persistence.go,p2p/providers/peerstore_provider/persistence/provider.go,Please add a comment as to why," }
+
+func (pabp *persistencePeerstoreProvider) GetUnstakedPeerstore() (typesP2P.Peerstore, error) {
+	return nil, fmt.Errorf(""persistence peerstore provider does not support unstaked peerstore"")","--- 

+++ 

@@ -1,24 +1,30 @@

 package persistence
 
 import (
-	""fmt""
-
 	""github.com/pokt-network/pocket/p2p/providers/peerstore_provider""
 	typesP2P ""github.com/pokt-network/pocket/p2p/types""
-	""github.com/pokt-network/pocket/runtime/configs""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 )
 
-var _ peerstore_provider.PeerstoreProvider = &persistencePeerstoreProvider{}
+var (
+	_ peerstore_provider.PeerstoreProvider = &persistencePeerstoreProvider{}
+	_ persistencePStoreProviderFactory     = &persistencePeerstoreProvider{}
+)
 
+type persistencePStoreProviderOption func(*persistencePeerstoreProvider)
+type persistencePStoreProviderFactory = modules.FactoryWithOptions[peerstore_provider.PeerstoreProvider, persistencePStoreProviderOption]
+
+// TECHDEBT(#810): refactor to implement `Submodule` interface.
 type persistencePeerstoreProvider struct {
 	base_modules.IntegratableModule
-	base_modules.InterruptableModule
 }
 
-// TECHDEBT: refactor
-func NewPersistencePeerstoreProvider(bus modules.Bus, options ...func(*persistencePeerstoreProvider)) *persistencePeerstoreProvider {
+func Create(bus modules.Bus, options ...persistencePStoreProviderOption) (peerstore_provider.PeerstoreProvider, error) {
+	return new(persistencePeerstoreProvider).Create(bus, options...)
+}
+
+func (*persistencePeerstoreProvider) Create(bus modules.Bus, options ...persistencePStoreProviderOption) (peerstore_provider.PeerstoreProvider, error) {
 	pabp := &persistencePeerstoreProvider{
 		IntegratableModule: *base_modules.NewIntegratableModule(bus),
 	}
@@ -27,41 +33,30 @@

 		o(pabp)
 	}
 
-	return pabp
-}
-
-// TECHDEBT: remove
-func Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return new(persistencePeerstoreProvider).Create(bus, options...)
-}
-
-// TECHDEBT: refactor
-func (*persistencePeerstoreProvider) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return NewPersistencePeerstoreProvider(bus), nil
+	return pabp, nil
 }
 
 func (*persistencePeerstoreProvider) GetModuleName() string {
 	return peerstore_provider.ModuleName
 }
 
-func (pabp *persistencePeerstoreProvider) GetStakedPeerstoreAtHeight(height uint64) (typesP2P.Peerstore, error) {
-	readCtx, err := pabp.GetBus().GetPersistenceModule().NewReadContext(int64(height))
+// GetStakedPeerstoreAtHeight implements the respective `PeerstoreProvider` interface method.
+func (persistencePSP *persistencePeerstoreProvider) GetStakedPeerstoreAtHeight(height uint64) (typesP2P.Peerstore, error) {
+	readCtx, err := persistencePSP.GetBus().GetPersistenceModule().NewReadContext(int64(height))
 	if err != nil {
 		return nil, err
 	}
 	defer readCtx.Release()
 
+	// TECHDEBT(#818): consider all staked actors, not just validators.
 	validators, err := readCtx.GetAllValidators(int64(height))
 	if err != nil {
 		return nil, err
 	}
-	return peerstore_provider.ActorsToPeerstore(pabp, validators)
+	return peerstore_provider.ActorsToPeerstore(persistencePSP, validators)
 }
 
-func (pabp *persistencePeerstoreProvider) GetP2PConfig() *configs.P2PConfig {
-	return pabp.GetBus().GetRuntimeMgr().GetConfig().P2P
+// GetStakedPeerstoreAtHeight implements the respective `PeerstoreProvider` interface method.
+func (persistencePSP *persistencePeerstoreProvider) GetUnstakedPeerstore() (typesP2P.Peerstore, error) {
+	return peerstore_provider.GetUnstakedPeerstore(persistencePSP.GetBus())
 }
-
-func (pabp *persistencePeerstoreProvider) GetUnstakedPeerstore() (typesP2P.Peerstore, error) {
-	return nil, fmt.Errorf(""persistence peerstore provider does not support unstaked peerstore"")
-}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1251330951,https://github.com/pokt-network/pocket/pull/732#discussion_r1251330951,,266,4602283cbd676952b2a5c20f73758305cc833ce5,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/background/router.go,nan,"I suggest changing
 ```
+func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {
```
 to
```
+// setupPeerDiscovery sets up the Kademlia Distributed Hash Table (DHT)
+func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {
```","+	return nil
+}
+
+func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {","--- 

+++ 

@@ -49,6 +49,7 @@

 	host libp2pHost.Host
 	// cancelReadSubscription is the cancel function for the context which is
 	// monitored in the `#readSubscription()` go routine. Call to terminate it.
+	// only one read subscription exists per router at any point in time
 	cancelReadSubscription context.CancelFunc
 
 	// Fields below are assigned during creation via `#setupDependencies()`.
@@ -215,6 +216,7 @@

 }
 
 func (rtr *backgroundRouter) setupDependencies(ctx context.Context, cfg *config.BackgroundConfig) error {
+	// NB: The order in which the internal components are setup below is important
 	if err := rtr.setupUnicastRouter(); err != nil {
 		return err
 	}
@@ -237,7 +239,9 @@

 
 	if err := rtr.setupPeerstore(
 		ctx,
-		cfg.PeerstoreProvider, cfg.CurrentHeightProvider); err != nil {
+		cfg.PeerstoreProvider,
+		cfg.CurrentHeightProvider,
+	); err != nil {
 		return fmt.Errorf(""setting up peerstore: %w"", err)
 	}
 	return nil
@@ -263,6 +267,7 @@

 	return nil
 }
 
+// setupPeerDiscovery sets up the Kademlia Distributed Hash Table (DHT)
 func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {
 	dhtMode := dht.ModeAutoServer
 	// NB: don't act as a bootstrap node in peer discovery in client debug mode
@@ -274,6 +279,7 @@

 	return err
 }
 
+// setupPubsub sets up a new gossip sub topic using libp2p
 func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {
 	// TECHDEBT(#730): integrate libp2p tracing via `pubsub.WithEventTracer()`.
 
@@ -313,8 +319,8 @@

 }
 
 func (rtr *backgroundRouter) bootstrap(ctx context.Context) error {
-	// CONSIDERATION: add `GetPeers` method to `PeerstoreProvider` interface
-	// to avoid this loop.
+	// CONSIDERATION: add `GetPeers` method, which returns a map,
+	// to the `PeerstoreProvider` interface to simplify this loop.
 	for _, peer := range rtr.pstore.GetPeerList() {
 		if err := utils.AddPeerToLibp2pHost(rtr.host, peer); err != nil {
 			return err
@@ -352,22 +358,26 @@

 func (rtr *backgroundRouter) topicValidator(_ context.Context, _ libp2pPeer.ID, msg *pubsub.Message) bool {
 	var backgroundMsg typesP2P.BackgroundMessage
 	if err := proto.Unmarshal(msg.Data, &backgroundMsg); err != nil {
+		rtr.logger.Error().Err(err).Msg(""unmarshalling Background message"")
 		return false
 	}
 
 	if backgroundMsg.Data == nil {
+		rtr.logger.Debug().Msg(""no data in Background message"")
 		return false
 	}
 
-	networkMessage := messaging.PocketEnvelope{}
-	if err := proto.Unmarshal(backgroundMsg.Data, &networkMessage); err != nil {
-		rtr.logger.Error().Err(err).Msg(""Error decoding network message"")
+	poktEnvelope := messaging.PocketEnvelope{}
+	if err := proto.Unmarshal(backgroundMsg.Data, &poktEnvelope); err != nil {
+		rtr.logger.Error().Err(err).Msg(""Error decoding Background message"")
 		return false
 	}
 
 	return true
 }
 
+// readSubscription is a while loop for receiving and handling messages from the
+// subscription. It is intended to be called as a goroutine.
 func (rtr *backgroundRouter) readSubscription(ctx context.Context) {
 	for {
 		if err := ctx.Err(); err != nil {",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184394394,https://github.com/pokt-network/pocket/pull/707#discussion_r1184394394,,27,4159d7a40f8d99835d17a00a052b6782cb3f795d,d7cc85d553cf9b0257f16c3a213ed4de96b8363a,p2p/utils/config.go,p2p/config/config.go,Seeing this after the comment above. I don't think  they should be part of the struct.,"+}
+
+func (cfg *RouterConfig) IsValid() (err error) {
+	// TECHDEBT: can `Hostname` or `MaxMempoolCount` be invalid?","--- 

+++ 

@@ -1,31 +1,46 @@

-package utils
+package config
 
 import (
 	""fmt""
-
 	""github.com/libp2p/go-libp2p/core/host""
-	""go.uber.org/multierr""
-
 	""github.com/pokt-network/pocket/p2p/providers""
 	""github.com/pokt-network/pocket/shared/crypto""
+	""go.uber.org/multierr""
 )
 
-// RouterConfig is used to configure `Router` implementations using the given
-// libp2p host and current height and peerstore providers.
-// TECHDEBT: I would prefer for this to be in p2p/types/router.go but this causes
-// an import cycle between `typesP2P` and `providers`.
-type RouterConfig struct {
+// baseConfig implements `RouterConfig` using the given libp2p host and current
+// height and peerstore providers. Intended for internal use by other `RouterConfig`
+// implementations with common config parameters.
+//
+// NB: intentionally *not* embedding `baseConfig` to improve readability of usages
+// of would-be embedders (e.g. `BackgroundConfig`).
+type baseConfig struct {
+	Host                  host.Host
 	Addr                  crypto.Address
 	CurrentHeightProvider providers.CurrentHeightProvider
-	Host                  host.Host
-	Hostname              string
-	MaxMempoolCount       uint64
 	PeerstoreProvider     providers.PeerstoreProvider
 }
 
-func (cfg *RouterConfig) IsValid() (err error) {
-	// TECHDEBT: can `Hostname` or `MaxMempoolCount` be invalid?
+// BackgroundConfig implements `RouterConfig` for use with `BackgroundRouter`.
+type BackgroundConfig struct {
+	Host                  host.Host
+	Addr                  crypto.Address
+	CurrentHeightProvider providers.CurrentHeightProvider
+	PeerstoreProvider     providers.PeerstoreProvider
+}
 
+// RainTreeConfig implements `RouterConfig` for use with `RainTreeRouter`.
+type RainTreeConfig struct {
+	Host                  host.Host
+	Addr                  crypto.Address
+	CurrentHeightProvider providers.CurrentHeightProvider
+	PeerstoreProvider     providers.PeerstoreProvider
+
+	MaxNonces uint64
+}
+
+// IsValid implements the respective member of the `RouterConfig` interface.
+func (cfg *baseConfig) IsValid() (err error) {
 	if cfg.Addr == nil {
 		err = multierr.Append(err, fmt.Errorf(""pokt address not configured""))
 	}
@@ -43,3 +58,29 @@

 	}
 	return err
 }
+
+// IsValid implements the respective member of the `RouterConfig` interface.
+func (cfg *BackgroundConfig) IsValid() (err error) {
+	baseCfg := baseConfig{
+		Host:                  cfg.Host,
+		Addr:                  cfg.Addr,
+		CurrentHeightProvider: cfg.CurrentHeightProvider,
+		PeerstoreProvider:     cfg.PeerstoreProvider,
+	}
+	return multierr.Append(err, baseCfg.IsValid())
+}
+
+// IsValid implements the respective member of the `RouterConfig` interface.
+func (cfg *RainTreeConfig) IsValid() (err error) {
+	if cfg.MaxNonces == 0 {
+		err = multierr.Append(err, fmt.Errorf(""max nonces must be greater than 0""))
+	}
+
+	baseCfg := baseConfig{
+		Host:                  cfg.Host,
+		Addr:                  cfg.Addr,
+		CurrentHeightProvider: cfg.CurrentHeightProvider,
+		PeerstoreProvider:     cfg.PeerstoreProvider,
+	}
+	return multierr.Append(err, baseCfg.IsValid())
+}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1174221889,https://github.com/pokt-network/pocket/pull/684#discussion_r1174221889,,279,33fefdfbfb6dbfb6107ab93e63c3abca7951a012,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/handlers.go,nan,"I suggest changing
 ```
+func (s *rpcServer) PostV1QueryAccounttxs(ctx echo.Context) error {
```
 to
```
+func (s *rpcServer) PostV1QueryAccountTxs(ctx echo.Context) error {
```","+	})
+}
+
+func (s *rpcServer) PostV1QueryAccounttxs(ctx echo.Context) error {","--- 

+++ 

@@ -2,17 +2,11 @@

 
 import (
 	""encoding/hex""
-	""errors""
-	""fmt""
-	""math/big""
 	""net/http""
-	""strconv""
 
 	""github.com/labstack/echo/v4""
 	""github.com/pokt-network/pocket/app""
-	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
-	""github.com/pokt-network/pocket/shared/utils""
 )
 
 // CONSIDER: Remove all the V1 prefixes from the RPC module
@@ -36,7 +30,7 @@

 		return ctx.String(http.StatusBadRequest, ""cannot decode tx bytes"")
 	}
 
-	if err = s.GetBus().GetUtilityModule().HandleTransaction(txBz); err != nil {
+	if err := s.GetBus().GetUtilityModule().HandleTransaction(txBz); err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
 
@@ -47,9 +41,8 @@

 	return nil
 }
 
-// DISCUSSION: This may need to be changed when the GetSession function is actually implemented
-func (s *rpcServer) PostV1ClientDispatch(ctx echo.Context) error {
-	var body DispatchRequest
+func (s *rpcServer) PostV1ClientGetSession(ctx echo.Context) error {
+	var body SessionRequest
 	if err := ctx.Bind(&body); err != nil {
 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
@@ -63,29 +56,32 @@

 	rpcApp := protocolActorToRPCProtocolActor(application)
 
 	rpcServicers := make([]ProtocolActor, 0)
-	for _, serv := range session.GetServicers() {
-		actor := protocolActorToRPCProtocolActor(serv)
+	for _, servicer := range session.GetServicers() {
+		actor := protocolActorToRPCProtocolActor(servicer)
 		rpcServicers = append(rpcServicers, actor)
 	}
 
 	rpcFishermen := make([]ProtocolActor, 0)
-	for _, fm := range session.GetFishermen() {
-		actor := protocolActorToRPCProtocolActor(fm)
+	for _, fisher := range session.GetFishermen() {
+		actor := protocolActorToRPCProtocolActor(fisher)
 		rpcFishermen = append(rpcFishermen, actor)
 	}
 
 	return ctx.JSON(http.StatusOK, Session{
-		SessionId:   session.GetId(),
-		Height:      session.GetHeight(),
-		Chain:       string(session.GetRelayChain()),
-		Geozone:     string(session.GetGeoZone()),
-		Application: rpcApp,
-		Servicers:   rpcServicers,
-		Fishermen:   rpcFishermen,
-	})
-}
-
-// DISCUSSION: This may need to be changed when the SendRelay function is actually implemented
+		SessionId:        session.GetId(),
+		SessionNumber:    session.GetSessionNumber(),
+		SessionHeight:    session.GetSessionHeight(),
+		NumSessionBlocks: session.GetNumSessionBlocks(),
+		Chain:            string(session.GetRelayChain()),
+		Geozone:          string(session.GetGeoZone()),
+		Application:      rpcApp,
+		Servicers:        rpcServicers,
+		Fishermen:        rpcFishermen,
+	})
+}
+
+// TECHDEBT: This will need to be changed when the HandleRelay function is actually implemented
+// because it copies data structures from v0. For example, AATs are no longer necessary in v1.
 func (s *rpcServer) PostV1ClientRelay(ctx echo.Context) error {
 	var body RelayRequest
 	if err := ctx.Bind(&body); err != nil {
@@ -133,7 +129,7 @@

 		Meta:    relayMeta,
 	}
 
-	relayResponse, err := s.GetBus().GetUtilityModule().SendRelay(relayRequest)
+	relayResponse, err := s.GetBus().GetUtilityModule().HandleRelay(relayRequest)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -144,7 +140,8 @@

 	})
 }
 
-// DISCUSSION: This may need to be changed when the SendRelay function is actually implemented
+// TECHDEBT: This will need to be changed when the HandleChallenge function is actually implemented
+// because it copies data structures from v0
 func (s *rpcServer) PostV1ClientChallenge(ctx echo.Context) error {
 	var body ChallengeRequest
 	if err := ctx.Bind(&body); err != nil {
@@ -193,830 +190,6 @@

 	})
 }
 
-// Queries
-
-func (s *rpcServer) PostV1QueryAccount(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	accBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	amount, err := readCtx.GetAccountAmount(accBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, Account{
-		Address: body.Address,
-		Coins:   []Coin{{Amount: amount, Denom: ""upokt""}},
-	})
-}
-
-func (s *rpcServer) PostV1QueryAccounts(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allAccounts, err := readCtx.GetAllAccounts(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allAccounts), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAccountsResponse{})
-	}
-
-	accounts := make([]Account, 0)
-	for _, account := range allAccounts[start : end+1] {
-		accounts = append(accounts, Account{
-			Address: account.Address,
-			Coins:   []Coin{{Amount: account.Amount, Denom: ""upokt""}},
-		})
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAccountsResponse{
-		Result:     accounts,
-		Page:       body.Page,
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryAccounttxs(ctx echo.Context) error {
-	var body QueryAddressPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-	sort := checkSort(*body.Sort)
-	sortDesc := true
-	if sort == ""asc"" {
-		sortDesc = false
-	}
-
-	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResults, err := txIndexer.GetBySender(body.Address, sortDesc)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(txResults), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{})
-	}
-
-	pageTxs := make([]Transaction, 0)
-	for _, txResult := range txResults[start : end+1] {
-		rpcTx, err := s.txResultToRPCTransaction(txResult)
-		if err != nil {
-			return ctx.String(http.StatusInternalServerError, err.Error())
-		}
-		pageTxs = append(pageTxs, *rpcTx)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{
-		Txs:        pageTxs,
-		Page:       body.Page,
-		TotalTxs:   int64(len(txResults)),
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) GetV1QueryAllChainParams(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	paramSlice, err := readCtx.GetAllParams()
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	resp := make([]Parameter, 0)
-	for i := 0; i < len(paramSlice); i++ {
-		resp = append(resp, Parameter{
-			ParameterName:  paramSlice[i][0],
-			ParameterValue: paramSlice[i][1],
-		})
-	}
-	return ctx.JSON(200, resp)
-}
-
-func (s *rpcServer) PostV1QueryApp(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	application, err := readCtx.GetApp(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(application)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryApps(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allApps, err := readCtx.GetAllApps(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allApps), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAppsResponse{})
-	}
-
-	rpcApps := make([]ProtocolActor, 0)
-	for _, app := range allApps[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(app)
-		rpcApps = append(rpcApps, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAppsResponse{
-		Apps:       rpcApps,
-		TotalApps:  int64(len(allApps)),
-		Page:       body.Page,
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryBalance(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	accBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	amountStr, err := readCtx.GetAccountAmount(accBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	amount, err := strconv.ParseInt(amountStr, 10, 64)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryBalanceResponse{
-		Balance: amount,
-	})
-}
-
-func (s *rpcServer) PostV1QueryBlock(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := uint64(body.Height)
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
-	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	block := new(coreTypes.Block)
-	if err := codec.GetCodec().Unmarshal(blockBz, block); err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	rpcBlock, err := s.blockToRPCBlock(block)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcBlock)
-}
-
-func (s *rpcServer) PostV1QueryBlocktxs(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-	sort := checkSort(*body.Sort)
-	sortDesc := true
-	if sort == ""asc"" {
-		sortDesc = false
-	}
-
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := uint64(body.Height)
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
-	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	block := new(coreTypes.Block)
-	if err := codec.GetCodec().Unmarshal(blockBz, block); err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	rpcBlock, err := s.blockToRPCBlock(block)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allTxs := rpcBlock.Transactions
-	if sortDesc {
-		for i, j := 0, len(allTxs)-1; i < j; i, j = i+1, j-1 {
-			allTxs[i], allTxs[j] = allTxs[j], allTxs[i]
-		}
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allTxs), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryTxsResponse{})
-	}
-
-	return ctx.JSON(http.StatusOK, QueryTxsResponse{
-		Transactions: allTxs[start : end+1],
-		TotalTxs:     int64(len(allTxs)),
-		Page:         body.Page,
-		TotalPages:   int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryFisherman(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	fisherman, err := readCtx.GetFisherman(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(fisherman)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryFishermen(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allFishermen, err := readCtx.GetAllFishermen(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allFishermen), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryFishermenResponse{})
-	}
-
-	rpcFishermen := make([]ProtocolActor, 0)
-	for _, fm := range allFishermen[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(fm)
-		rpcFishermen = append(rpcFishermen, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryFishermenResponse{
-		Fishermen:      rpcFishermen,
-		TotalFishermen: int64(len(allFishermen)),
-		Page:           body.Page,
-		TotalPages:     int64(totalPages),
-	})
-}
-
-func (s *rpcServer) GetV1QueryHeight(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-
-	return ctx.JSON(http.StatusOK, QueryHeight{
-		Height: int64(currentHeight),
-	})
-}
-
-func (s *rpcServer) PostV1QueryParam(ctx echo.Context) error {
-	var body QueryParameter
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	paramValue, err := readCtx.GetStringParam(body.ParamName, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, Parameter{
-		ParameterName:  body.ParamName,
-		ParameterValue: paramValue,
-	})
-}
-
-func (s *rpcServer) PostV1QueryServicer(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	servicer, err := readCtx.GetServicer(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(servicer)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryServicers(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allServicers, err := readCtx.GetAllServicers(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allServicers), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryServicersResponse{})
-	}
-
-	rpcServicers := make([]ProtocolActor, 0)
-	for _, serv := range allServicers[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(serv)
-		rpcServicers = append(rpcServicers, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryServicersResponse{
-		Servicers:      rpcServicers,
-		TotalServicers: int64(len(allServicers)),
-		Page:           body.Page,
-		TotalPages:     int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QuerySupply(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	pools, err := readCtx.GetAllPools(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	rpcPools := make([]Pool, 0)
-	total := new(big.Int)
-	for _, pool := range pools {
-		name := coreTypes.PoolAddressToFriendlyName(pool.Address)
-		amount, success := new(big.Int).SetString(pool.Amount, 10)
-		if !success {
-			return ctx.String(http.StatusInternalServerError, ""failed to convert amount to big.Int"")
-		}
-		total = total.Add(total, amount)
-		rpcPools = append(rpcPools, Pool{
-			Address: pool.Address,
-			Name:    name,
-			Amount:  pool.Amount,
-			Denom:   ""upokt"",
-		})
-	}
-
-	return ctx.JSON(http.StatusOK, QuerySupplyResponse{
-		Pools: rpcPools,
-		Total: Coin{
-			Amount: total.String(),
-			Denom:  ""upokt"",
-		},
-	})
-}
-
-func (s *rpcServer) PostV1QuerySupportedchains(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	chains, err := readCtx.GetSupportedChains(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QuerySupportedChainsResponse{
-		SupportedChains: chains,
-	})
-}
-
-func (s *rpcServer) PostV1QueryTx(ctx echo.Context) error {
-	var body QueryHash
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	hashBz, err := hex.DecodeString(body.Hash)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResult, err := txIndexer.GetByHash(hashBz)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	rpcTx, err := s.txResultToRPCTransaction(txResult)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcTx)
-}
-
-func (s *rpcServer) PostV1QueryUnconfirmedtx(ctx echo.Context) error {
-	var body QueryHash
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	mempool := s.GetBus().GetUtilityModule().GetMempool()
-	uncTx := mempool.Get(body.Hash)
-	if uncTx == nil {
-		return ctx.String(http.StatusBadRequest, fmt.Sprintf(""hash not found in mempool: %s"", body.Hash))
-	}
-
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions([][]byte{uncTx})
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcUncTxs[0])
-}
-
-func (s *rpcServer) PostV1QueryUnconfirmedtxs(ctx echo.Context) error {
-	var body QueryPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	mempool := s.GetBus().GetUtilityModule().GetMempool()
-	uncTxs := mempool.GetAll()
-
-	start, end, totalPages, err := getPageIndexes(len(uncTxs), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryTxsResponse{})
-	}
-
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions(uncTxs[start : end+1])
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryTxsResponse{
-		Transactions: rpcUncTxs,
-		TotalTxs:     int64(len(uncTxs)),
-		Page:         body.Page,
-		TotalPages:   int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryUpgrade(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-	reatCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	version, err := reatCtx.GetVersionAtHeight(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryUpgradeResponse{
-		Height:  height,
-		Version: version,
-	})
-}
-
-func (s *rpcServer) PostV1QueryValidator(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	validator, err := readCtx.GetValidator(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(validator)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryValidators(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allValidators, err := readCtx.GetAllValidators(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allValidators), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryValidatorsResponse{})
-	}
-
-	rpcValidators := make([]ProtocolActor, 0)
-	for _, val := range allValidators[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(val)
-		rpcValidators = append(rpcValidators, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryValidatorsResponse{
-		Validators:      rpcValidators,
-		TotalValidators: int64(len(allValidators)),
-		Page:            body.Page,
-		TotalPages:      int64(totalPages),
-	})
-}
-
 func (s *rpcServer) GetV1P2pStakedActorsAddressBook(ctx echo.Context, params GetV1P2pStakedActorsAddressBookParams) error {
 	var height int64
 	var actors []Actor",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1212356838,https://github.com/pokt-network/pocket/pull/710#discussion_r1212356838,,20,8539e8eae9c4a9c14177235911cd10ce1b14c1b8,dbc0deb6a7aec39359745e9be952a4992689052a,build/config/config.node5.servicer.json,build/config/config.servicer1.json,"if we choose to rename the nodes, we should also update the `node_schemas` to reflect it appropriately","+  },
+  ""persistence"": {
+    ""postgres_url"": ""postgres://postgres:postgres@pocket-db:5432/postgres"",
+    ""node_schema"": ""node5"",","--- 

+++ 

@@ -17,7 +17,7 @@

   },
   ""persistence"": {
     ""postgres_url"": ""postgres://postgres:postgres@pocket-db:5432/postgres"",
-    ""node_schema"": ""node5"",
+    ""node_schema"": ""servicer1"",
     ""block_store_path"": ""/var/blockstore"",
     ""tx_indexer_path"": ""/var/txindexer"",
     ""trees_store_dir"": ""/var/trees"",
@@ -28,7 +28,7 @@

     ""health_check_period"": ""30s""
   },
   ""p2p"": {
-    ""hostname"": ""node5.servicer"",
+    ""hostname"": ""servicer1"",
     ""port"": 42069,
     ""use_rain_tree"": true,
     ""is_empty_connection_type"": false,
@@ -51,6 +51,7 @@

     ""use_cors"": false
   },
   ""servicer"": {
-    ""enabled"": true
+    ""enabled"": true,
+    ""chains"": [""0001""]
   }
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/QuickSwap/interface-v2/pulls/comments/1445699817,https://github.com/QuickSwap/interface-v2/pull/1156#discussion_r1445699817,,1,248e410d19492151bd5410b3e3ae1ce8af1c1995,bfbf06dacd1d2b8b8b5aed55e464c336b5641dba,src/pages/SwapPage/BuySellWidget/DraginDipatchAdvertisement.tsx,src/pages/SwapPage/BuySellWidget/DragonDispatchAdvertisement.tsx,Please fix the name of this file,"@@ -0,0 +1,56 @@
+import { Box, Button, CircularProgress } from '@material-ui/core';","--- 

+++ 

@@ -1,5 +1,5 @@

 import { Box, Button, CircularProgress } from '@material-ui/core';
-import DragonDipatchIcon from 'assets/images/featured/DragonDipatchIcon.png';
+import DragonDispatchIcon from 'assets/images/featured/DragonDispatchIcon.png';
 import { useSubscribeNewsletter } from 'hooks/useNewsletterSignup';
 import React, { useState } from 'react';
 import { useTranslation } from 'react-i18next';
@@ -13,12 +13,22 @@

   };
 
   return (
-    <Box className='flex'>
+    <Box>
+      <Box className='flex'>
+        <Box>
+          <img
+            className='wallet'
+            src={DragonDispatchIcon}
+            alt='buy with fiat'
+          />
+        </Box>
+        <Box pl={1} className='flex'>
+          <Box className='text-white text-lg my-auto'>
+            {t('dragonDipatchAd')}
+          </Box>
+        </Box>
+      </Box>
       <Box>
-        <img className='wallet' src={DragonDipatchIcon} alt='buy with fiat' />
-      </Box>
-      <Box pl={1}>
-        <Box className='text-white text-lg'>{t('dragonDipatchAd')}</Box>
         <Box className='flex items-center'>
           <Box className='newsletterSignupFormSwap'>
             <input",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1176390423,https://github.com/pokt-network/pocket/pull/683#discussion_r1176390423,217,218,50d0c7174f166c4424631ca68fc3ed05fbb46ed6,fb245bc027ae55aaee936daf9f85ecb2d9ad8b40,utility/session.go,nan,"Nice call out! :100: 

super nit: Wdyt about adopting [rfc2119 style requirement keywords](https://www.rfc-editor.org/rfc/rfc2119) (e.g. `MUST`, `SHOULD`, `MAY`, `... NOT`) (not sure if I chose the right one in my suggestion)
I suggest changing
 ```
+// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
+// for it to be language agnostic, a general purpose algorithm needs ot be used.
```
 to
```
+// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
+// for it to be language agnostic, a general purpose algorithm MUST be used.
```

---

Alternatively there's a typo  - I think that's what @dylanlott was commenting on:
```suggestion
// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
// for it to be language agnostic, a general purpose algorithm needs to be used.
```
","+// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
+// for it to be language agnostic, a general purpose algorithm needs ot be used.","--- 

+++ 

@@ -7,15 +7,17 @@

 	""math""
 	""math/rand""
 
+	""golang.org/x/exp/slices""
+
 	""github.com/pokt-network/pocket/logger""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/utility/types""
-	""golang.org/x/exp/slices""
 )
 
 // GetSession implements of the exposed `UtilityModule.GetSession` function
+// TECHDEBT(#519): Add custom error types depending on the type of issue that occurred and assert on them in the unit tests.
 func (m *utilityModule) GetSession(appAddr string, height int64, relayChain, geoZone string) (*coreTypes.Session, error) {
 	persistenceModule := m.GetBus().GetPersistenceModule()
 	readCtx, err := persistenceModule.NewReadContext(height)
@@ -95,7 +97,7 @@

 
 // hydrateSessionApplication hydrates the full Application actor based on the address provided
 func (s *sessionHydrator) hydrateSessionApplication(appAddr string) error {
-	// TECHDEBT: We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
+	// TECHDEBT(#706): We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
 	addr, err := hex.DecodeString(appAddr)
 	if err != nil {
 		return err
@@ -112,7 +114,7 @@

 		return fmt.Errorf(""application %s does not stake for relay chain %s"", app.Address, s.session.RelayChain)
 	}
 
-	if !(app.PausedHeight == -1 && app.UnstakingHeight == -1) {
+	if app.PausedHeight != -1 || app.UnstakingHeight != -1 {
 		return fmt.Errorf(""application %s is either unstaked or paused"", app.Address)
 	}
 
@@ -135,6 +137,9 @@

 	}
 	prevHashBz, err := hex.DecodeString(prevHash)
 
+	if err != nil {
+		return err
+	}
 	appPubKeyBz := []byte(s.session.Application.PublicKey)
 	relayChainBz := []byte(string(s.session.RelayChain))
 	geoZoneBz := []byte(s.session.GeoZone)
@@ -163,13 +168,13 @@

 	candidateServicers := make([]*coreTypes.Actor, 0)
 	for _, servicer := range servicers {
 		// Sanity check the servicer is not paused, jailed or unstaking
-		if !(servicer.PausedHeight == -1 && servicer.UnstakingHeight == -1) {
+		if servicer.PausedHeight != -1 || servicer.UnstakingHeight != -1 {
 			return fmt.Errorf(""hydrateSessionServicers should not have encountered a paused or unstaking servicer: %s"", servicer.Address)
 		}
 
-		// TODO(#697): Filter by geo-zone
-
-		// OPTIMIZE: If this was a map[string]struct{}, we could have avoided the loop
+		// TECHDEBT(#697): Filter by geo-zone
+
+		// OPTIMIZE: If `servicer.Chains` was a map[string]struct{}, we could eliminate `slices.Contains()`'s loop
 		if slices.Contains(servicer.Chains, s.session.RelayChain) {
 			candidateServicers = append(candidateServicers, servicer)
 		}
@@ -197,7 +202,7 @@

 	candidateFishermen := make([]*coreTypes.Actor, 0)
 	for _, fisher := range fishermen {
 		// Sanity check the fisher is not paused, jailed or unstaking
-		if !(fisher.PausedHeight == -1 && fisher.UnstakingHeight == -1) {
+		if fisher.PausedHeight != -1 || fisher.UnstakingHeight != -1 {
 			return fmt.Errorf(""hydrateSessionFishermen should not have encountered a paused or unstaking fisherman: %s"", fisher.Address)
 		}
 
@@ -215,7 +220,7 @@

 
 // pseudoRandomSelection returns a random subset of the candidates.
 // DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
-// for it to be language agnostic, a general purpose algorithm needs ot be used.
+// for it to be language agnostic, a general purpose algorithm MUST be used.
 func pseudoRandomSelection(candidates []*coreTypes.Actor, numTarget int, sessionId []byte) []*coreTypes.Actor {
 	// If there aren't enough candidates, return all of them
 	if numTarget > len(candidates) {
@@ -224,6 +229,7 @@

 	}
 
 	// Take the first 8 bytes of sessionId to use as the seed
+	// NB: There is specific reason why `BigEndian` was chosen over `LittleEndian` in this specific context.
 	seed := int64(binary.BigEndian.Uint64(crypto.SHA3Hash(sessionId)[:8]))
 
 	// Retrieve the indices for the candidates
@@ -238,10 +244,11 @@

 
 // OPTIMIZE: Postgres uses a `Twisted Mersenne Twister (TMT)` randomness algorithm.
 // We could potentially look into changing everything into a single SQL query but
-// would nee dto verify that it can be implemented in a platform agnostic way.
+// would need to verify that it can be implemented in a platform agnostic way.
 
 // uniqueRandomIndices returns a map of `numIndices` unique random numbers less than `maxIndex`
 // seeded by `seed`.
+// panics if `numIndicies > maxIndex` since that code path SHOULD never be executed.
 // NB: A map pointing to empty structs is used to simulate set behaviour.
 func uniqueRandomIndices(seed, maxIndex, numIndices int64) map[int64]struct{} {
 	// This should never happen",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228803869,https://github.com/pokt-network/pocket/pull/803#discussion_r1228803869,,445,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/servicer/module.go,nan,"Should this be a TODO instead of a DISCUSS?

Seems like it might validate a whole ticket for this task alone, or make it an M5 thing.","+		req.Header.Set(""Content-Type"", ""application/json"")
+	}
+
+	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays","--- 

+++ 

@@ -13,6 +13,7 @@

 	""time""
 
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
@@ -24,11 +25,12 @@

 	""golang.org/x/exp/slices""
 )
 
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
 	errValidateServicer    = errors.New(""relay failed servicer validation"")
-	errValidateApplication = errors.New(""relay failed application validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
 
 	_ modules.ServicerModule = &servicer{}
 )
@@ -37,11 +39,12 @@

 	ServicerModuleName = ""servicer""
 )
 
-// sessionTokens is used to cache the original number of tokens available
+// sessionTokens is used to cache the starting number of tokens available
 // during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	sessionNumber          int64
-	originalCountAvailable *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -52,9 +55,10 @@

 	config *configs.ServicerConfig
 
 	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
 	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
-	// INVESTIGATE: considering the computational complexity, should we skip caching this value?
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
@@ -71,7 +75,9 @@

 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	s := &servicer{}
+	s := &servicer{
+		totalTokens: make(map[string]*sessionTokens),
+	}
 
 	for _, option := range options {
 		option(s)
@@ -120,7 +126,12 @@

 
 	// TODO(M6): Look into data integrity checks and response validation.
 
-	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(relay, response)
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -128,55 +139,50 @@

 		return response, nil
 	}
 
-	session, err := s.getSession(relay)
-	if err != nil {
-		return nil, err
-	}
-
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.StoreServiceRelay(session, relay.Meta.ApplicationAddress, relayDigest, relayReqResBytes); err != nil {
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.Release(); err != nil {
-		s.logger.Warn().Err(err).Msg(""failed to release local context"")
-	}
-
 	return response, nil
 }
 
 // isRelayVolumeApplicable returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) isRelayVolumeApplicable(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
 	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
 	if err != nil {
 		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
 	}
 
 	relayDigest := crypto.SHA3Hash(relayReqResBytes)
-
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.isRelayVolumeApplicableOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
 	}
 
 	return signedDigest, relayReqResBytes, collision, nil
 }
 
-// INCOMPLETE: implement this
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) isRelayVolumeApplicableOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
@@ -184,11 +190,11 @@

 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
 	switch payload := relay.RelayPayload.(type) {
 	case *coreTypes.Relay_JsonRpcPayload:
-		return s.executeHTTPRelay(relay.Meta, payload.JsonRpcPayload)
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
 	case *coreTypes.Relay_RestPayload:
-		return nil, fmt.Errorf(""Error executing relay on application %s: REST not supported"", relay.Meta.ApplicationAddress)
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
 	default:
-		return nil, fmt.Errorf(""Error exeucting relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
 	}
 }
 
@@ -235,15 +241,16 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -254,7 +261,7 @@

 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -263,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -270,13 +280,14 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer has remaining for the Application in the session provided.
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
 //
 //	If nothing is cached, the maximum number of session tokens is computed.
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.originalCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.originalCountAvailable), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
@@ -305,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -359,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
 	session, err := s.getSession(relay)
 	if err != nil {
 		return err
 	}
 
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -374,14 +379,14 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
 // of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
 func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
@@ -390,7 +395,7 @@

 		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
 	}
 
-	// DOCUMENT: find the right document to explain the following:
+	// TODO(M5): find the right document to explain the following:
 	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
 	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
 	//		matches the beginning of the session.
@@ -400,8 +405,7 @@

 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
 	if err != nil {
 		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
@@ -409,8 +413,8 @@

 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
 }
 
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func (s *servicer) executeHTTPRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JsonRpcPayload) (*coreTypes.RelayResponse, error) {
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
 	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
 		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
 	}
@@ -420,29 +424,48 @@

 		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
 	}
 
-	chainUrl, err := url.Parse(serviceConfig.Url)
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
 	if err != nil {
 		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
 	}
-	targetUrl := chainUrl.JoinPath(payload.HttpPath)
-
-	req, err := http.NewRequest(payload.Method, targetUrl.String(), bytes.NewBuffer([]byte(payload.Data)))
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
 	if err != nil {
 		return nil, err
 	}
-	if serviceConfig.BasicAuth != nil && serviceConfig.BasicAuth.UserName != """" {
-		req.SetBasicAuth(serviceConfig.BasicAuth.UserName, serviceConfig.BasicAuth.Password)
-	}
-
-	// DISCUSS: do we need a default user-agent for HTTP requests?
-	for k, v := range payload.Headers {
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
 		req.Header.Set(k, v)
 	}
-	if len(payload.Headers) == 0 {
+	if req.Header.Get(""Content-Type"") == """" {
 		req.Header.Set(""Content-Type"", ""application/json"")
 	}
 
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
 	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
 	if err != nil {
 		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160188897,https://github.com/pokt-network/pocket/pull/653#discussion_r1160188897,,144,e5cf35e950020a6e16608501447b8b1c774485c9,f4b9e504bd881326f9528ee370010ca55b67141f,e2e/tests/steps_init_test.go,nan,Move the string template into a const,"+// stakeValidator runs Validator stake command with the address, amount, chains..., and serviceURL provided
+func stakeValidator(amount string) error {
+	privateKey := getPrivateKey(validatorKeys, validatorA)
+	validatorServiceUrl := fmt.Sprintf(""v1-validator%s:%d"", validatorA, defaults.DefaultP2PPort)","--- 

+++ 

@@ -4,49 +4,60 @@

 
 import (
 	""fmt""
-	""log""
 	""os""
 	""path/filepath""
 	""strings""
 	""testing""
 
+	pocketLogger ""github.com/pokt-network/pocket/logger""
 	""github.com/pokt-network/pocket/runtime/defaults""
 	cryptoPocket ""github.com/pokt-network/pocket/shared/crypto""
 	pocketk8s ""github.com/pokt-network/pocket/shared/k8s""
+
+	""github.com/cucumber/godog""
 	""k8s.io/client-go/kubernetes""
 	""k8s.io/client-go/tools/clientcmd""
-
-	""github.com/cucumber/godog""
 )
 
 var (
-	// validatorKeys is hydrated by the clientset with credentials for all validators
+	logger = pocketLogger.Global.CreateLoggerForModule(""e2e"")
+
+	// validatorKeys is hydrated by the clientset with credentials for all validators.
+	// validatorKeys maps validator IDs to their private key as a hex string.
 	validatorKeys map[string]string
 	// clientset is the kubernetes API we acquire from the user's $HOME/.kube/config
 	clientset *kubernetes.Clientset
 	// validator holds command results between runs and reports errors to the test suite
 	validator = &validatorPod{}
 	// validatorA maps to suffix ID 001 of the kube pod that we use as our control agent
-	validatorA string = ""001""
-	// validatorB maps to suffix ID 002
-	validatorB string = ""002""
-	chainId           = ""0001""
+)
+
+const (
+	// defines the host & port scheme that LocalNet uses for naming validators.
+	// e.g. v1-validator-001 thru v1-validator-999
+	validatorServiceURLTmpl = ""v1-validator%s:%d""
+	// validatorA maps to suffix ID 001 and is also used by the cluster-manager
+	// though it has no special permissions.
+	validatorA = ""001""
+	// validatorB maps to suffix ID 002 and receives in the Send test.
+	validatorB = ""002""
+	chainId    = ""0001""
 )
 
 func init() {
 	cs, err := getClientset()
 	if err != nil {
-		log.Fatalf(""failed to get clientset: %v"", err)
+		logger.Fatal().Err(err).Msg(""failed to get clientset"")
 	}
 	clientset = cs
 	vkmap, err := pocketk8s.FetchValidatorPrivateKeys(clientset)
 	if err != nil {
-		log.Fatalf(""failed to get validator keys: %v"", err)
+		logger.Fatal().Err(err).Msg(""failed to get validator key map"")
 	}
 	validatorKeys = vkmap
 }
 
-// TestFeatures runs the e2e tests specifiedin any .features files in this directory
+// TestFeatures runs the e2e tests specified in any .features files in this directory
 // * This test suite assumes that a LocalNet is running that can be accessed by `kubectl`
 func TestFeatures(t *testing.T) {
 	suite := godog.TestSuite{
@@ -68,19 +79,18 @@

 	ctx.Step(`^the user should be able to see standard output containing ""([^""]*)""$`, theUserShouldBeAbleToSeeStandardOutputContaining)
 	ctx.Step(`^the user has a validator$`, theUserHasAValidator)
 	ctx.Step(`^the validator should have exited without error$`, theValidatorShouldHaveExitedWithoutError)
-	ctx.Step(`^the user stakes their validator with (\d+) POKT$`, theUserStakesTheirValidatorWithPOKT)
-	ctx.Step(`^the user should be able to unstake their wallet$`, theUserShouldBeAbleToUnstakeTheirWallet)
-	ctx.Step(`^the user sends (\d+) POKT to another address$`, theUserSendsPOKTToAnotherAddress)
+	ctx.Step(`^the user stakes their validator with amount (\d+) uPOKT$`, theUserStakesTheirValidatorWith)
+	ctx.Step(`^the user should be able to unstake their validator$`, theUserShouldBeAbleToUnstakeTheirValidator)
+	ctx.Step(`^the user sends (\d+) uPOKT to another address$`, theUserSendsToAnotherAddress)
 }
 
 func theUserHasAValidator() error {
 	res, err := validator.RunCommand(""help"")
-	if err != nil {
-		log.Printf(""validator error: %+v"", err)
-		return err
-	}
-	validator.result = res
-	return err
+	validator.result = res
+	if err != nil {
+		return err
+	}
+	return nil
 }
 
 func theValidatorShouldHaveExitedWithoutError() error {
@@ -89,13 +99,10 @@

 
 func theUserRunsTheCommand(cmd string) error {
 	cmds := strings.Split(cmd, "" "")
-	result, err := validator.RunCommand(cmds...)
-	if err != nil {
-		validator.result = result
-		return err
-	}
-	if result.Err != nil {
-		return result.Err
+	res, err := validator.RunCommand(cmds...)
+	validator.result = res
+	if err != nil {
+		return err
 	}
 	return nil
 }
@@ -107,16 +114,16 @@

 	return nil
 }
 
-func theUserStakesTheirValidatorWithPOKT(amount int) error {
+func theUserStakesTheirValidatorWith(amount int) error {
 	return stakeValidator(fmt.Sprintf(""%d"", amount))
 }
 
-func theUserShouldBeAbleToUnstakeTheirWallet() error {
+func theUserShouldBeAbleToUnstakeTheirValidator() error {
 	return unstakeValidator()
 }
 
-// sends amount of POKT from v1-validator-001 to v1-validator-002
-func theUserSendsPOKTToAnotherAddress(amount int) error {
+// sends amount from v1-validator-001 to v1-validator-002
+func theUserSendsToAnotherAddress(amount int) error {
 	privateKey := getPrivateKey(validatorKeys, validatorA)
 	valB := getPrivateKey(validatorKeys, validatorB)
 	args := []string{
@@ -128,23 +135,19 @@

 		valB.Address().String(),
 		fmt.Sprintf(""%d"", amount),
 	}
-	validator.RunCommand(args...)
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
 // stakeValidator runs Validator stake command with the address, amount, chains..., and serviceURL provided
 func stakeValidator(amount string) error {
 	privateKey := getPrivateKey(validatorKeys, validatorA)
-	validatorServiceUrl := fmt.Sprintf(""v1-validator%s:%d"", validatorA, defaults.DefaultP2PPort)
+	validatorServiceUrl := fmt.Sprintf(validatorServiceURLTmpl, validatorA, defaults.DefaultP2PPort)
 	args := []string{
-		// NB: ignore passing a --pwd flag because
-		// validator keys have empty passwords
 		""--non_interactive=true"",
 		""--remote_cli_url="" + rpcURL,
 		""Validator"",
@@ -155,11 +158,10 @@

 		validatorServiceUrl,
 	}
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
@@ -174,11 +176,10 @@

 		privKey.Address().String(),
 	}
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
@@ -187,11 +188,11 @@

 	privHexString := keyMap[validatorId]
 	keyPair, err := cryptoPocket.CreateNewKeyFromString(privHexString, """", """")
 	if err != nil {
-		log.Fatalf(""failed to extract keypair %+v"", err)
+		logger.Fatal().Err(err).Msg(""failed to extract keypair"")
 	}
 	privateKey, err := keyPair.Unarmour("""")
 	if err != nil {
-		log.Fatalf(""failed to extract keypair %+v"", err)
+		logger.Fatal().Err(err).Msg(""failed to extract privkey"")
 	}
 	return privateKey
 }
@@ -210,7 +211,7 @@

 	}
 	clientset, err := kubernetes.NewForConfig(kubeConfig)
 	if err != nil {
-		return nil, fmt.Errorf(""failed to get clientset from config: %w"", err)
+		return nil, fmt.Errorf(""failed to12gg get clientset from config: %w"", err)
 	}
 	return clientset, nil
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220554441,https://github.com/pokt-network/pocket/pull/803#discussion_r1220554441,,271,f37b394454f45c27d058dac20af974a6ed273903,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/service/service.go,utility/servicer/module.go,Why not use the `currentHeight` from the session?,"+}
+
+// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
+func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session, currentHeight int64) (*big.Int, error) {","--- 

+++ 

@@ -1,9 +1,8 @@

-package service
+package servicer
 
 import (
 	""bytes""
 	""encoding/hex""
-	""encoding/json""
 	""errors""
 	""fmt""
 	""io""
@@ -13,33 +12,39 @@

 	""sync""
 	""time""
 
-	""golang.org/x/exp/slices""
-
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
+	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 	""github.com/pokt-network/pocket/shared/utils""
 	typesUtil ""github.com/pokt-network/pocket/utility/types""
+	""golang.org/x/exp/slices""
 )
 
-// DISCUSS: where should the RelayAccracyParameter be defined?
-const RelayAccuracyParameter = 0.2
-
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
-	errValidateServicer    = errors.New(""relay does not match the servicer"")
-	errValidateApplication = errors.New(""relay failed application validation"")
-
-	_ modules.Servicer = &servicer{}
+	errValidateServicer    = errors.New(""relay failed servicer validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
+
+	_ modules.ServicerModule = &servicer{}
 )
 
+const (
+	ServicerModuleName = ""servicer""
+)
+
+// sessionTokens is used to cache the starting number of tokens available
+// during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	SessionNumber int64
-	Count         *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -49,19 +54,29 @@

 	logger *modules.Logger
 	config *configs.ServicerConfig
 
+	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
-	// totalTokens holds the total number of tokens assigned to this servicer for the app in the current session
-	// DISCUSS: considering the computational complexity, should we skip caching this value?
+	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
-func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return new(servicer).Create(bus, options...)
+var (
+	_ modules.ServicerModule = &servicer{}
+)
+
+func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.ServicerModule, error) {
+	m, err := new(servicer).Create(bus, options...)
+	if err != nil {
+		return nil, err
+	}
+	return m.(modules.ServicerModule), nil
 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
 	s := &servicer{
-		logger: logger.Global.CreateLoggerForModule(servicerModuleName),
+		totalTokens: make(map[string]*sessionTokens),
 	}
 
 	for _, option := range options {
@@ -70,19 +85,27 @@

 
 	bus.RegisterModule(s)
 
+	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
+
 	cfg := bus.GetRuntimeMgr().GetConfig()
-	s.config = cfg.Utility.ServicerConfig
+	s.config = cfg.Servicer
 
 	return s, nil
 }
 
+// TODO: implement this function
 func (s *servicer) Start() error {
-	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
-	return nil
-}
-
-func (*servicer) GetModuleName() string {
-	return servicerModuleName
+	s.logger.Info().Msg(""_ Servicer module started _"")
+	return nil
+}
+
+func (s *servicer) Stop() error {
+	s.logger.Info().Msg(""_ Servicer module stopped _"")
+	return nil
+}
+
+func (s *servicer) GetModuleName() string {
+	return ServicerModuleName
 }
 
 // HandleRelay processes a relay after performing validation.
@@ -101,8 +124,14 @@

 		return nil, fmt.Errorf(""Error executing relay: %w"", err)
 	}
 
-	// DISCUSS: should we validate the response from the node?
-	relayDigest, shouldStore, err := s.hasCollision(relay, response)
+	// TODO(M6): Look into data integrity checks and response validation.
+
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -110,83 +139,63 @@

 		return response, nil
 	}
 
-	height := s.GetBus().GetConsensusModule().CurrentHeight()
-	writeCtx, err := s.GetBus().GetPersistenceModule().NewRWContext(int64(height))
-	if err != nil {
-		return nil, fmt.Errorf(""Error getting a write context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	defer writeCtx.Release()
-
-	// DISCUSS: should we extend/use UnitOfWork for updating/retrieving token usage?
-	if err := writeCtx.RecordRelayService(relay.Meta.ApplicationAddress, relayDigest, relay, response); err != nil {
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
+	}
+
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
 	return response, nil
 }
 
-// hasCollision returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) hasCollision(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest []byte, collides bool, err error) {
-	relayBytes, err := marshal(relay, response)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
-	}
-
-	relayDigest := crypto.SHA3Hash(relayBytes)
-
+// isRelayVolumeApplicable returns:
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
+	}
+
+	relayDigest := crypto.SHA3Hash(relayReqResBytes)
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.hasCollisionOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
-	}
-
-	return signedDigest, collision, nil
-}
-
-// INCOMPLETE: implement this
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
+	}
+
+	return signedDigest, relayReqResBytes, collision, nil
+}
+
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) hasCollisionOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
-func marshal(request *coreTypes.Relay, response *coreTypes.RelayResponse) ([]byte, error) {
-	if request == nil || response == nil {
-		return nil, fmt.Errorf(""error marshalling: got nil value as input"")
-	}
-
-	s := struct {
-		*coreTypes.Relay
-		*coreTypes.RelayResponse
-	}{
-		request,
-		response,
-	}
-	return json.Marshal(s)
-}
-
-// executeRelay performs the passed relay using an HTTP request to the chain-specific target URL
+// executeRelay performs the passed relay using the correct method depending on the relay payload type.
 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
-	if relay.Meta == nil || relay.Meta.RelayChain == nil || relay.Meta.RelayChain.Id == """" {
-		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", relay.Meta.ApplicationAddress)
-	}
-
-	chainConfig, ok := s.config.Chains[relay.Meta.RelayChain.Id]
-	if !ok {
-		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", relay.Meta.RelayChain.Id, errValidateRelayMeta)
-	}
-
-	res, err := executeHTTPRequest(chainConfig, relay.Payload)
-	if err != nil {
-		return nil, fmt.Errorf(""Error executing HTTP request for relay on application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	return res, nil
+	switch payload := relay.RelayPayload.(type) {
+	case *coreTypes.Relay_JsonRpcPayload:
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
+	case *coreTypes.Relay_RestPayload:
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
+	default:
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+	}
 }
 
 // validateRelayMeta ensures the relay metadata is valid for being handled by the servicer
@@ -208,8 +217,8 @@

 }
 
 func (s *servicer) validateRelayChainSupport(relayChain *coreTypes.Identifiable, currentHeight int64) error {
-	if _, ok := s.config.Chains[relayChain.Id]; !ok {
-		return fmt.Errorf(""chain %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
+	if _, ok := s.config.Services[relayChain.Id]; !ok {
+		return fmt.Errorf(""service %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
 	}
 
 	// DISCUSS: either update NewReadContext to take a uint64, or the GetCurrentHeight to return an int64.
@@ -232,26 +241,27 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session, currentHeight int64) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session, currentHeight)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return fmt.Errorf(""Error getting read context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
-	}
-
-	usedAppSessionTokens, err := readCtx.GetServicerTokenUsage(session)
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
+	}
+
+	usedAppSessionTokens, err := localCtx.GetSessionTokensUsed(session)
 	if err != nil {
 		return fmt.Errorf(""Error getting servicer token usage: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -260,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -267,18 +280,21 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session, currentHeight int64) (*big.Int, error) {
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
+//
+//	If nothing is cached, the maximum number of session tokens is computed.
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.Count != nil && tokens.SessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.Count), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
 	//	This is distributed rate limiting (DRL): no need to know how many requests have
 	//		been performed for this application by other servicers. Instead, simply enforce
 	//		this servicer's share of the application's tokens for this session.
-	appSessionTokens, err := s.calculateAppSessionTokens(session.Application.StakedAmount, currentHeight)
+	appSessionTokens, err := s.calculateAppSessionTokens(session)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating application %s total tokens for session %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -290,7 +306,7 @@

 	// This multiplication is performed to minimize the chance of under-utilization of application's tokens,
 	//	while removing the overhead of communication between servicers which would be necessary otherwise.
 	// see https://arxiv.org/abs/2305.10672 for details on application and servicer distributed rate-limiting
-	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+RelayAccuracyParameter))
+	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+s.config.RelayMiningVolumeAccuracy))
 	roundedTokens, _ := adjustedTokens.Int(big.NewInt(1))
 
 	s.setAppSessionTokens(session, &sessionTokens{session.SessionNumber, roundedTokens})
@@ -300,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -329,6 +341,17 @@

 	return nil
 }
 
+// getSession returns a session for the current height and the passed relay
+func (s *servicer) getSession(relay *coreTypes.Relay) (*coreTypes.Session, error) {
+	height := s.GetBus().GetConsensusModule().CurrentHeight()
+	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
+	if err != nil {
+		return nil, fmt.Errorf(""failed to get a session for height %d for relay meta %s: %w"", height, relay.Meta, err)
+	}
+
+	return session, nil
+}
+
 // admitRelay decides whether the relay should be served
 func (s *servicer) admitRelay(relay *coreTypes.Relay) error {
 	// TODO: utility module should initialize the servicer (if this module instance is a servicer)
@@ -343,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
-	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
-	if err != nil {
-		return fmt.Errorf(""%s: failed to get a session for height %d for relay meta %s: %w"", errPrefix, height, relay.Meta, err)
-	}
-
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
+	session, err := s.getSession(relay)
+	if err != nil {
+		return err
+	}
+
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -358,35 +379,105 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session, int64(height)); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
-// of every session. Each servicer will serve a maximum of (Session Tokens / Number of Servicers in the Session) relays for the application
-func (s *servicer) calculateAppSessionTokens(appStakeStr string, currentHeight int64) (*big.Int, error) {
-	appStake, err := utils.StringToBigInt(appStakeStr)
-	if err != nil {
-		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", appStakeStr, coreTypes.ErrStringToBigInt(err))
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", currentHeight, err)
+// of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
+func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+	appStake, err := utils.StringToBigInt(session.Application.StakedAmount)
+	if err != nil {
+		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
+	}
+
+	// TODO(M5): find the right document to explain the following:
+	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
+	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
+	//		matches the beginning of the session.
+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", session.SessionHeight, err)
 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, currentHeight, err)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
 
 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
+}
+
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
+	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
+		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
+	}
+
+	serviceConfig, ok := s.config.Services[meta.RelayChain.Id]
+	if !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
+	if err != nil {
+		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
+	}
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
+	if err != nil {
+		return nil, err
+	}
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
+		req.Header.Set(k, v)
+	}
+	if req.Header.Get(""Content-Type"") == """" {
+		req.Header.Set(""Content-Type"", ""application/json"")
+	}
+
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
+	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
+	if err != nil {
+		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
+	}
+	defer resp.Body.Close()
+
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return nil, fmt.Errorf(""Error reading response body: %w"", err)
+	}
+
+	return &coreTypes.RelayResponse{Payload: string(body)}, nil
 }
 
 // IMPROVE: Add session height tolerance to account for session rollovers
@@ -404,188 +495,3 @@

 		sessionStartingBlock,
 		sessionLastBlock)
 }
-
-// TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1
-// TODO: remove: use coreTypes.Relay instead
-type Relay interface {
-	RelayPayload
-	RelayMeta
-}
-
-type RelayPayload interface {
-	GetData() string               // the actual data string for the external chain
-	GetMethod() string             // the http CRUD method
-	GetHTTPPath() string           // the HTTP Path
-	GetHeaders() map[string]string // http headers
-}
-
-type RelayMeta interface {
-	GetBlockHeight() int64 // the block height when the request is made
-	GetServicerPublicKey() crypto.PublicKey
-	GetRelayChain() RelayChain
-	GetGeoZone() GeoZone
-	GetToken() AAT
-	GetSignature() string
-}
-
-type RelayResponse interface {
-	Payload() string
-	ServicerSignature() string
-}
-
-type RelayChain Identifiable
-type GeoZone Identifiable
-
-type AAT interface {
-	GetVersion() string              // confirm a valid AAT version
-	GetApplicationPublicKey() string // confirm the identity/signature of the app
-	GetClientPublicKey() string      // confirm the identity/signature of the client
-	GetApplicationSignature() string // confirm the application signed the token
-}
-
-type Identifiable interface {
-	Name() string
-	ID() string
-}
-
-var _ Relay = &relay{}
-
-type relay struct{}
-
-// Validate a submitted relay by a client before servicing
-func (r *relay) Validate() coreTypes.Error {
-
-	// validate payload
-
-	// validate the metadata
-
-	// ensure the RelayChain is supported locally
-
-	// ensure session block height is current
-
-	// get the session context
-
-	// get the application object from the r.AAT()
-
-	// get session node count from that session height
-
-	// get maximum possible relays for the application
-
-	// ensure not over serviced
-
-	// generate the session from seed data
-
-	// validate self against the session
-
-	return nil
-}
-
-// Store a submitted relay by a client for volume tracking
-func (r *relay) Store() coreTypes.Error {
-
-	// marshal relay object into protoBytes
-
-	// calculate the hashOf(protoBytes) <needed for volume tracking>
-
-	// persist relay object, indexing under session
-
-	return nil
-}
-
-// Execute a submitted relay by a client after validation
-func (r *relay) Execute() (RelayResponse, coreTypes.Error) {
-
-	// retrieve the RelayChain url from the servicer's local configuration file
-
-	// execute http request with the relay payload
-
-	// format and digitally sign the response
-
-	return nil, nil
-}
-
-// Get volume metric applicable relays from store
-func (r *relay) ReapStoreForHashCollision(sessionBlockHeight int64, hashEndWith string) ([]Relay, coreTypes.Error) {
-
-	// Pull all relays whose hash collides with the revealed secret key
-	// It's important to note, the secret key isn't revealed by the network until the session is over
-	// to prevent volume based bias. The secret key is usually a pseudorandom selection using the block hash as a seed.
-	// (See the session protocol)
-	//
-	// Demonstrable pseudocode below:
-	//   `SELECT * from RELAY where HashOf(relay) ends with hashEndWith AND sessionBlockHeight=sessionBlockHeight`
-
-	// This function also signifies deleting the non-volume-applicable Relays
-
-	return nil, nil
-}
-
-// Report volume metric applicable relays to Fisherman
-func (r *relay) ReportVolumeMetrics(fishermanServiceURL string, volumeRelays []Relay) coreTypes.Error {
-
-	// Send all volume applicable relays to the assigned trusted Fisherman for
-	// a proper verification of the volume completed. Send volumeRelays to fishermanServiceURL
-	// through http.
-
-	// NOTE: an alternative design is a 2 step, claim - proof lifecycle where the individual servicers
-	// build a merkle sum index tree from all the relays, submits a root and subsequent merkle proof to the
-	// network.
-	//
-	// Pros: Can report volume metrics directly to the chain in a trustless fashion
-	// Cons: Large chain bloat, non-trivial compute requirement for creation of claim/proof transactions and trees,
-	//       non-trivial compute requirement to process claim / proofs during ApplyBlock()
-
-	return nil
-}
-
-func (r *relay) GetData() string                        { return """" }
-func (r *relay) GetMethod() string                      { return """" }
-func (r *relay) GetHTTPPath() string                    { return """" }
-func (r *relay) GetHeaders() map[string]string          { return nil }
-func (r *relay) GetBlockHeight() int64                  { return 0 }
-func (r *relay) GetServicerPublicKey() crypto.PublicKey { return nil }
-func (r *relay) GetRelayChain() RelayChain              { return nil }
-func (r *relay) GetGeoZone() GeoZone                    { return nil }
-func (r *relay) GetToken() AAT                          { return nil }
-func (r *relay) GetSignature() string                   { return """" }
-
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func executeHTTPRequest(cfg *configs.ChainConfig, relay *coreTypes.RelayPayload) (*coreTypes.RelayResponse, error) {
-	chainUrl, err := url.Parse(cfg.Url)
-	if err != nil {
-		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", cfg.Url, err)
-	}
-	targetUrl := chainUrl.JoinPath(relay.HttpPath)
-
-	req, err := http.NewRequest(relay.Method, targetUrl.String(), bytes.NewBuffer([]byte(relay.Data)))
-	if err != nil {
-		return nil, err
-	}
-	if cfg.BasicAuth != nil && cfg.BasicAuth.UserName != """" {
-		req.SetBasicAuth(cfg.BasicAuth.UserName, cfg.BasicAuth.Password)
-	}
-	if cfg.UserAgent != """" {
-		req.Header.Set(""User-Agent"", cfg.UserAgent)
-	}
-
-	for k, v := range relay.Headers {
-		req.Header.Set(k, v)
-	}
-	if len(relay.Headers) == 0 {
-		req.Header.Set(""Content-Type"", ""application/json"")
-	}
-
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
-	resp, err := (&http.Client{Timeout: time.Duration(cfg.TimeoutMilliseconds) * time.Millisecond}).Do(req)
-	if err != nil {
-		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
-	}
-	defer resp.Body.Close()
-
-	body, err := io.ReadAll(resp.Body)
-	if err != nil {
-		return nil, fmt.Errorf(""Error reading response body: %w"", err)
-	}
-
-	return &coreTypes.RelayResponse{Payload: string(body)}, nil
-}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1210872347,https://github.com/pokt-network/pocket/pull/778#discussion_r1210872347,,10,3f7c8f63ff1d05e64f8672113a70545632501fca,a516c51f2d000d25f1469d28e6586ef7875d5a37,shared/core/types/relay.go,nan,"I suggest changing
 ```
+	errInvalidRelayInvalidPayload  = errors.New(""invalid relay payload"")
```
 to
```
+	// TODO: Once the proto structures are finalized, add an exhaustive list of errors and tests
+	errInvalidRelayInvalidPayload  = errors.New(""invalid relay payload"")
```","+)
+
+var (
+	errInvalidRelayInvalidPayload  = errors.New(""invalid relay payload"")","--- 

+++ 

@@ -6,36 +6,40 @@

 	""fmt""
 )
 
+const jsonRpcVersion = ""2.0""
+
 var (
-	errInvalidRelayInvalidPayload  = errors.New(""invalid relay payload"")
-	errInvalidJSONRPCInvalidRPC    = errors.New(""invalid value for JSONRPC field"")
+	// TODO: Once the proto structures are finalized, add an exhaustive list of errors and tests
+	errInvalidRelayPayload         = errors.New(""invalid relay payload"")
+	errInvalidJSONRPC              = errors.New(""invalid value for JSONRPC field"")
 	errInvalidJSONRPCMissingMethod = errors.New(""Method field not set"")
 	errInvalidRESTPayload          = errors.New(""invalid REST payload"")
 )
 
+// IMPROVE: use a factory function to build test relays
 // INCOMPLETE: perform any possible metadata validation
 // Validate performs validation on the relay payload
-func (r Relay) Validate() error {
-	if jsonRpcPayload := r.GetJsonRpcPayload(); jsonRpcPayload != nil {
-		return jsonRpcPayload.Validate()
+func (r *Relay) Validate() error {
+	switch payload := r.RelayPayload.(type) {
+	case *Relay_JsonRpcPayload:
+		return payload.JsonRpcPayload.Validate()
+	case *Relay_RestPayload:
+		return payload.RestPayload.Validate()
+	default:
+		return fmt.Errorf(""%w: %v"", errInvalidRelayPayload, r)
 	}
-
-	if jsonPayload := r.GetRestPayload(); jsonPayload != nil {
-		return jsonPayload.Validate()
-	}
-
-	return fmt.Errorf(""%w: %v"", errInvalidRelayInvalidPayload, r)
 }
 
 // Validate performs validation on JSONRPC payload. More specifically, it verifies that:
 //  1. The JSONRPC field is set to ""2.0"" as per the JSONRPC spec requirement, and
 //  2. The Method field is not empty
-func (p JSONRPCPayload) Validate() error {
-	if p.JsonRpc != ""2.0"" {
-		return fmt.Errorf(""%w: %s"", errInvalidJSONRPCInvalidRPC, p.JsonRpc)
+func (p *JSONRPCPayload) Validate() error {
+	if p.JsonRpc != jsonRpcVersion {
+		return fmt.Errorf(""%w: %s"", errInvalidJSONRPC, p.JsonRpc)
 	}
 
 	// DISCUSS: do we need/want chain-specific validation? Potential for reusing the existing logic of Portal V2/pocket-go
+	//	Potential items to consider when validating: number of RelayChains to stake, permissioned RelayChains, other types of services, e.g. <Type>.<ID>.<Config>
 	if p.Method == """" {
 		return errInvalidJSONRPCMissingMethod
 	}
@@ -43,11 +47,10 @@

 }
 
 // Validate verifies that the payload is valid REST, i.e. valid JSON
-func (p RESTPayload) Validate() error {
+func (p *RESTPayload) Validate() error {
 	var parsed json.RawMessage
-	err := json.Unmarshal([]byte(p.Contents), &parsed)
-	if err != nil {
-		return fmt.Errorf(""%w: %s: %w"", errInvalidRESTPayload, p.Contents, err)
+	if err := json.Unmarshal([]byte(p.Contents), &parsed); err != nil {
+		return fmt.Errorf(""%w: %s: %s"", errInvalidRESTPayload, p.Contents, err.Error())
 	}
 	return nil
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160213967,https://github.com/pokt-network/pocket/pull/652#discussion_r1160213967,,20,f092dcd17501dfcb1376f815c09a480a63b1f6d2,c9f18a38f7eaa521dc9be5e477fe860fea301fe8,utility/unit_of_work/block.go,nan,Should we put this log in the helper function?,"+		""source"": ""beginBlock"",
+	}).Logger()
+
+	log.Debug().Bool(""TODO"", true).Msg(""determining prevBlockByzantineValidators"")","--- 

+++ 

@@ -12,6 +12,10 @@

 	typesUtil ""github.com/pokt-network/pocket/utility/types""
 )
 
+const (
+	IgnoreProposalBlockCheckHash = ""0100000000000000000000000000000000000000000000000000000000000010""
+)
+
 func (uow *baseUtilityUnitOfWork) beginBlock() typesUtil.Error {
 	log := uow.logger.With().Fields(map[string]interface{}{
 		""source"": ""beginBlock"",
@@ -60,14 +64,14 @@

 }
 
 func (uow *baseUtilityUnitOfWork) handleProposerRewards(proposer []byte) typesUtil.Error {
-	feePoolName := coreTypes.Pools_POOLS_FEE_COLLECTOR.FriendlyName()
-	feesAndRewardsCollected, err := uow.getPoolAmount(feePoolName)
+	feePoolAddress := coreTypes.Pools_POOLS_FEE_COLLECTOR.Address()
+	feesAndRewardsCollected, err := uow.getPoolAmount(feePoolAddress)
 	if err != nil {
 		return err
 	}
 
 	// Nullify the rewards pool
-	if err := uow.setPoolAmount(feePoolName, big.NewInt(0)); err != nil {
+	if err := uow.setPoolAmount(feePoolAddress, big.NewInt(0)); err != nil {
 		return err
 	}
 
@@ -90,7 +94,7 @@

 	if err := uow.addAccountAmount(proposer, amountToProposer); err != nil {
 		return err
 	}
-	if err := uow.addPoolAmount(coreTypes.Pools_POOLS_DAO.FriendlyName(), amountToDAO); err != nil {
+	if err := uow.addPoolAmount(coreTypes.Pools_POOLS_DAO.Address(), amountToDAO); err != nil {
 		return err
 	}
 	return nil
@@ -104,22 +108,22 @@

 		actorType := coreTypes.ActorType(actorTypeNum)
 
 		var readyToUnbond []*moduleTypes.UnstakingActor
-		var poolName string
+		var poolAddress []byte
 
 		var er error
 		switch actorType {
 		case coreTypes.ActorType_ACTOR_TYPE_APP:
 			readyToUnbond, er = uow.persistenceReadContext.GetAppsReadyToUnstake(uow.height, int32(coreTypes.StakeStatus_Unstaking))
-			poolName = coreTypes.Pools_POOLS_APP_STAKE.FriendlyName()
+			poolAddress = coreTypes.Pools_POOLS_APP_STAKE.Address()
 		case coreTypes.ActorType_ACTOR_TYPE_FISH:
 			readyToUnbond, er = uow.persistenceReadContext.GetFishermenReadyToUnstake(uow.height, int32(coreTypes.StakeStatus_Unstaking))
-			poolName = coreTypes.Pools_POOLS_FISHERMAN_STAKE.FriendlyName()
+			poolAddress = coreTypes.Pools_POOLS_FISHERMAN_STAKE.Address()
 		case coreTypes.ActorType_ACTOR_TYPE_SERVICER:
 			readyToUnbond, er = uow.persistenceReadContext.GetServicersReadyToUnstake(uow.height, int32(coreTypes.StakeStatus_Unstaking))
-			poolName = coreTypes.Pools_POOLS_SERVICER_STAKE.FriendlyName()
+			poolAddress = coreTypes.Pools_POOLS_SERVICER_STAKE.Address()
 		case coreTypes.ActorType_ACTOR_TYPE_VAL:
 			readyToUnbond, er = uow.persistenceReadContext.GetValidatorsReadyToUnstake(uow.height, int32(coreTypes.StakeStatus_Unstaking))
-			poolName = coreTypes.Pools_POOLS_VALIDATOR_STAKE.FriendlyName()
+			poolAddress = coreTypes.Pools_POOLS_VALIDATOR_STAKE.Address()
 		case coreTypes.ActorType_ACTOR_TYPE_UNSPECIFIED:
 			continue
 		}
@@ -140,7 +144,7 @@

 				return typesUtil.ErrHexDecodeFromString(err)
 			}
 
-			if err := uow.subPoolAmount(poolName, stakeAmount); err != nil {
+			if err := uow.subPoolAmount(poolAddress, stakeAmount); err != nil {
 				return err
 			}
 			if err := uow.addAccountAmount(outputAddrBz, stakeAmount); err != nil {",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160231836,https://github.com/pokt-network/pocket/pull/652#discussion_r1160231836,,49,f092dcd17501dfcb1376f815c09a480a63b1f6d2,c9f18a38f7eaa521dc9be5e477fe860fea301fe8,utility/unit_of_work/module.go,nan,I think we can remove everything other than `err` from the return value,"-	lastByzantineValidators, err := u.prevBlockByzantineValidators()
-	if err != nil {
-		return """", nil, err
+func (uow *baseUtilityUnitOfWork) ApplyBlock() (stateHash string, txs [][]byte, err error) {","--- 

+++ 

@@ -1,6 +1,8 @@

 package unit_of_work
 
 import (
+	""fmt""
+
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/mempool""
 	""github.com/pokt-network/pocket/shared/modules""
@@ -36,6 +38,8 @@

 	proposalStateHash    string
 	proposalProposerAddr []byte
 	proposalBlockTxs     [][]byte
+
+	stateHash string
 }
 
 func (uow *baseUtilityUnitOfWork) SetProposalBlock(blockHash string, proposerAddr []byte, txs [][]byte) error {
@@ -45,46 +49,58 @@

 	return nil
 }
 
-// CLEANUP: code re-use ApplyBlock() for CreateAndApplyBlock()
-func (uow *baseUtilityUnitOfWork) ApplyBlock() (stateHash string, txs [][]byte, err error) {
+func (uow *baseUtilityUnitOfWork) ApplyBlock() error {
 	log := uow.logger.With().Fields(map[string]interface{}{
 		""source"": ""ApplyBlock"",
 	}).Logger()
 
 	log.Debug().Msg(""checking if proposal block has been set"")
 	if !uow.isProposalBlockSet() {
-		return """", nil, utilTypes.ErrProposalBlockNotSet()
+		return utilTypes.ErrProposalBlockNotSet()
 	}
 
 	// begin block lifecycle phase
 	log.Debug().Msg(""calling beginBlock"")
 	if err := uow.beginBlock(); err != nil {
-		return """", nil, err
+		return err
 	}
 
 	log.Debug().Msg(""processing transactions from proposal block"")
 	txMempool := uow.GetBus().GetUtilityModule().GetMempool()
-	if err := uow.processTransactionsFromProposalBlock(txMempool, uow.proposalBlockTxs); err != nil {
-		return """", nil, err
+	if err := uow.processTransactionsFromProposalBlock(txMempool); err != nil {
+		return err
 	}
 
 	// end block lifecycle phase
 	log.Debug().Msg(""calling endBlock"")
 	if err := uow.endBlock(uow.proposalProposerAddr); err != nil {
-		return """", nil, err
+		return err
 	}
-	// TODO(@deblasis): this should be from a ReadContext (the ephemeral/staging one)
 	// return the app hash (consensus module will get the validator set directly)
 	log.Debug().Msg(""computing state hash"")
-	stateHash, err = uow.persistenceRWContext.ComputeStateHash()
+	stateHash, err := uow.persistenceRWContext.ComputeStateHash()
 	if err != nil {
-		log.Fatal().Err(err).Msg(""Updating the app hash failed. TODO: Look into roll-backing the entire commit..."")
-		return """", nil, utilTypes.ErrAppHash(err)
+		log.Fatal().Err(err).Bool(""TODO"", true).Msg(""Updating the app hash failed. TODO: Look into roll-backing the entire commit..."")
+		return utilTypes.ErrAppHash(err)
 	}
+
+	// IMPROVE(#655): this acts as a feature flag to allow tests to ignore the check if needed, ideally the tests should have a way to determine
+	// the hash and set it into the proposal block it's currently hard to do because the state is different at every test run (non-determinism)
+	if uow.proposalStateHash != IgnoreProposalBlockCheckHash {
+		if uow.proposalStateHash != stateHash {
+			log.Fatal().Bool(""TODO"", true).
+				Str(""proposalStateHash"", uow.proposalStateHash).
+				Str(""stateHash"", stateHash).
+				Msg(""State hash mismatch. TODO: Look into roll-backing the entire commit..."")
+			return utilTypes.ErrAppHash(fmt.Errorf(""state hash mismatch: expected %s from the proposal, got %s"", uow.proposalStateHash, stateHash))
+		}
+	}
+
 	log.Info().Str(""state_hash"", stateHash).Msgf(""ApplyBlock succeeded!"")
 
-	// return the app hash; consensus module will get the validator set directly
-	return stateHash, nil, nil
+	uow.stateHash = stateHash
+
+	return nil
 }
 
 // TODO(@deblasis): change tracking here
@@ -114,14 +130,16 @@

 	return nil
 }
 
+// isProposalBlockSet returns true if the proposal block has been set.
+// TODO: it should also check that uow.proposalBlockTxs is not empty but if we do, tests fail.
 func (uow *baseUtilityUnitOfWork) isProposalBlockSet() bool {
 	return uow.proposalStateHash != """" && uow.proposalProposerAddr != nil
 }
 
 // processTransactionsFromProposalBlock processes the transactions from the proposal block.
-// It also removes the transactions from the mempool if they are already present.
-func (uow *baseUtilityUnitOfWork) processTransactionsFromProposalBlock(txMempool mempool.TXMempool, txsBytes [][]byte) (err error) {
-	for index, txProtoBytes := range txsBytes {
+// It also removes the transactions from the mempool if they are also present.
+func (uow *baseUtilityUnitOfWork) processTransactionsFromProposalBlock(txMempool mempool.TXMempool) (err error) {
+	for index, txProtoBytes := range uow.proposalBlockTxs {
 		tx, err := coreTypes.TxFromBytes(txProtoBytes)
 		if err != nil {
 			return err
@@ -151,8 +169,13 @@

 
 		// TODO(#564): make sure that indexing is reversible in case of a rollback
 		if err := uow.persistenceRWContext.IndexTransaction(txResult); err != nil {
-			uow.logger.Fatal().Err(err).Msgf(""TODO(#327): We can apply the transaction but not index it. Crash the process for now: %v\n"", err)
+			uow.logger.Fatal().Err(err).Msg(""TODO(#327): We can apply the transaction but not index it. Crash the process for now"")
 		}
 	}
 	return nil
 }
+
+// GetStateHash returns the state hash of the unit of work. It is only available after the block has been applied.
+func (uow *baseUtilityUnitOfWork) GetStateHash() string {
+	return uow.stateHash
+}",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/Codrux2200/Rtype/pulls/comments/1376324113,https://github.com/Codrux2200/Rtype/pull/28#discussion_r1376324113,,7,41021c60e8db8324c6ccaecb897673652cf814f9,9312baf88c376ebeb1a0ca5971d9c249c9a369c9,build.bat,nan,Why ? It can take a long time to rebuild ??,"+move build\Release\rtype-server.exe .
+move build\Release\rtype-client.exe .
+
+del /s /q build","--- 

+++ 

@@ -1,8 +1,11 @@

+for /f %%a in ('wmic cpu get NumberOfCores ^| findstr /r ""[0-9]""') do (
+    set ""cores=%%a""
+)
+
 cmake -B build
 cmake --build build --config Release -j %cores%
 
 move build\Release\rtype-server.exe .
 move build\Release\rtype-client.exe .
 
-del /s /q build
 :end",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828143165,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1828143165,119,126,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,nan,"I suggest changing
 ```
+    """"""Tracking data from SLEAP for multi-animal experiments.
+
+    Tracked objects position data from a particular
+    VideoSource for multi-animal experiment using the SLEAP tracking
+    method per chunk.
+    """"""
+
+    definition = """"""
```
 to
```
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Tracked objects position data from a particular
+VideoSource for multi-animal experiment using the SLEAP tracking method per chunk.
```
We can still keep the definition of the table right?","+    """"""Tracking data from SLEAP for multi-animal experiments.
+
+    Tracked objects position data from a particular
+    VideoSource for multi-animal experiment using the SLEAP tracking
+    method per chunk.
+    """"""
+
+    definition = """"""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1181995263,https://github.com/pokt-network/pocket/pull/684#discussion_r1181995263,,142,964333fcec0ccb79708af5ce07dbab1cf5efb3a5,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/handlers.go,nan,Ditto as above. Please specify its techdebt and WILL need to change,"+	})
+}
+
+// DISCUSSION: This may need to be changed when the HandleChallenge function is actually implemented","--- 

+++ 

@@ -80,7 +80,8 @@

 	})
 }
 
-// DISCUSSION: This may need to be changed when the SendRelay function is actually implemented
+// TECHDEBT: This will need to be changed when the HandleRelay function is actually implemented
+// because it copies data structures from v0. For example, AATs are no longer necessary in v1.
 func (s *rpcServer) PostV1ClientRelay(ctx echo.Context) error {
 	var body RelayRequest
 	if err := ctx.Bind(&body); err != nil {
@@ -128,7 +129,7 @@

 		Meta:    relayMeta,
 	}
 
-	relayResponse, err := s.GetBus().GetUtilityModule().SendRelay(relayRequest)
+	relayResponse, err := s.GetBus().GetUtilityModule().HandleRelay(relayRequest)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -139,7 +140,8 @@

 	})
 }
 
-// DISCUSSION: This may need to be changed when the HandleChallenge function is actually implemented
+// TECHDEBT: This will need to be changed when the HandleChallenge function is actually implemented
+// because it copies data structures from v0
 func (s *rpcServer) PostV1ClientChallenge(ctx echo.Context) error {
 	var body ChallengeRequest
 	if err := ctx.Bind(&body); err != nil {",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826121570,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1826121570,66,73,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/analysis/block_plotting.py,nan,"I suggest changing
 ```
+    """"""Based on a list of patches, generates a dictionary of the following items.
+
     - patch_colors_dict: patch name to color
     - patch_markers_dict: patch name to marker
     - patch_symbols_dict: patch name to symbol
     - patch_linestyles_dict: patch name to linestyle
+
     """"""
```
 to
```
+    """"""
+    Generates a dictionary of patch styles given a list of patch_names.
+
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
+
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
+    """"""
```","+    """"""Based on a list of patches, generates a dictionary of the following items.
+
     - patch_colors_dict: patch name to color
     - patch_markers_dict: patch name to marker
     - patch_symbols_dict: patch name to symbol
     - patch_linestyles_dict: patch name to linestyle
+
     """"""","--- 

+++ 

@@ -34,11 +34,11 @@

     )
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        curl_lightness = (lightness * val) + (
+        cur_lightness = (lightness * val) + (
             min_lightness * (1 - val)
         )  # get cur lightness relative to `hex_col`
-        curl_lightness = max(min(curl_lightness, lightness), min_lightness)  # set min, max bounds
-        cur_rgb_col = hls_to_rgb(hue, curl_lightness, saturation)  # convert to rgb
+        cur_lightness = max(min(cur_lightness, lightness), min_lightness)  # set min, max bounds
+        cur_rgb_col = hls_to_rgb(hue, cur_lightness, saturation)  # convert to rgb
         cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
             *tuple(int(c * 255) for c in cur_rgb_col)
         )  # convert to hex
@@ -63,13 +63,15 @@

 
 
 def gen_patch_style_dict(patch_names):
-    """"""Based on a list of patches, generates a dictionary of the following items.
+    """"""Generates a dictionary of patch styles given a list of patch_names.
 
-    - patch_colors_dict: patch name to color
-    - patch_markers_dict: patch name to marker
-    - patch_symbols_dict: patch name to symbol
-    - patch_linestyles_dict: patch name to linestyle
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
 
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
     """"""
     return {
         ""colors"": dict(zip(patch_names, patch_colors, strict=False)),",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1253758605,https://github.com/pokt-network/pocket/pull/732#discussion_r1253758605,,125,4f879217cbd947cb98258e447397b7cc8f063884,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/README.md,nan,"I suggest changing
 ```
+
```
 to
```
+
+_tl;dr An instance (i.e. client) implements an interface (i.e. supplier)_
+
```"," ```
 
+#### Interface Realization
+","--- 

+++ 

@@ -7,6 +7,8 @@

 - [Definitions](#definitions)
 - [Interface & Integration](#interface--integration)
 - [Module Architecture](#module-architecture)
+  - [Architecture Design Language](#architecture-design-language)
+  - [Legends](#legends)
   - [P2P Module / Router Decoupling](#p2p-module--router-decoupling)
   - [Message Propagation & Handling](#message-propagation--handling)
   - [Message Deduplication](#message-deduplication)
@@ -71,6 +73,8 @@

 
 ## Module Architecture
 
+_(TODO: move ""arch. design lang."" & ""legends"" sections into `shared` to support common usage)_
+
 ### Architecture Design Language
 
 The architecture design language expressed in this documentation is based on [UML](https://www.uml-diagrams.org/).
@@ -123,6 +127,8 @@

 
 #### Interface Realization
 
+_TL;DR An instance (i.e. client) implements the associated interface (i.e. supplierl)._
+
 > Realization is a specialized abstraction relationship between two sets of model elements, one representing a specification (the supplier) and the other represents an implementation of the latter (the client).
 
 > Realization can be used to model stepwise refinement, optimizations, transformations, templates, model synthesis, framework composition, etc.
@@ -131,6 +137,8 @@

 
 #### Direct Usage
 
+_TL;DR one instance (i.e. client) is dependent the associated instance(s) (i.e. supplier) to function properly._
+
 > Dependency is a directed relationship which is used to show that some UML element or a set of elements requires, needs or depends on other model elements for specification or implementation. Because of this, dependency is called a supplier - client relationship, where supplier provides something to the client, and thus the client is in some sense incomplete while semantically or structurally dependent on the supplier element(s). Modification of the supplier may impact the client elements.
 
 > Usage is a dependency in which one named element (client) requires another named element (supplier) for its full definition or implementation.
@@ -139,6 +147,8 @@

 
 #### Composition
 
+_TL;DR deleting an instance also deletes the associated instance(s)._
+
 > A ""strong"" form of aggregation
 
 > If a composite (whole) is deleted, all of its composite parts are ""normally"" deleted with it.
@@ -147,7 +157,10 @@

 
 #### Aggregation
 
-> A ""weak"" form of composition
+
+_TL;DR deleting an instance does not necessarily delete the associated instance(s)._
+
+> A ""weak"" form of aggregation
 
 > Shared part could be included in several composites, and if some or all of the composites are deleted, shared part may still exist.
 
@@ -155,7 +168,11 @@

 
 #### Cardinality
 
-Cardinality indicates the number or range of simultaneous instances of the classifier at the ""cardinality-side"" association end that are associated with the classifier at the other end of the given association type.
+_TL;DR indicates a number, or range of instances associated (i.e. supplier(s))_
+
+Cardinality indicates the number or range of simultaneous instances of supplier that are associated with the client.
+Applicable to multiple association types.
+Can be expressed arbitrarily (e.g. wildcards, variable, equation, etc.)
 
 _(see: [UML Association](https://www.uml-diagrams.org/association.html#association-end))_
 
@@ -169,19 +186,21 @@

 
 **Unicast**
 
-| Sender         | Receiver       | Router          | Example Usage                                        |
-|----------------|----------------|-----------------|------------------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree only   | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Staked Actor   | Background only | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Unstaked Actor | Background only | Consensus (state sync) & Debug (CLI) messages        |
+| Sender         | Receiver       | Router          | Example Usage                                                        |
+|----------------|----------------|-----------------|----------------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree only   | Consensus hotstuff messages (validators only) & state sync responses |
+| Staked Actor   | Untaked Actor  | Background only | Consensus state sync responses                                       |
+| Unstaked Actor | Staked Actor   | Background only | Consensus state sync responses, debug messages                       |
+| Unstaked Actor | Unstaked Actor | Background only | Consensus state sync responses, debug messages                       |
 
 **Broadcast**
 
-| Broadcaster    | Receiver       | Router                | Example Usage                              |
-|----------------|----------------|-----------------------|--------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages                        |
-| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (gossipsub redundancy) |
-| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages                        |
+| Broadcaster    | Receiver       | Router                | Example Usage                                                   |
+|----------------|----------------|-----------------------|-----------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages, consensus state sync requests              |
+| Staked Actor   | Untaked Actor  | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages, consensus state sync requests              |
 
 Both router submodule implementations embed a `UnicastRouter` which enables them to send and receive messages directly to/from a single peer.
 
@@ -486,7 +505,7 @@

 This pairing always has an associated TTL (time-to-live), near the end of which it must
 be refreshed.
 
-In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves 7/8th through their TTL.
+In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves every 3 hours through their TTL (see: [`RoutingDiscovery#Advertise()`](https://github.com/libp2p/go-libp2p/blob/87c2561238cb0340ddb182c61be8dbbc7a12a780/p2p/discovery/routing/routing.go#L34) and [`ProviderManager#AddProvider()`](https://github.com/libp2p/go-libp2p-kad-dht/blob/v0.24.2/providers/providers_manager.go#L255)).
 This refreshes the libp2p peerstore automatically.
 
 In the raintree gossip overlay network (`raintreeRouter`), the libp2p peerstore is **NOT** currently refreshed _(TODO: [#859](https://github.com/pokt-network/network/isues/859))_.",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1210847204,https://github.com/pokt-network/pocket/pull/778#discussion_r1210847204,,23,3f7c8f63ff1d05e64f8672113a70545632501fca,a516c51f2d000d25f1469d28e6586ef7875d5a37,shared/core/types/proto/session.proto,nan,"I suggest changing
 ```
+    // IMPROVE: a map may be a better choice for storing servicers/fishermen
```
 to
```
+    // IMPROVE: `map<string, core.Actor>` with the address as the key can simplify and optimize the logic on the clients
```

One piece of feedback I'll provide is adding more context/detail to the TODO-like comments you're adding.

You, as the author, will understand it. I, as the person who has full context, will understand it, but others might not. If we step away from some part of the code, we may be in the same shoes too.

IMO it takes an extra 30 seconds but goes a long way.","     // CONSIDERATION: Should a single session support multiple geo zones?
     string geo_zone = 6; // the target geographic region where the actors are present
     core.Actor application = 7; // the application that is being served
+    // IMPROVE: a map may be a better choice for storing servicers/fishermen","--- 

+++ 

@@ -11,7 +11,7 @@

 message Session {
     string id = 1; // a universally unique ID for the session
     int64 session_number = 2; // a monotonically increasing number representing the # on the chain
-    int64 session_height = 3; // the block height at which this session started
+    int64 session_height = 3; // the number of blocks (out of numBlocksPerSession) in this session
     int64 num_session_blocks = 4; // the number of blocks the session is valid from
      // CONSIDERATION: Should we add a `RelayChain` enum and use it across the board?
      // CONSIDERATION: Should a single session support multiple relay chains?
@@ -20,7 +20,7 @@

     // CONSIDERATION: Should a single session support multiple geo zones?
     string geo_zone = 6; // the target geographic region where the actors are present
     core.Actor application = 7; // the application that is being served
-    // IMPROVE: a map may be a better choice for storing servicers/fishermen
+    // IMPROVE: `map<string, core.Actor>` with the address as the key can simplify and optimize the logic on the clients
     repeated core.Actor servicers = 8; // the set of servicers that are serving the application
     repeated core.Actor fishermen = 9; // the set of fishermen that are fishing for servicers
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220544069,https://github.com/pokt-network/pocket/pull/803#discussion_r1220544069,,120,f37b394454f45c27d058dac20af974a6ed273903,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/service/service.go,utility/servicer/module.go,"This is a good question. `UnitOfWork` was meant to be more related to a single, atomic, rollbackable state transition related to on-chain things. A session is more of an ""off chain"" behaviour (spanning multiple blocks) that eventually transfers into a state transition.

Does that make sense? If so, can you add some of this context to the `UnitOfWork` interface?","+	}
+	defer writeCtx.Release()
+
+	// DISCUSS: should we extend/use UnitOfWork for updating/retrieving token usage?","--- 

+++ 

@@ -1,9 +1,8 @@

-package service
+package servicer
 
 import (
 	""bytes""
 	""encoding/hex""
-	""encoding/json""
 	""errors""
 	""fmt""
 	""io""
@@ -13,33 +12,39 @@

 	""sync""
 	""time""
 
-	""golang.org/x/exp/slices""
-
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
+	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 	""github.com/pokt-network/pocket/shared/utils""
 	typesUtil ""github.com/pokt-network/pocket/utility/types""
+	""golang.org/x/exp/slices""
 )
 
-// DISCUSS: where should the RelayAccracyParameter be defined?
-const RelayAccuracyParameter = 0.2
-
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
-	errValidateServicer    = errors.New(""relay does not match the servicer"")
-	errValidateApplication = errors.New(""relay failed application validation"")
-
-	_ modules.Servicer = &servicer{}
+	errValidateServicer    = errors.New(""relay failed servicer validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
+
+	_ modules.ServicerModule = &servicer{}
 )
 
+const (
+	ServicerModuleName = ""servicer""
+)
+
+// sessionTokens is used to cache the starting number of tokens available
+// during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	SessionNumber int64
-	Count         *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -49,19 +54,29 @@

 	logger *modules.Logger
 	config *configs.ServicerConfig
 
+	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
-	// totalTokens holds the total number of tokens assigned to this servicer for the app in the current session
-	// DISCUSS: considering the computational complexity, should we skip caching this value?
+	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
-func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return new(servicer).Create(bus, options...)
+var (
+	_ modules.ServicerModule = &servicer{}
+)
+
+func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.ServicerModule, error) {
+	m, err := new(servicer).Create(bus, options...)
+	if err != nil {
+		return nil, err
+	}
+	return m.(modules.ServicerModule), nil
 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
 	s := &servicer{
-		logger: logger.Global.CreateLoggerForModule(servicerModuleName),
+		totalTokens: make(map[string]*sessionTokens),
 	}
 
 	for _, option := range options {
@@ -70,19 +85,27 @@

 
 	bus.RegisterModule(s)
 
+	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
+
 	cfg := bus.GetRuntimeMgr().GetConfig()
-	s.config = cfg.Utility.ServicerConfig
+	s.config = cfg.Servicer
 
 	return s, nil
 }
 
+// TODO: implement this function
 func (s *servicer) Start() error {
-	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
-	return nil
-}
-
-func (*servicer) GetModuleName() string {
-	return servicerModuleName
+	s.logger.Info().Msg(""_ Servicer module started _"")
+	return nil
+}
+
+func (s *servicer) Stop() error {
+	s.logger.Info().Msg(""_ Servicer module stopped _"")
+	return nil
+}
+
+func (s *servicer) GetModuleName() string {
+	return ServicerModuleName
 }
 
 // HandleRelay processes a relay after performing validation.
@@ -101,8 +124,14 @@

 		return nil, fmt.Errorf(""Error executing relay: %w"", err)
 	}
 
-	// DISCUSS: should we validate the response from the node?
-	relayDigest, shouldStore, err := s.hasCollision(relay, response)
+	// TODO(M6): Look into data integrity checks and response validation.
+
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -110,83 +139,63 @@

 		return response, nil
 	}
 
-	height := s.GetBus().GetConsensusModule().CurrentHeight()
-	writeCtx, err := s.GetBus().GetPersistenceModule().NewRWContext(int64(height))
-	if err != nil {
-		return nil, fmt.Errorf(""Error getting a write context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	defer writeCtx.Release()
-
-	// DISCUSS: should we extend/use UnitOfWork for updating/retrieving token usage?
-	if err := writeCtx.RecordRelayService(relay.Meta.ApplicationAddress, relayDigest, relay, response); err != nil {
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
+	}
+
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
 	return response, nil
 }
 
-// hasCollision returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) hasCollision(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest []byte, collides bool, err error) {
-	relayBytes, err := marshal(relay, response)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
-	}
-
-	relayDigest := crypto.SHA3Hash(relayBytes)
-
+// isRelayVolumeApplicable returns:
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
+	}
+
+	relayDigest := crypto.SHA3Hash(relayReqResBytes)
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.hasCollisionOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
-	}
-
-	return signedDigest, collision, nil
-}
-
-// INCOMPLETE: implement this
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
+	}
+
+	return signedDigest, relayReqResBytes, collision, nil
+}
+
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) hasCollisionOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
-func marshal(request *coreTypes.Relay, response *coreTypes.RelayResponse) ([]byte, error) {
-	if request == nil || response == nil {
-		return nil, fmt.Errorf(""error marshalling: got nil value as input"")
-	}
-
-	s := struct {
-		*coreTypes.Relay
-		*coreTypes.RelayResponse
-	}{
-		request,
-		response,
-	}
-	return json.Marshal(s)
-}
-
-// executeRelay performs the passed relay using an HTTP request to the chain-specific target URL
+// executeRelay performs the passed relay using the correct method depending on the relay payload type.
 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
-	if relay.Meta == nil || relay.Meta.RelayChain == nil || relay.Meta.RelayChain.Id == """" {
-		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", relay.Meta.ApplicationAddress)
-	}
-
-	chainConfig, ok := s.config.Chains[relay.Meta.RelayChain.Id]
-	if !ok {
-		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", relay.Meta.RelayChain.Id, errValidateRelayMeta)
-	}
-
-	res, err := executeHTTPRequest(chainConfig, relay.Payload)
-	if err != nil {
-		return nil, fmt.Errorf(""Error executing HTTP request for relay on application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	return res, nil
+	switch payload := relay.RelayPayload.(type) {
+	case *coreTypes.Relay_JsonRpcPayload:
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
+	case *coreTypes.Relay_RestPayload:
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
+	default:
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+	}
 }
 
 // validateRelayMeta ensures the relay metadata is valid for being handled by the servicer
@@ -208,8 +217,8 @@

 }
 
 func (s *servicer) validateRelayChainSupport(relayChain *coreTypes.Identifiable, currentHeight int64) error {
-	if _, ok := s.config.Chains[relayChain.Id]; !ok {
-		return fmt.Errorf(""chain %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
+	if _, ok := s.config.Services[relayChain.Id]; !ok {
+		return fmt.Errorf(""service %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
 	}
 
 	// DISCUSS: either update NewReadContext to take a uint64, or the GetCurrentHeight to return an int64.
@@ -232,26 +241,27 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session, currentHeight int64) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session, currentHeight)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return fmt.Errorf(""Error getting read context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
-	}
-
-	usedAppSessionTokens, err := readCtx.GetServicerTokenUsage(session)
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
+	}
+
+	usedAppSessionTokens, err := localCtx.GetSessionTokensUsed(session)
 	if err != nil {
 		return fmt.Errorf(""Error getting servicer token usage: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -260,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -267,18 +280,21 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session, currentHeight int64) (*big.Int, error) {
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
+//
+//	If nothing is cached, the maximum number of session tokens is computed.
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.Count != nil && tokens.SessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.Count), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
 	//	This is distributed rate limiting (DRL): no need to know how many requests have
 	//		been performed for this application by other servicers. Instead, simply enforce
 	//		this servicer's share of the application's tokens for this session.
-	appSessionTokens, err := s.calculateAppSessionTokens(session.Application.StakedAmount, currentHeight)
+	appSessionTokens, err := s.calculateAppSessionTokens(session)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating application %s total tokens for session %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -290,7 +306,7 @@

 	// This multiplication is performed to minimize the chance of under-utilization of application's tokens,
 	//	while removing the overhead of communication between servicers which would be necessary otherwise.
 	// see https://arxiv.org/abs/2305.10672 for details on application and servicer distributed rate-limiting
-	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+RelayAccuracyParameter))
+	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+s.config.RelayMiningVolumeAccuracy))
 	roundedTokens, _ := adjustedTokens.Int(big.NewInt(1))
 
 	s.setAppSessionTokens(session, &sessionTokens{session.SessionNumber, roundedTokens})
@@ -300,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -329,6 +341,17 @@

 	return nil
 }
 
+// getSession returns a session for the current height and the passed relay
+func (s *servicer) getSession(relay *coreTypes.Relay) (*coreTypes.Session, error) {
+	height := s.GetBus().GetConsensusModule().CurrentHeight()
+	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
+	if err != nil {
+		return nil, fmt.Errorf(""failed to get a session for height %d for relay meta %s: %w"", height, relay.Meta, err)
+	}
+
+	return session, nil
+}
+
 // admitRelay decides whether the relay should be served
 func (s *servicer) admitRelay(relay *coreTypes.Relay) error {
 	// TODO: utility module should initialize the servicer (if this module instance is a servicer)
@@ -343,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
-	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
-	if err != nil {
-		return fmt.Errorf(""%s: failed to get a session for height %d for relay meta %s: %w"", errPrefix, height, relay.Meta, err)
-	}
-
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
+	session, err := s.getSession(relay)
+	if err != nil {
+		return err
+	}
+
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -358,35 +379,105 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session, int64(height)); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
-// of every session. Each servicer will serve a maximum of (Session Tokens / Number of Servicers in the Session) relays for the application
-func (s *servicer) calculateAppSessionTokens(appStakeStr string, currentHeight int64) (*big.Int, error) {
-	appStake, err := utils.StringToBigInt(appStakeStr)
-	if err != nil {
-		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", appStakeStr, coreTypes.ErrStringToBigInt(err))
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", currentHeight, err)
+// of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
+func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+	appStake, err := utils.StringToBigInt(session.Application.StakedAmount)
+	if err != nil {
+		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
+	}
+
+	// TODO(M5): find the right document to explain the following:
+	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
+	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
+	//		matches the beginning of the session.
+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", session.SessionHeight, err)
 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, currentHeight, err)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
 
 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
+}
+
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
+	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
+		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
+	}
+
+	serviceConfig, ok := s.config.Services[meta.RelayChain.Id]
+	if !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
+	if err != nil {
+		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
+	}
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
+	if err != nil {
+		return nil, err
+	}
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
+		req.Header.Set(k, v)
+	}
+	if req.Header.Get(""Content-Type"") == """" {
+		req.Header.Set(""Content-Type"", ""application/json"")
+	}
+
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
+	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
+	if err != nil {
+		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
+	}
+	defer resp.Body.Close()
+
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return nil, fmt.Errorf(""Error reading response body: %w"", err)
+	}
+
+	return &coreTypes.RelayResponse{Payload: string(body)}, nil
 }
 
 // IMPROVE: Add session height tolerance to account for session rollovers
@@ -404,188 +495,3 @@

 		sessionStartingBlock,
 		sessionLastBlock)
 }
-
-// TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1
-// TODO: remove: use coreTypes.Relay instead
-type Relay interface {
-	RelayPayload
-	RelayMeta
-}
-
-type RelayPayload interface {
-	GetData() string               // the actual data string for the external chain
-	GetMethod() string             // the http CRUD method
-	GetHTTPPath() string           // the HTTP Path
-	GetHeaders() map[string]string // http headers
-}
-
-type RelayMeta interface {
-	GetBlockHeight() int64 // the block height when the request is made
-	GetServicerPublicKey() crypto.PublicKey
-	GetRelayChain() RelayChain
-	GetGeoZone() GeoZone
-	GetToken() AAT
-	GetSignature() string
-}
-
-type RelayResponse interface {
-	Payload() string
-	ServicerSignature() string
-}
-
-type RelayChain Identifiable
-type GeoZone Identifiable
-
-type AAT interface {
-	GetVersion() string              // confirm a valid AAT version
-	GetApplicationPublicKey() string // confirm the identity/signature of the app
-	GetClientPublicKey() string      // confirm the identity/signature of the client
-	GetApplicationSignature() string // confirm the application signed the token
-}
-
-type Identifiable interface {
-	Name() string
-	ID() string
-}
-
-var _ Relay = &relay{}
-
-type relay struct{}
-
-// Validate a submitted relay by a client before servicing
-func (r *relay) Validate() coreTypes.Error {
-
-	// validate payload
-
-	// validate the metadata
-
-	// ensure the RelayChain is supported locally
-
-	// ensure session block height is current
-
-	// get the session context
-
-	// get the application object from the r.AAT()
-
-	// get session node count from that session height
-
-	// get maximum possible relays for the application
-
-	// ensure not over serviced
-
-	// generate the session from seed data
-
-	// validate self against the session
-
-	return nil
-}
-
-// Store a submitted relay by a client for volume tracking
-func (r *relay) Store() coreTypes.Error {
-
-	// marshal relay object into protoBytes
-
-	// calculate the hashOf(protoBytes) <needed for volume tracking>
-
-	// persist relay object, indexing under session
-
-	return nil
-}
-
-// Execute a submitted relay by a client after validation
-func (r *relay) Execute() (RelayResponse, coreTypes.Error) {
-
-	// retrieve the RelayChain url from the servicer's local configuration file
-
-	// execute http request with the relay payload
-
-	// format and digitally sign the response
-
-	return nil, nil
-}
-
-// Get volume metric applicable relays from store
-func (r *relay) ReapStoreForHashCollision(sessionBlockHeight int64, hashEndWith string) ([]Relay, coreTypes.Error) {
-
-	// Pull all relays whose hash collides with the revealed secret key
-	// It's important to note, the secret key isn't revealed by the network until the session is over
-	// to prevent volume based bias. The secret key is usually a pseudorandom selection using the block hash as a seed.
-	// (See the session protocol)
-	//
-	// Demonstrable pseudocode below:
-	//   `SELECT * from RELAY where HashOf(relay) ends with hashEndWith AND sessionBlockHeight=sessionBlockHeight`
-
-	// This function also signifies deleting the non-volume-applicable Relays
-
-	return nil, nil
-}
-
-// Report volume metric applicable relays to Fisherman
-func (r *relay) ReportVolumeMetrics(fishermanServiceURL string, volumeRelays []Relay) coreTypes.Error {
-
-	// Send all volume applicable relays to the assigned trusted Fisherman for
-	// a proper verification of the volume completed. Send volumeRelays to fishermanServiceURL
-	// through http.
-
-	// NOTE: an alternative design is a 2 step, claim - proof lifecycle where the individual servicers
-	// build a merkle sum index tree from all the relays, submits a root and subsequent merkle proof to the
-	// network.
-	//
-	// Pros: Can report volume metrics directly to the chain in a trustless fashion
-	// Cons: Large chain bloat, non-trivial compute requirement for creation of claim/proof transactions and trees,
-	//       non-trivial compute requirement to process claim / proofs during ApplyBlock()
-
-	return nil
-}
-
-func (r *relay) GetData() string                        { return """" }
-func (r *relay) GetMethod() string                      { return """" }
-func (r *relay) GetHTTPPath() string                    { return """" }
-func (r *relay) GetHeaders() map[string]string          { return nil }
-func (r *relay) GetBlockHeight() int64                  { return 0 }
-func (r *relay) GetServicerPublicKey() crypto.PublicKey { return nil }
-func (r *relay) GetRelayChain() RelayChain              { return nil }
-func (r *relay) GetGeoZone() GeoZone                    { return nil }
-func (r *relay) GetToken() AAT                          { return nil }
-func (r *relay) GetSignature() string                   { return """" }
-
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func executeHTTPRequest(cfg *configs.ChainConfig, relay *coreTypes.RelayPayload) (*coreTypes.RelayResponse, error) {
-	chainUrl, err := url.Parse(cfg.Url)
-	if err != nil {
-		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", cfg.Url, err)
-	}
-	targetUrl := chainUrl.JoinPath(relay.HttpPath)
-
-	req, err := http.NewRequest(relay.Method, targetUrl.String(), bytes.NewBuffer([]byte(relay.Data)))
-	if err != nil {
-		return nil, err
-	}
-	if cfg.BasicAuth != nil && cfg.BasicAuth.UserName != """" {
-		req.SetBasicAuth(cfg.BasicAuth.UserName, cfg.BasicAuth.Password)
-	}
-	if cfg.UserAgent != """" {
-		req.Header.Set(""User-Agent"", cfg.UserAgent)
-	}
-
-	for k, v := range relay.Headers {
-		req.Header.Set(k, v)
-	}
-	if len(relay.Headers) == 0 {
-		req.Header.Set(""Content-Type"", ""application/json"")
-	}
-
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
-	resp, err := (&http.Client{Timeout: time.Duration(cfg.TimeoutMilliseconds) * time.Millisecond}).Do(req)
-	if err != nil {
-		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
-	}
-	defer resp.Body.Close()
-
-	body, err := io.ReadAll(resp.Body)
-	if err != nil {
-		return nil, fmt.Errorf(""Error reading response body: %w"", err)
-	}
-
-	return &coreTypes.RelayResponse{Payload: string(body)}, nil
-}",,,,