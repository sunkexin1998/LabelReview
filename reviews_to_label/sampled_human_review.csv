source_action,Comment_URL,Comment_HTML_URL,Review_Start_Line,Review_End_Line,Original_Commit_id,Merge_Commit_id,Diff_path,New_path,Body,Diff_hunk,Change_Until_Merged,Whether it contain issues or suggestions (Not Contain: 0; Contain: 1),List of issues or suggestions,Addressed Status Classification (Not Enough Informationï¼š-1; Not Addressed: 0; Partly Addressed: 1; Fully Addressed: 2),Detail
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1171806715,https://github.com/pokt-network/pocket/pull/683#discussion_r1171806715,,197,cc15ba1ea64bab29113ec8247bc952b2cc657856,fb245bc027ae55aaee936daf9f85ecb2d9ad8b40,utility/session.go,nan,"""fishermen"" ","+		return err
+	}
+
+	// returns all the staked fisherman at this session height","--- 

+++ 

@@ -7,32 +7,17 @@

 	""math""
 	""math/rand""
 
+	""golang.org/x/exp/slices""
+
 	""github.com/pokt-network/pocket/logger""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/utility/types""
-	""golang.org/x/exp/slices""
 )
 
-// sessionHydrator is an internal structure used to prepare a Session returned by `GetSession` below
-type sessionHydrator struct {
-	logger modules.Logger
-
-	// The height of the request for which the session is being hydrated
-	blockHeight int64
-
-	// The session being hydrated and returned
-	session *coreTypes.Session
-
-	// Caches a readCtx to avoid draining too many connections to the database
-	readCtx modules.PersistenceReadContext
-
-	// A redundant helper that maintains a hex decoded copy of `session.Id` used for session hydration
-	sessionIdBz []byte
-}
-
-// GetSession is an implementation of the exposed `UtilityModule.GetSession` function
+// GetSession implements of the exposed `UtilityModule.GetSession` function
+// TECHDEBT(#519): Add custom error types depending on the type of issue that occurred and assert on them in the unit tests.
 func (m *utilityModule) GetSession(appAddr string, height int64, relayChain, geoZone string) (*coreTypes.Session, error) {
 	persistenceModule := m.GetBus().GetPersistenceModule()
 	readCtx, err := persistenceModule.NewReadContext(height)
@@ -48,56 +33,71 @@

 
 	sessionHydrator := &sessionHydrator{
 		logger:      m.logger.With().Str(""source"", ""sessionHydrator"").Logger(),
+		session:     session,
 		blockHeight: height,
-		session:     session,
 		readCtx:     readCtx,
 	}
 
-	if err := sessionHydrator.hydrateSessionHeight(height); err != nil {
-		return nil, err
+	if err := sessionHydrator.hydrateSessionMetadata(); err != nil {
+		return nil, fmt.Errorf(""failed to hydrate session metadata: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionApplication(appAddr); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session application: %w"", err)
 	}
 
 	if err := sessionHydrator.validateApplicationSession(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to validate application session: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionID(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session ID: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionServicers(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session servicers: %w"", err)
 	}
 
 	if err := sessionHydrator.hydrateSessionFishermen(); err != nil {
-		return nil, err
+		return nil, fmt.Errorf(""failed to hydrate session fishermen: %w"", err)
 	}
 
 	return sessionHydrator.session, nil
 }
 
-// hydrateSessionHeight hydrates the height at which the session started given the current block height
-func (s *sessionHydrator) hydrateSessionHeight(blockHeight int64) error {
-	numBlocksPerSession, err := s.readCtx.GetIntParam(types.BlocksPerSessionParamName, blockHeight)
-	if err != nil {
-		return err
-	}
-	numBlocksAheadOfSession := blockHeight % int64(numBlocksPerSession)
+type sessionHydrator struct {
+	logger modules.Logger
+
+	// The session being hydrated and returned
+	session *coreTypes.Session
+
+	// The height at which the request is being made to get session information
+	blockHeight int64
+
+	// Caches a readCtx to avoid draining too many connections to the database
+	readCtx modules.PersistenceReadContext
+
+	// A redundant helper that maintains a hex decoded copy of `session.Id` used for session hydration
+	sessionIdBz []byte
+}
+
+// hydrateSessionMetadata hydrates the height at which the session started, its number, and the number of blocks per session
+func (s *sessionHydrator) hydrateSessionMetadata() error {
+	numBlocksPerSession, err := s.readCtx.GetIntParam(types.BlocksPerSessionParamName, s.blockHeight)
+	if err != nil {
+		return err
+	}
+	numBlocksAheadOfSession := s.blockHeight % int64(numBlocksPerSession)
 
 	s.session.NumSessionBlocks = int64(numBlocksPerSession)
-	s.session.SessionNumber = int64(blockHeight / int64(numBlocksPerSession))
-	s.session.SessionHeight = blockHeight - numBlocksAheadOfSession
-	return nil
-}
-
-// hydrateSessionApplication hydrates the full Application actor based on the address the session is being
-// dispatched for.
+	s.session.SessionNumber = int64(s.blockHeight / int64(numBlocksPerSession))
+	s.session.SessionHeight = s.blockHeight - numBlocksAheadOfSession
+	return nil
+}
+
+// hydrateSessionApplication hydrates the full Application actor based on the address provided
 func (s *sessionHydrator) hydrateSessionApplication(appAddr string) error {
-	// TECHDEBT: We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
+	// TECHDEBT(#706): We can remove this decoding process once we use `strings` instead of `[]byte` for addresses
 	addr, err := hex.DecodeString(appAddr)
 	if err != nil {
 		return err
@@ -106,20 +106,22 @@

 	return err
 }
 
-// validateApplicationSession validates that the application can dispatch a session at the requested geo zone and for the request relay chain
+// validateApplicationSession validates that the application can have a valid session for the provided relay chain and geo zone
 func (s *sessionHydrator) validateApplicationSession() error {
-	// TODO(#XXX): Filter by geo-zone
 	app := s.session.Application
 
 	if !slices.Contains(app.Chains, s.session.RelayChain) {
 		return fmt.Errorf(""application %s does not stake for relay chain %s"", app.Address, s.session.RelayChain)
 	}
 
-	if !(app.PausedHeight == -1 && app.UnstakingHeight == -1) {
+	if app.PausedHeight != -1 || app.UnstakingHeight != -1 {
 		return fmt.Errorf(""application %s is either unstaked or paused"", app.Address)
 	}
 
-	// TODO: Consider what else we should validate for here
+	// TODO(#697): Filter by geo-zone
+
+	// INVESTIGATE: Consider what else we should validate for here (e.g. Application stake amount, etc.)
+
 	return nil
 }
 
@@ -135,6 +137,9 @@

 	}
 	prevHashBz, err := hex.DecodeString(prevHash)
 
+	if err != nil {
+		return err
+	}
 	appPubKeyBz := []byte(s.session.Application.PublicKey)
 	relayChainBz := []byte(string(s.session.RelayChain))
 	geoZoneBz := []byte(s.session.GeoZone)
@@ -163,21 +168,14 @@

 	candidateServicers := make([]*coreTypes.Actor, 0)
 	for _, servicer := range servicers {
 		// Sanity check the servicer is not paused, jailed or unstaking
-		if !(servicer.PausedHeight == -1 && servicer.UnstakingHeight == -1) {
+		if servicer.PausedHeight != -1 || servicer.UnstakingHeight != -1 {
 			return fmt.Errorf(""hydrateSessionServicers should not have encountered a paused or unstaking servicer: %s"", servicer.Address)
 		}
 
-		// TODO(#XXX): Filter by geo-zone
-
-		// OPTIMIZE: If this was a map[string]struct{}, we could have avoided the loop
-		var chain string
-		for _, chain = range servicer.Chains {
-			if chain != s.session.RelayChain {
-				chain = """"
-				continue
-			}
-		}
-		if chain != """" {
+		// TECHDEBT(#697): Filter by geo-zone
+
+		// OPTIMIZE: If `servicer.Chains` was a map[string]struct{}, we could eliminate `slices.Contains()`'s loop
+		if slices.Contains(servicer.Chains, s.session.RelayChain) {
 			candidateServicers = append(candidateServicers, servicer)
 		}
 	}
@@ -188,13 +186,13 @@

 
 // hydrateSessionFishermen finds the fishermen that are staked at the session height and populates the session with them
 func (s *sessionHydrator) hydrateSessionFishermen() error {
-	// number of fisherman per session at this height
+	// number of fishermen per session at this height
 	numFishermen, err := s.readCtx.GetIntParam(types.FishermanPerSessionParamName, s.session.SessionHeight)
 	if err != nil {
 		return err
 	}
 
-	// returns all the staked fisherman at this session height
+	// returns all the staked fishermen at this session height
 	fishermen, err := s.readCtx.GetAllFishermen(s.session.SessionHeight)
 	if err != nil {
 		return err
@@ -202,24 +200,17 @@

 
 	// OPTIMIZE: Consider updating the persistence module so a single SQL query can retrieve all of the actors at once.
 	candidateFishermen := make([]*coreTypes.Actor, 0)
-	for _, fisherman := range fishermen {
-		// Sanity check the fisherman is not paused, jailed or unstaking
-		if !(fisherman.PausedHeight == -1 && fisherman.UnstakingHeight == -1) {
-			return fmt.Errorf(""hydrateSessionFishermen should not have encountered a paused or unstaking fisherman: %s"", fisherman.Address)
-		}
-
-		// TODO(#XXX): Filter by geo-zone
+	for _, fisher := range fishermen {
+		// Sanity check the fisher is not paused, jailed or unstaking
+		if fisher.PausedHeight != -1 || fisher.UnstakingHeight != -1 {
+			return fmt.Errorf(""hydrateSessionFishermen should not have encountered a paused or unstaking fisherman: %s"", fisher.Address)
+		}
+
+		// TODO(#697): Filter by geo-zone
 
 		// OPTIMIZE: If this was a map[string]struct{}, we could have avoided the loop
-		var chain string
-		for _, chain = range fisherman.Chains {
-			if chain != s.session.RelayChain {
-				chain = """"
-				continue
-			}
-		}
-		if chain != """" {
-			candidateFishermen = append(candidateFishermen, fisherman)
+		if slices.Contains(fisher.Chains, s.session.RelayChain) {
+			candidateFishermen = append(candidateFishermen, fisher)
 		}
 	}
 
@@ -228,8 +219,8 @@

 }
 
 // pseudoRandomSelection returns a random subset of the candidates.
-// TECHDEBT: We are using a `Go` native implementation for a pseudo-random number generator. In order
-// for it to be language agnostic, a general purpose algorithm needs ot be used.
+// DECIDE: We are using a `Go` native implementation for a pseudo-random number generator. In order
+// for it to be language agnostic, a general purpose algorithm MUST be used.
 func pseudoRandomSelection(candidates []*coreTypes.Actor, numTarget int, sessionId []byte) []*coreTypes.Actor {
 	// If there aren't enough candidates, return all of them
 	if numTarget > len(candidates) {
@@ -238,6 +229,7 @@

 	}
 
 	// Take the first 8 bytes of sessionId to use as the seed
+	// NB: There is specific reason why `BigEndian` was chosen over `LittleEndian` in this specific context.
 	seed := int64(binary.BigEndian.Uint64(crypto.SHA3Hash(sessionId)[:8]))
 
 	// Retrieve the indices for the candidates
@@ -252,10 +244,11 @@

 
 // OPTIMIZE: Postgres uses a `Twisted Mersenne Twister (TMT)` randomness algorithm.
 // We could potentially look into changing everything into a single SQL query but
-// would nee dto verify that it can be implemented in a platform agnostic way.
+// would need to verify that it can be implemented in a platform agnostic way.
 
 // uniqueRandomIndices returns a map of `numIndices` unique random numbers less than `maxIndex`
 // seeded by `seed`.
+// panics if `numIndicies > maxIndex` since that code path SHOULD never be executed.
 // NB: A map pointing to empty structs is used to simulate set behaviour.
 func uniqueRandomIndices(seed, maxIndex, numIndices int64) map[int64]struct{} {
 	// This should never happen",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160180621,https://github.com/pokt-network/pocket/pull/653#discussion_r1160180621,,24,e5cf35e950020a6e16608501447b8b1c774485c9,f4b9e504bd881326f9528ee370010ca55b67141f,e2e/tests/steps_init_test.go,nan,Can you add a comment on the key-value types here?,"+
+var (
+	// validatorKeys is hydrated by the clientset with credentials for all validators
+	validatorKeys map[string]string","--- 

+++ 

@@ -4,49 +4,60 @@

 
 import (
 	""fmt""
-	""log""
 	""os""
 	""path/filepath""
 	""strings""
 	""testing""
 
+	pocketLogger ""github.com/pokt-network/pocket/logger""
 	""github.com/pokt-network/pocket/runtime/defaults""
 	cryptoPocket ""github.com/pokt-network/pocket/shared/crypto""
 	pocketk8s ""github.com/pokt-network/pocket/shared/k8s""
+
+	""github.com/cucumber/godog""
 	""k8s.io/client-go/kubernetes""
 	""k8s.io/client-go/tools/clientcmd""
-
-	""github.com/cucumber/godog""
 )
 
 var (
-	// validatorKeys is hydrated by the clientset with credentials for all validators
+	logger = pocketLogger.Global.CreateLoggerForModule(""e2e"")
+
+	// validatorKeys is hydrated by the clientset with credentials for all validators.
+	// validatorKeys maps validator IDs to their private key as a hex string.
 	validatorKeys map[string]string
 	// clientset is the kubernetes API we acquire from the user's $HOME/.kube/config
 	clientset *kubernetes.Clientset
 	// validator holds command results between runs and reports errors to the test suite
 	validator = &validatorPod{}
 	// validatorA maps to suffix ID 001 of the kube pod that we use as our control agent
-	validatorA string = ""001""
-	// validatorB maps to suffix ID 002
-	validatorB string = ""002""
-	chainId           = ""0001""
+)
+
+const (
+	// defines the host & port scheme that LocalNet uses for naming validators.
+	// e.g. v1-validator-001 thru v1-validator-999
+	validatorServiceURLTmpl = ""v1-validator%s:%d""
+	// validatorA maps to suffix ID 001 and is also used by the cluster-manager
+	// though it has no special permissions.
+	validatorA = ""001""
+	// validatorB maps to suffix ID 002 and receives in the Send test.
+	validatorB = ""002""
+	chainId    = ""0001""
 )
 
 func init() {
 	cs, err := getClientset()
 	if err != nil {
-		log.Fatalf(""failed to get clientset: %v"", err)
+		logger.Fatal().Err(err).Msg(""failed to get clientset"")
 	}
 	clientset = cs
 	vkmap, err := pocketk8s.FetchValidatorPrivateKeys(clientset)
 	if err != nil {
-		log.Fatalf(""failed to get validator keys: %v"", err)
+		logger.Fatal().Err(err).Msg(""failed to get validator key map"")
 	}
 	validatorKeys = vkmap
 }
 
-// TestFeatures runs the e2e tests specifiedin any .features files in this directory
+// TestFeatures runs the e2e tests specified in any .features files in this directory
 // * This test suite assumes that a LocalNet is running that can be accessed by `kubectl`
 func TestFeatures(t *testing.T) {
 	suite := godog.TestSuite{
@@ -68,19 +79,18 @@

 	ctx.Step(`^the user should be able to see standard output containing ""([^""]*)""$`, theUserShouldBeAbleToSeeStandardOutputContaining)
 	ctx.Step(`^the user has a validator$`, theUserHasAValidator)
 	ctx.Step(`^the validator should have exited without error$`, theValidatorShouldHaveExitedWithoutError)
-	ctx.Step(`^the user stakes their validator with (\d+) POKT$`, theUserStakesTheirValidatorWithPOKT)
-	ctx.Step(`^the user should be able to unstake their wallet$`, theUserShouldBeAbleToUnstakeTheirWallet)
-	ctx.Step(`^the user sends (\d+) POKT to another address$`, theUserSendsPOKTToAnotherAddress)
+	ctx.Step(`^the user stakes their validator with amount (\d+) uPOKT$`, theUserStakesTheirValidatorWith)
+	ctx.Step(`^the user should be able to unstake their validator$`, theUserShouldBeAbleToUnstakeTheirValidator)
+	ctx.Step(`^the user sends (\d+) uPOKT to another address$`, theUserSendsToAnotherAddress)
 }
 
 func theUserHasAValidator() error {
 	res, err := validator.RunCommand(""help"")
-	if err != nil {
-		log.Printf(""validator error: %+v"", err)
-		return err
-	}
-	validator.result = res
-	return err
+	validator.result = res
+	if err != nil {
+		return err
+	}
+	return nil
 }
 
 func theValidatorShouldHaveExitedWithoutError() error {
@@ -89,13 +99,10 @@

 
 func theUserRunsTheCommand(cmd string) error {
 	cmds := strings.Split(cmd, "" "")
-	result, err := validator.RunCommand(cmds...)
-	if err != nil {
-		validator.result = result
-		return err
-	}
-	if result.Err != nil {
-		return result.Err
+	res, err := validator.RunCommand(cmds...)
+	validator.result = res
+	if err != nil {
+		return err
 	}
 	return nil
 }
@@ -107,16 +114,16 @@

 	return nil
 }
 
-func theUserStakesTheirValidatorWithPOKT(amount int) error {
+func theUserStakesTheirValidatorWith(amount int) error {
 	return stakeValidator(fmt.Sprintf(""%d"", amount))
 }
 
-func theUserShouldBeAbleToUnstakeTheirWallet() error {
+func theUserShouldBeAbleToUnstakeTheirValidator() error {
 	return unstakeValidator()
 }
 
-// sends amount of POKT from v1-validator-001 to v1-validator-002
-func theUserSendsPOKTToAnotherAddress(amount int) error {
+// sends amount from v1-validator-001 to v1-validator-002
+func theUserSendsToAnotherAddress(amount int) error {
 	privateKey := getPrivateKey(validatorKeys, validatorA)
 	valB := getPrivateKey(validatorKeys, validatorB)
 	args := []string{
@@ -128,23 +135,19 @@

 		valB.Address().String(),
 		fmt.Sprintf(""%d"", amount),
 	}
-	validator.RunCommand(args...)
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
 // stakeValidator runs Validator stake command with the address, amount, chains..., and serviceURL provided
 func stakeValidator(amount string) error {
 	privateKey := getPrivateKey(validatorKeys, validatorA)
-	validatorServiceUrl := fmt.Sprintf(""v1-validator%s:%d"", validatorA, defaults.DefaultP2PPort)
+	validatorServiceUrl := fmt.Sprintf(validatorServiceURLTmpl, validatorA, defaults.DefaultP2PPort)
 	args := []string{
-		// NB: ignore passing a --pwd flag because
-		// validator keys have empty passwords
 		""--non_interactive=true"",
 		""--remote_cli_url="" + rpcURL,
 		""Validator"",
@@ -155,11 +158,10 @@

 		validatorServiceUrl,
 	}
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
@@ -174,11 +176,10 @@

 		privKey.Address().String(),
 	}
 	res, err := validator.RunCommand(args...)
-	if err != nil {
-		validator.result = res
-		return err
-	}
-	validator.result = res
+	validator.result = res
+	if err != nil {
+		return err
+	}
 	return nil
 }
 
@@ -187,11 +188,11 @@

 	privHexString := keyMap[validatorId]
 	keyPair, err := cryptoPocket.CreateNewKeyFromString(privHexString, """", """")
 	if err != nil {
-		log.Fatalf(""failed to extract keypair %+v"", err)
+		logger.Fatal().Err(err).Msg(""failed to extract keypair"")
 	}
 	privateKey, err := keyPair.Unarmour("""")
 	if err != nil {
-		log.Fatalf(""failed to extract keypair %+v"", err)
+		logger.Fatal().Err(err).Msg(""failed to extract privkey"")
 	}
 	return privateKey
 }
@@ -210,7 +211,7 @@

 	}
 	clientset, err := kubernetes.NewForConfig(kubeConfig)
 	if err != nil {
-		return nil, fmt.Errorf(""failed to get clientset from config: %w"", err)
+		return nil, fmt.Errorf(""failed to12gg get clientset from config: %w"", err)
 	}
 	return clientset, nil
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184436462,https://github.com/pokt-network/pocket/pull/707#discussion_r1184436462,,67,4159d7a40f8d99835d17a00a052b6782cb3f795d,d7cc85d553cf9b0257f16c3a213ed4de96b8363a,p2p/background/router.go,nan,"maybe a `CONSIDERATION: If switching to NewRandomSub, there will be a max size`","+		return nil, err
+	}
+
+	// NOTE_TO_SELF: `pubsub.NewRandomSub` requires a `size` arg.","--- 

+++ 

@@ -11,6 +11,7 @@

 	libp2pHost ""github.com/libp2p/go-libp2p/core/host""
 
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/p2p/config""
 	""github.com/pokt-network/pocket/p2p/protocol""
 	typesP2P ""github.com/pokt-network/pocket/p2p/types""
 	""github.com/pokt-network/pocket/p2p/utils""
@@ -38,23 +39,27 @@

 	// (i.e. multiple, unidentified receivers)
 	// TECHDEBT: investigate diff between randomSub and gossipSub
 	gossipSub *pubsub.PubSub
-	// topic similar to pubsub but received messages are filtered by a ""topic"" string.
+	// topic is similar to pubsub but received messages are filtered by a ""topic"" string.
 	// Published messages are also given the respective topic before broadcast.
 	topic *pubsub.Topic
 	// subscription provides an interface to continuously read messages from.
 	subscription *pubsub.Subscription
-	kadDHT       *dht.IpfsDHT
-	pstore       typesP2P.Peerstore
+	// kadDHT is a kademlia distributed hash table used for routing and peer discovery.
+	kadDHT *dht.IpfsDHT
+	// TECHDEBT: `pstore` will likely be removed in future refactoring / simplification
+	// of the `Router` interface.
+	// pstore is the background router's peerstore.
+	pstore typesP2P.Peerstore
 }
 
 // NewBackgroundRouter returns a `backgroundRouter` as a `typesP2P.Router`
 // interface using the given configuration.
-func NewBackgroundRouter(bus modules.Bus, cfg *utils.RouterConfig) (typesP2P.Router, error) {
+func NewBackgroundRouter(bus modules.Bus, cfg *config.BackgroundConfig) (typesP2P.Router, error) {
 	// TECHDEBT(#595): add ctx to interface methods and propagate down.
 	ctx := context.TODO()
 
 	networkLogger := logger.Global.CreateLoggerForModule(""backgroundRouter"")
-	networkLogger.Info().Msg(""Initializing background"")
+	networkLogger.Info().Msg(""Initializing background router"")
 
 	// seed initial peerstore with current on-chain peer info (i.e. staked actors)
 	pstore, err := cfg.PeerstoreProvider.GetStakedPeerstoreAtHeight(
@@ -64,7 +69,7 @@

 		return nil, err
 	}
 
-	// NOTE_TO_SELF: `pubsub.NewRandomSub` requires a `size` arg.
+	// CONSIDERATION: If switching to `NewRandomSub`, there will be a max size
 	gossipSub, err := pubsub.NewGossipSub(ctx, cfg.Host)
 	if err != nil {
 		return nil, fmt.Errorf(""creating gossip pubsub: %w"", err)",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/xun082/create-neat/pulls/comments/1528641886,https://github.com/xun082/create-neat/pull/59#discussion_r1528641886,,19,ce23fa78dfaf11626f80f5749501257e98004738,f805998e76fda454324dcbbc4daed50fbf7352f7,packages/core/package.json,nan,what,"   ""bin"": {
-    ""create-neat"": ""./dist/index.js""
+    ""create-neat"": ""./dist/index.js"",
+    ""c"": ""./dist/index.js""","--- 

+++ 

@@ -15,8 +15,7 @@

     ""directory"": ""packages/core""
   },
   ""bin"": {
-    ""create-neat"": ""./dist/index.js"",
-    ""c"": ""./dist/index.js""
+    ""create-neat"": ""./dist/index.js""
   },
   ""scripts"": {
     ""build"": ""tsc"",",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1174214813,https://github.com/pokt-network/pocket/pull/684#discussion_r1174214813,,9,33fefdfbfb6dbfb6107ab93e63c3abca7951a012,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,shared/core/types/proto/challenge.proto,nan,Can you add a `TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1; <insert_link_to_v0_ref_here>`,"+
+import ""relay.proto"";
+
+message Challenge {","--- 

+++ 

@@ -6,6 +6,8 @@

 
 import ""relay.proto"";
 
+// TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1
+// https://editor.swagger.io/?url=https://raw.githubusercontent.com/pokt-network/pocket-core/staging/doc/specs/rpc-spec.yaml
 message Challenge {
    string session_id = 1;
    string address = 2;",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829250184,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1829250184,152,153,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/api.py,nan,"I suggest changing
 ```
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
+                    stacklevel=2,
```
 to
```
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"", stacklevel=2
```
Revert black","+                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
+                    stacklevel=2,","--- 

+++ 

@@ -75,7 +75,7 @@

     :param datetime, optional end: The right bound of the time range to extract.
     :param datetime, optional time: An object or series specifying the timestamps to extract.
     :param datetime, optional tolerance:
-    The maximum distance between original and new timestamps for inexact matches.
+        The maximum distance between original and new timestamps for inexact matches.
     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
     :param optional kwargs: Optional keyword arguments to forward to the reader when reading chunk data.
     :return: A pandas data frame containing epoch event metadata, sorted by time.
@@ -149,15 +149,11 @@

 
             if not data.index.has_duplicates:
                 warnings.warn(
-                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
-                    stacklevel=2,
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"", stacklevel=2
                 )
                 data = data.sort_index()
             else:
-                warnings.warn(
-                    f""data index for {reader.pattern} contains duplicate keys!"",
-                    stacklevel=2,
-                )
+                warnings.warn(f""data index for {reader.pattern} contains duplicate keys!"", stacklevel=2)
                 data = data[~data.index.duplicated(keep=""first"")]
             return data.loc[start:end]
     return data",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228789399,https://github.com/pokt-network/pocket/pull/803#discussion_r1228789399,,240,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/servicer/module.go,nan,"1. Can't we just get it directly from `Session`?
2. If not, we should be able to do `s.GetBus().GetConsensusModule().CurrentHeight()`
3. If neither 1 or 2 work, this is `TECHDEBT`, not `IMPROVE` IMO","-	*/
-	return nil
+func (s *servicer) validateApplication(session *coreTypes.Session) error {
+	// IMPROVE: use a function to get current height from the current session","--- 

+++ 

@@ -13,6 +13,7 @@

 	""time""
 
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
@@ -24,11 +25,12 @@

 	""golang.org/x/exp/slices""
 )
 
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
 	errValidateServicer    = errors.New(""relay failed servicer validation"")
-	errValidateApplication = errors.New(""relay failed application validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
 
 	_ modules.ServicerModule = &servicer{}
 )
@@ -37,11 +39,12 @@

 	ServicerModuleName = ""servicer""
 )
 
-// sessionTokens is used to cache the original number of tokens available
+// sessionTokens is used to cache the starting number of tokens available
 // during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	sessionNumber          int64
-	originalCountAvailable *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -52,9 +55,10 @@

 	config *configs.ServicerConfig
 
 	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
 	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
-	// INVESTIGATE: considering the computational complexity, should we skip caching this value?
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
@@ -71,7 +75,9 @@

 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	s := &servicer{}
+	s := &servicer{
+		totalTokens: make(map[string]*sessionTokens),
+	}
 
 	for _, option := range options {
 		option(s)
@@ -120,7 +126,12 @@

 
 	// TODO(M6): Look into data integrity checks and response validation.
 
-	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(relay, response)
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -128,55 +139,50 @@

 		return response, nil
 	}
 
-	session, err := s.getSession(relay)
-	if err != nil {
-		return nil, err
-	}
-
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.StoreServiceRelay(session, relay.Meta.ApplicationAddress, relayDigest, relayReqResBytes); err != nil {
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.Release(); err != nil {
-		s.logger.Warn().Err(err).Msg(""failed to release local context"")
-	}
-
 	return response, nil
 }
 
 // isRelayVolumeApplicable returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) isRelayVolumeApplicable(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
 	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
 	if err != nil {
 		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
 	}
 
 	relayDigest := crypto.SHA3Hash(relayReqResBytes)
-
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.isRelayVolumeApplicableOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
 	}
 
 	return signedDigest, relayReqResBytes, collision, nil
 }
 
-// INCOMPLETE: implement this
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) isRelayVolumeApplicableOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
@@ -184,11 +190,11 @@

 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
 	switch payload := relay.RelayPayload.(type) {
 	case *coreTypes.Relay_JsonRpcPayload:
-		return s.executeHTTPRelay(relay.Meta, payload.JsonRpcPayload)
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
 	case *coreTypes.Relay_RestPayload:
-		return nil, fmt.Errorf(""Error executing relay on application %s: REST not supported"", relay.Meta.ApplicationAddress)
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
 	default:
-		return nil, fmt.Errorf(""Error exeucting relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
 	}
 }
 
@@ -235,15 +241,16 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -254,7 +261,7 @@

 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -263,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -270,13 +280,14 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer has remaining for the Application in the session provided.
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
 //
 //	If nothing is cached, the maximum number of session tokens is computed.
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.originalCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.originalCountAvailable), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
@@ -305,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -359,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
 	session, err := s.getSession(relay)
 	if err != nil {
 		return err
 	}
 
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -374,14 +379,14 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
 // of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
 func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
@@ -390,7 +395,7 @@

 		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
 	}
 
-	// DOCUMENT: find the right document to explain the following:
+	// TODO(M5): find the right document to explain the following:
 	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
 	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
 	//		matches the beginning of the session.
@@ -400,8 +405,7 @@

 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
 	if err != nil {
 		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
@@ -409,8 +413,8 @@

 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
 }
 
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func (s *servicer) executeHTTPRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JsonRpcPayload) (*coreTypes.RelayResponse, error) {
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
 	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
 		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
 	}
@@ -420,29 +424,48 @@

 		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
 	}
 
-	chainUrl, err := url.Parse(serviceConfig.Url)
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
 	if err != nil {
 		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
 	}
-	targetUrl := chainUrl.JoinPath(payload.HttpPath)
-
-	req, err := http.NewRequest(payload.Method, targetUrl.String(), bytes.NewBuffer([]byte(payload.Data)))
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
 	if err != nil {
 		return nil, err
 	}
-	if serviceConfig.BasicAuth != nil && serviceConfig.BasicAuth.UserName != """" {
-		req.SetBasicAuth(serviceConfig.BasicAuth.UserName, serviceConfig.BasicAuth.Password)
-	}
-
-	// DISCUSS: do we need a default user-agent for HTTP requests?
-	for k, v := range payload.Headers {
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
 		req.Header.Set(k, v)
 	}
-	if len(payload.Headers) == 0 {
+	if req.Header.Get(""Content-Type"") == """" {
 		req.Header.Set(""Content-Type"", ""application/json"")
 	}
 
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
 	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
 	if err != nil {
 		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1160294627,https://github.com/pokt-network/pocket/pull/622#discussion_r1160294627,,36,010c045d060cf8114d09066846c1f7d1fe5f7318,6b9b2db0f25b5fb605c1cb6d454b47708ff4eacd,utility/unit_of_work/gov.go,nan,Fucking amazing," 	return typesUtil.ErrUnknownParam(paramName)
 }
 
-func (u *baseUtilityUnitOfWork) getParameter(paramName string) (any, error) {","--- 

+++ 

@@ -2,7 +2,9 @@

 
 import (
 	""math/big""
-
+	""strings""
+
+	""github.com/pokt-network/pocket/logger""
 	""github.com/pokt-network/pocket/persistence""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/utils""
@@ -11,11 +13,19 @@

 )
 
 func init() {
-	GovParams = generateParamConfigs()
+	govParamTypes = prepareGovParamParamTypesMap()
+	for _, key := range utils.GovParamMetadataKeys {
+		if isOwner := strings.Contains(key, ""_owner""); isOwner {
+			continue
+		}
+		if _, ok := govParamTypes[key]; !ok {
+			logger.Global.Fatal().Msgf(""govParamTypes map does not contain: %s"", key)
+		}
+	}
 }
 
 var (
-	GovParams map[string]int
+	govParamTypes map[string]int
 )
 
 const (
@@ -26,7 +36,7 @@

 	STRING
 )
 
-func generateParamConfigs() map[string]int {
+func prepareGovParamParamTypesMap() map[string]int {
 	return map[string]int{
 		typesUtil.AppMinimumStakeParamName:                 BIGINT,
 		typesUtil.AppMaxChainsParamName:                    INT,
@@ -59,6 +69,8 @@

 		typesUtil.MessageSendFee:                           BIGINT,
 		typesUtil.MessageStakeFishermanFee:                 BIGINT,
 		typesUtil.MessageEditStakeFishermanFee:             BIGINT,
+		typesUtil.MessageUnstakeFishermanFee:               BIGINT,
+		typesUtil.MessagePauseFishermanFee:                 BIGINT,
 		typesUtil.MessageUnpauseFishermanFee:               BIGINT,
 		typesUtil.MessageFishermanPauseServicerFee:         BIGINT,
 		typesUtil.MessageTestScoreFee:                      BIGINT,
@@ -82,25 +94,25 @@

 	}
 }
 
-func getGovParam[T *big.Int | int | int64 | []byte | string](u *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {
+func getGovParam[T *big.Int | int | int64 | []byte | string](uow *baseUtilityUnitOfWork, paramName string) (i T, err typesUtil.Error) {
 	switch tp := any(i).(type) {
 	case *big.Int:
-		v, er := u.getBigIntParam(paramName)
+		v, er := uow.getBigIntParam(paramName)
 		return any(v).(T), er
 	case int:
-		v, er := u.getIntParam(paramName)
+		v, er := uow.getIntParam(paramName)
 		return any(v).(T), er
 	case int64:
-		v, er := u.getInt64Param(paramName)
+		v, er := uow.getInt64Param(paramName)
 		return any(v).(T), er
 	case []byte:
-		v, er := u.getByteArrayParam(paramName)
+		v, er := uow.getByteArrayParam(paramName)
 		return any(v).(T), er
 	case string:
-		v, er := u.getStringParam(paramName)
+		v, er := uow.getStringParam(paramName)
 		return any(v).(T), er
 	default:
-		u.logger.Fatal().Msgf(""unhandled parameter type: %T"", tp)
+		uow.logger.Fatal().Msgf(""unhandled parameter type: %T"", tp)
 	}
 	return
 }",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826216808,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1826216808,,1585,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,nan,"Full suggestion that gets rid of noqa 501. Note that bullet lists need to start after a linebreak. When breaking the bullet description into multiple lines, these need to align with the first line for the docs to render correctly.
```python
    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.

    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""

       - Remove all events within 1 second of each other
       - Remove all events without threshold value (NaN)
    2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""

       - Find matching beam break timestamps within 1.2s after each pellet delivery
    3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""

       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
       previous threshold update
    5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""

    Args:
        patch_key (dict): primary key for the patch
        start (datetime): start timestamp
        end (datetime): end timestamp

    Returns:
        pd.DataFrame: DataFrame with the following columns:

        - threshold_update_timestamp (index)
        - pellet_timestamp
        - beam_break_timestamp
        - offset
        - rate
    """"""
```
","     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
-    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
+        - These are the pellet delivery events ""B"" associated with the previous threshold update","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820625134,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1820625134,,179,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/create_experiments/create_socialexperiment_0.py,nan,Fixed bare except (E722) ,"         ld = [jl.levenshtein_distance(subjid, x[-len(subjid) :]) for x in valid_ids]
         return valid_ids[np.argmin(ld)]
-    except:
+    except ValueError:","--- 

+++ 

@@ -1,11 +1,9 @@

-""""""Function to create new experiments for social0-r1.""""""
+""""""Functions to create new experiments for social0-r1.""""""
 
 import pathlib
 
 from aeon.dj_pipeline import acquisition, lab, subject
-from aeon.dj_pipeline.create_experiments.create_experiment_01 import (
-    ingest_exp01_metadata,
-)
+from aeon.dj_pipeline.create_experiments.create_experiment_01 import ingest_exp01_metadata
 
 # ============ Manual and automatic steps to for experiment 0.1 populate ============
 experiment_name = ""social0-r1""
@@ -38,10 +36,7 @@

         skip_duplicates=True,
     )
     acquisition.Experiment.Subject.insert(
-        [
-            {""experiment_name"": experiment_name, ""subject"": s[""subject""]}
-            for s in subject_list
-        ],
+        [{""experiment_name"": experiment_name, ""subject"": s[""subject""]} for s in subject_list],
         skip_duplicates=True,
     )
 
@@ -97,12 +92,8 @@

     # manually update coordinates of foodpatch and nest
     patch_coordinates = {""Patch1"": (1.13, 1.59, 0), ""Patch2"": (1.19, 0.50, 0)}
 
-    for patch_key in (
-        acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}
-    ).fetch(""KEY""):
-        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(
-            ""food_patch_description""
-        )
+    for patch_key in (acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}).fetch(""KEY""):
+        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(""food_patch_description"")
         x, y, z = patch_coordinates[patch]
         acquisition.ExperimentFoodPatch.Position.update1(
             {",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1254315920,https://github.com/pokt-network/pocket/pull/732#discussion_r1254315920,,60,8c0b8c3b0702bbfe0d920e0f8ff8803ad36acf67,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/event_handler.go,nan,need to check `m.isStakedActor()` here...," 
 		if stateMachineTransitionEvent.NewState == string(coreTypes.StateMachineState_P2P_Bootstrapping) {
-			if m.router.GetPeerstore().Size() == 0 {
+			if m.stakedActorRouter.GetPeerstore().Size() == 0 {","--- 

+++ 

@@ -57,13 +57,25 @@

 		m.logger.Debug().Fields(messaging.TransitionEventToMap(stateMachineTransitionEvent)).Msg(""Received state machine transition event"")
 
 		if stateMachineTransitionEvent.NewState == string(coreTypes.StateMachineState_P2P_Bootstrapping) {
-			if m.stakedActorRouter.GetPeerstore().Size() == 0 {
-				m.logger.Warn().Msg(""No peers in addrbook, bootstrapping"")
+			staked, err := m.isStakedActor()
+			if err != nil {
+				return err
+			}
+			if staked {
+				// TECHDEBT(#859): this will never happen as the peerstore is
+				// seeded from consensus during P2P module construction.
+				if m.stakedActorRouter.GetPeerstore().Size() == 0 {
+					m.logger.Warn().Msg(""No peers in peerstore, bootstrapping"")
 
-				if err := m.bootstrap(); err != nil {
-					return err
+					if err := m.bootstrap(); err != nil {
+						return err
+					}
 				}
 			}
+
+			// TECHDEBT(#859): for unstaked actors, unstaked actor (background)
+			// router bootstrapping SHOULD complete before the event below is sent.
+
 			m.logger.Info().Bool(""TODO"", true).Msg(""Advertise self to network"")
 			if err := m.GetBus().GetStateMachineModule().SendEvent(coreTypes.StateMachineEvent_P2P_IsBootstrapped); err != nil {
 				return err",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220512526,https://github.com/pokt-network/pocket/pull/806#discussion_r1220512526,,12,df5eb7eecd9f14d3c38f5a4193d24f691f276cd2,12f9f498c8b05a91f4c08c6b27227d94013d9232,app/client/cli/flags/flags.go,nan,Can you add a very short comment for each of them?,"+
+	NonInteractive bool
+
+	Verbose bool","--- 

+++ 

@@ -1,13 +1,24 @@

 package flags
 
 var (
+	// RemoveCLIURL is the URL of the remote RPC node which the CLI will interact with.
+	// Formatted as <protocol>://<host>:<port> (uses RPC Port).
+	// (see: --help the root command for more info).
 	RemoteCLIURL string
 
+	// DataDir a path to store pocket related data (keybase etc.).
+	// (see: --help the root command for more info).
 	DataDir string
 
+	// ConfigPath is the path to the node config file.
+	// (see: --help the root command for more info).
 	ConfigPath string
 
+	// If true skips the interactive prompts wherever possible (useful for scripting & automation)
+	// (see: --help the root command for more info).
 	NonInteractive bool
 
+	// Show verbose output
+	// (see: --help the root command for more info).
 	Verbose bool
 )",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/hyperskill/mobile-app/pulls/comments/1479556773,https://github.com/hyperskill/mobile-app/pull/876#discussion_r1479556773,,100,185dd3d9db08d86483226f910a08c622c0e96ef3,28fdc9ac62da0bc97bfc77700b7a4eec61ae1737,shared/src/commonMain/kotlin/org/hyperskill/app/paywall/presentation/PaywallActionDispatcher.kt,nan,"I think yo can delete `/*ktlint-disable*/`, local execution `./gradlew ktlintCheck` passed successfully ","+            .let(onNewMessage)
+    }
+
+    /*ktlint-disable*/","--- 

+++ 

@@ -9,6 +9,9 @@

 import org.hyperskill.app.paywall.presentation.PaywallFeature.Message
 import org.hyperskill.app.purchases.domain.interactor.PurchaseInteractor
 import org.hyperskill.app.purchases.domain.model.PurchaseResult
+import org.hyperskill.app.sentry.domain.interactor.SentryInteractor
+import org.hyperskill.app.sentry.domain.model.transaction.HyperskillSentryTransactionBuilder
+import org.hyperskill.app.sentry.domain.withTransaction
 import org.hyperskill.app.subscriptions.domain.repository.SubscriptionsRepository
 import ru.nobird.app.presentation.redux.dispatcher.CoroutineActionDispatcher
 
@@ -17,6 +20,7 @@

     private val analyticInteractor: AnalyticInteractor,
     private val purchaseInteractor: PurchaseInteractor,
     private val subscriptionsRepository: SubscriptionsRepository,
+    private val sentryInteractor: SentryInteractor,
     private val logger: Logger
 ) : CoroutineActionDispatcher<Action, Message>(config.createConfig()) {
     override suspend fun doSuspendableAction(action: Action) {
@@ -40,43 +44,42 @@

     private suspend fun handleFetchMobileOnlyPrice(
         onNewMessage: (Message) -> Unit
     ) {
-        purchaseInteractor.getFormattedMobileOnlySubscriptionPrice()
-            .fold(
-                onSuccess = { price ->
-                    if (price != null) {
-                        InternalMessage.FetchMobileOnlyPriceSuccess(price)
-                    } else {
-                        logger.e { ""Receive null instead of formatted mobile-only subscription price"" }
-                        InternalMessage.FetchMobileOnlyPriceError
-                    }
-                },
-                onFailure = {
-                    logger.e(it) { ""Error during mobile-only subscription price fetching"" }
-                    InternalMessage.FetchMobileOnlyPriceError
-                }
-            )
-            .let(onNewMessage)
+        sentryInteractor.withTransaction(
+            transaction = HyperskillSentryTransactionBuilder.buildPaywallFetchSubscriptionPrice(),
+            onError = {
+                InternalMessage.FetchMobileOnlyPriceError
+            }
+        ) {
+            val price = purchaseInteractor
+                .getFormattedMobileOnlySubscriptionPrice()
+                .getOrThrow()
+            if (price != null) {
+                InternalMessage.FetchMobileOnlyPriceSuccess(price)
+            } else {
+                logger.e { ""Receive null instead of formatted mobile-only subscription price"" }
+                InternalMessage.FetchMobileOnlyPriceError
+            }
+        }.let(onNewMessage)
     }
 
     private suspend fun handleStartMobileOnlySubscriptionPurchase(
         action: InternalAction.StartMobileOnlySubscriptionPurchase,
         onNewMessage: (Message) -> Unit
     ) {
-        purchaseInteractor
-            .purchaseMobileOnlySubscription(action.purchaseParams)
-            .fold(
-                onSuccess = { purchaseResult ->
-                    if (purchaseResult is PurchaseResult.Error) {
-                        logger.e { getPurchaseErrorMessage(purchaseResult) }
-                    }
-                    InternalMessage.MobileOnlySubscriptionPurchaseSuccess(purchaseResult)
-                },
-                onFailure = {
-                    logger.e(it) { ""Subscription purchase failed!"" }
-                    InternalMessage.MobileOnlySubscriptionPurchaseError
-                }
-            )
-            .let(onNewMessage)
+        sentryInteractor.withTransaction(
+            transaction = HyperskillSentryTransactionBuilder.buildPaywallFeaturePurchaseSubscription(),
+            onError = {
+                InternalMessage.MobileOnlySubscriptionPurchaseError
+            }
+        ) {
+            val purchaseResult = purchaseInteractor
+                .purchaseMobileOnlySubscription(action.purchaseParams)
+                .getOrThrow()
+            if (purchaseResult is PurchaseResult.Error) {
+                logger.e { getPurchaseErrorMessage(purchaseResult) }
+            }
+            InternalMessage.MobileOnlySubscriptionPurchaseSuccess(purchaseResult)
+        }.let(onNewMessage)
     }
 
     private fun getPurchaseErrorMessage(error: PurchaseResult.Error): String =
@@ -85,19 +88,19 @@

     private suspend fun handleSyncSubscription(
         onNewMessage: (Message) -> Unit
     ) {
-        subscriptionsRepository
-            .syncSubscription()
-            .fold(
-                onSuccess = InternalMessage::SubscriptionSyncSuccess,
-                onFailure = {
-                    logger.e(it) { ""Failed to sync subscription"" }
-                    InternalMessage.SubscriptionSyncError
-                }
-            )
-            .let(onNewMessage)
+        sentryInteractor.withTransaction(
+            transaction = HyperskillSentryTransactionBuilder.buildPaywallFeatureSyncSubscription(),
+            onError = {
+                InternalMessage.SubscriptionSyncError
+            }
+        ) {
+            subscriptionsRepository
+                .syncSubscription()
+                .getOrThrow()
+                .let(InternalMessage::SubscriptionSyncSuccess)
+        }.let(onNewMessage)
     }
 
-    /*ktlint-disable*/
     private fun handleLogWrongSubscriptionTypeAfterSync(
         action: InternalAction.LogWrongSubscriptionTypeAfterSync
     ) {",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1824905241,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1824905241,8,9,fc49b511a4cd1c4445e65c0aafe5c61901499f83,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/streams.py,nan,@MilagrosMarin would you revert this change as well?,"+import datajoint as dj
+import pandas as pd","--- 

+++ 

@@ -2,34 +2,34 @@

 #---- THIS FILE IS AUTO-GENERATED BY `streams_maker.py` ----
 
 import re
-from uuid import UUID
-
-import aeon
 import datajoint as dj
 import pandas as pd
+from uuid import UUID
+
+import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.Schema(get_schema_name(""streams""))
 
 
-@schema
+@schema 
 class StreamType(dj.Lookup):
     """"""Catalog of all steam types for the different device types used across Project Aeon. One StreamType corresponds to one reader class in `aeon.io.reader`. The combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data loading routine for a particular device, using the `aeon.io.utils`.""""""
 
     definition = """"""  # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""
-
-
-@schema
+    """"""
+
+
+@schema 
 class DeviceType(dj.Lookup):
     """"""Catalog of all device types used across Project Aeon.""""""
 
@@ -46,7 +46,7 @@

         """"""
 
 
-@schema
+@schema 
 class Device(dj.Lookup):
     definition = """"""  # Physical devices, of a particular type, identified by unique serial number
     device_serial_number: varchar(12)
@@ -55,7 +55,7 @@

     """"""
 
 
-@schema
+@schema 
 class RfidReader(dj.Manual):
         definition = f""""""
         # rfid_reader placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -82,7 +82,7 @@

             """"""
 
 
-@schema
+@schema 
 class SpinnakerVideoSource(dj.Manual):
         definition = f""""""
         # spinnaker_video_source placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -109,7 +109,7 @@

             """"""
 
 
-@schema
+@schema 
 class UndergroundFeeder(dj.Manual):
         definition = f""""""
         # underground_feeder placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -136,7 +136,7 @@

             """"""
 
 
-@schema
+@schema 
 class WeightScale(dj.Manual):
         definition = f""""""
         # weight_scale placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -163,7 +163,7 @@

             """"""
 
 
-@schema
+@schema 
 class RfidReaderRfidEvents(dj.Imported):
         definition = """"""  # Raw per-chunk RfidEvents data stream from RfidReader (auto-generated with aeon_mecha-unknown)
     -> RfidReader
@@ -189,7 +189,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (RfidReader & key).fetch1('rfid_reader_name')
@@ -224,7 +223,7 @@

             )
 
 
-@schema
+@schema 
 class SpinnakerVideoSourceVideo(dj.Imported):
         definition = """"""  # Raw per-chunk Video data stream from SpinnakerVideoSource (auto-generated with aeon_mecha-unknown)
     -> SpinnakerVideoSource
@@ -232,7 +231,6 @@

     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of Video data
-    hw_counter: longblob
     hw_timestamp: longblob
     """"""
 
@@ -251,7 +249,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (SpinnakerVideoSource & key).fetch1('spinnaker_video_source_name')
@@ -286,7 +283,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederBeamBreak(dj.Imported):
         definition = """"""  # Raw per-chunk BeamBreak data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -312,7 +309,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -347,7 +343,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederDeliverPellet(dj.Imported):
         definition = """"""  # Raw per-chunk DeliverPellet data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -373,7 +369,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -408,7 +403,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederDepletionState(dj.Imported):
         definition = """"""  # Raw per-chunk DepletionState data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -436,7 +431,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -471,7 +465,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederEncoder(dj.Imported):
         definition = """"""  # Raw per-chunk Encoder data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -498,7 +492,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -533,7 +526,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederManualDelivery(dj.Imported):
         definition = """"""  # Raw per-chunk ManualDelivery data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -559,7 +552,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -594,7 +586,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederMissedPellet(dj.Imported):
         definition = """"""  # Raw per-chunk MissedPellet data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -620,7 +612,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -655,7 +646,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederRetriedDelivery(dj.Imported):
         definition = """"""  # Raw per-chunk RetriedDelivery data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -681,7 +672,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -716,7 +706,7 @@

             )
 
 
-@schema
+@schema 
 class WeightScaleWeightFiltered(dj.Imported):
         definition = """"""  # Raw per-chunk WeightFiltered data stream from WeightScale (auto-generated with aeon_mecha-unknown)
     -> WeightScale
@@ -743,7 +733,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (WeightScale & key).fetch1('weight_scale_name')
@@ -778,7 +767,7 @@

             )
 
 
-@schema
+@schema 
 class WeightScaleWeightRaw(dj.Imported):
         definition = """"""  # Raw per-chunk WeightRaw data stream from WeightScale (auto-generated with aeon_mecha-unknown)
     -> WeightScale
@@ -805,7 +794,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (WeightScale & key).fetch1('weight_scale_name')
@@ -838,3 +826,5 @@

                 },
                 ignore_extra_fields=True,
             )
+
+",,,,
mattzcarey/code-review-gpt,https://api.github.com/repos/pass-culture/data-gcp/pulls/comments/1867684164,https://github.com/pass-culture/data-gcp/pull/3554#discussion_r1867684164,,141,c86a607c5bf52e36fe61345bbaeaa2e94e9823e6,13c3b6b8038a39870d70a22e8821d52b9b944f8b,orchestration/dags/data_gcp_dbt/models/intermediate/pcapi/int_pcapi__log.sql,nan,"`offer_data_api_call_id` â‡¾ `jsonpayload.extra.offer_data_api_call_id`
Pourrait-on expliciter ce que contient ce champ ainsi que les suivants : 

Here are some alternative column name suggestionsÂ :
offer_suggestion_api_id
subcategory_predict_api_id
predictive_model_call_id
suggested_offer_api_id
api_call_subcat_suggestion_id
predictive_offer_call_id
subcat_offer_api_id
suggestion_model_api_id
api_offer_prediction_id
offer_creation_api_id
","+    jsonpayload.extra.status as user_status,
+    cast(jsonpayload.extra.user_satisfaction as string) as user_satisfaction,
+    cast(jsonpayload.extra.user_comment as string) as user_comment,
+    cast(offer_data_api_call_id as string) as offer_data_api_call_id,","--- 

+++ 

@@ -138,9 +138,11 @@

     jsonpayload.extra.status as user_status,
     cast(jsonpayload.extra.user_satisfaction as string) as user_satisfaction,
     cast(jsonpayload.extra.user_comment as string) as user_comment,
-    cast(offer_data_api_call_id as string) as offer_data_api_call_id,
-    cast(jsonpayload.extra.offer_subcategory as string) as offer_subcategory_chosen,
+    cast(jsonpayload.extra.offer_data_api_call_id as string) as suggested_offer_api_id,
+    cast(
+        jsonpayload.extra.offer_subcategory as string
+    ) as suggested_offer_api_subcategory,
     array_to_string(
         jsonpayload.extra.offer_subcategories, "",""
-    ) as offer_subcategories_suggested
+    ) as suggested_offer_api_subcategories
 from {{ source(""raw"", ""stdout"") }}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184450214,https://github.com/pokt-network/pocket/pull/710#discussion_r1184450214,,1,b6be2a930185debeb449f91b51d6c70a44a35401,dbc0deb6a7aec39359745e9be952a4992689052a,build/config/fisherman.json,build/config/config.fisherman1.json,"NIT regarding file nameing.

1. Can you rename `fisherman.json` to `fisherman1.json` in case we add more; ditto for the servicer
2. Can you rename the `config*.json` to `validator*.json` so everything is semantically identified","@@ -0,0 +1,57 @@
+{","--- 

+++ 

@@ -1,7 +1,6 @@

 {
   ""root_directory"": ""/go/src/github.com/pocket-network"",
   ""private_key"": ""90ccfd6ba76d876e02ba09440af67582e0f4a37cbda2ce4c30b251132b670eda2777a49cdfde21867a538ddcfca05002f0115b1955a75b80e965ed63fc95f809"",
-  ""use_lib_p2p"": false,
   ""consensus"": {
     ""max_mempool_bytes"": 500000000,
     ""pacemaker_config"": {
@@ -18,7 +17,7 @@

   },
   ""persistence"": {
     ""postgres_url"": ""postgres://postgres:postgres@pocket-db:5432/postgres"",
-    ""node_schema"": ""fisherman"",
+    ""node_schema"": ""fisherman1"",
     ""block_store_path"": ""/var/blockstore"",
     ""tx_indexer_path"": ""/var/txindexer"",
     ""trees_store_dir"": ""/var/trees"",
@@ -29,7 +28,7 @@

     ""health_check_period"": ""30s""
   },
   ""p2p"": {
-    ""hostname"": ""node1.fisherman"",
+    ""hostname"": ""fisherman1"",
     ""port"": 42069,
     ""use_rain_tree"": true,
     ""is_empty_connection_type"": false,",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220406444,https://github.com/pokt-network/pocket/pull/756#discussion_r1220406444,,46,3c65f9c895487df8f1b6109168200bba6ac7875f,70a1a0e2fe2c238fce8c2019e13e9167629a2639,shared/modules/persistence_module.go,nan,Optional NIT: I feel like single lining this whole statement (even if it's > 80 chars) will make it easier to read,"+type TreeStore interface {
+	// Update returns the new state hash for a given height.
+	// * Height is passed through to the Update function
+	// and used by the queries against the TxIndexer and the","--- 

+++ 

@@ -22,10 +22,11 @@

 	NewReadContext(height int64) (PersistenceReadContext, error)
 	ReleaseWriteContext() error // The module can maintain many read contexts, but only one write context can exist at a time
 
-	// BlockStore operations
+	// BlockStore maps a block height to an *coreTypes.IndexedTransaction
 	GetBlockStore() blockstore.BlockStore
 
-	// TreeStore operations
+	// TreeStore manages atomic access to a set of merkle trees
+	// that compose the state hash.
 	GetTreeStore() TreeStore
 
 	NewWriteContext() PersistenceRWContext
@@ -38,20 +39,15 @@

 	HandleDebugMessage(*messaging.DebugMessage) error
 }
 
-// TreeStore is fulfilled by the treeStore to create an
-// atomic tree component for use by the peristence context.
+// TreeStore defines the interface for atomic updates and rollbacks to the internal
+// merkle trees that compose the state hash of pocket.
 type TreeStore interface {
 	// Update returns the new state hash for a given height.
-	// * Height is passed through to the Update function
-	// and used by the queries against the TxIndexer and the
-	// It updates to the future but not to the past.
-	// * Passing a higher height will cause a change
-	// but repeatedly calling the same or a lower height will
-	// not incur a change.
+	// * Update inherits the pgx transaction's read view of the database and builds the trees according to that view.
+	// TODO(#808): Change interface to `Update(pgtx pgx.Tx, height uint64) (string, error)`
 	Update(pgtx pgx.Tx, txi indexer.TxIndexer, height uint64) (string, error)
-	// ClearAll completely clears the state of the trees.
-	// For debugging purposes only.
-	ClearAll() error
+	// DebugClearAll completely clears the state of the trees. For debugging purposes only.
+	DebugClearAll() error
 }
 
 // Interface defining the context within which the node can operate with the persistence layer.",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1182005752,https://github.com/pokt-network/pocket/pull/684#discussion_r1182005752,,110,964333fcec0ccb79708af5ce07dbab1cf5efb3a5,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/v1/openapi.yaml,nan,"I suggest changing
 ```
+          description: Session servicer response
```
 to
```
+          description: Session response
```","+        required: true
+      responses:
+        ""200"":
+          description: Session servicer response","--- 

+++ 

@@ -99,7 +99,7 @@

         - client
       summary: Sends a session request to the network and get the nodes that will be servicing your requests for the session
       requestBody:
-        description: Request the nodes that will be servicing the requests in your session
+        description: Retrieve the list of actors involved in servicing and verifying a session
         content:
           application/json:
             schema:
@@ -107,7 +107,7 @@

         required: true
       responses:
         ""200"":
-          description: Session servicer response
+          description: Session response
           content:
             application/json:
               schema:
@@ -122,11 +122,12 @@

           content:
             text/plain:
               example: ""description of failure""
+  # TODO: Update this handler and its schemas when the HandleRelay function has been implemented
   /v1/client/relay:
     post:
       tags:
         - client
-      summary: Sends a challenge request to the network to service the RPC request
+      summary: Sends a relay to the servicer to receive a response
       requestBody:
         description: Request a relay to be sent on behalf of your application
         content:
@@ -151,11 +152,12 @@

           content:
             text/plain:
               example: ""description of failure""
+  # TODO: Update this handler and its schemas when the HandleChallenge function has been implemented
   /v1/client/challenge:
     post:
       tags:
         - client
-      summary: Sends a relay request to the network to for invalid data returned from an RPC request
+      summary: Sends a challenge request to the network
       requestBody:
         description: Request a challenge for invalid data returned from an RPC request
         content:
@@ -229,7 +231,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -294,7 +296,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressPaginated""
+              $ref: ""#/components/schemas/QueryAccountPaginated""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               page: 1
@@ -322,7 +324,7 @@

     get:
       tags:
         - query
-      summary: Returns the current values of all governance parameters
+      summary: Returns the current values of all on-chain governance parameters
       responses:
         ""200"":
           description: Returns all the chain parameters
@@ -345,7 +347,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -367,7 +369,9 @@

           content:
             text/plain:
               example: ""description of failure""
-  # TODO: (h5law) Think of an equivalent to staking status
+  # TODO: (h5law) Determine a parameter to give the request that differentiates
+  # the staking status of an app and returns only that type of staked application
+  # This will be equivalent to the `staking_status` field from the V0 RPC spec
   /v1/query/apps:
     post:
       tags:
@@ -411,7 +415,7 @@

         content:
           application/json:
             schema:
-              $ref: '#/components/schemas/QueryAddressHeight'
+              $ref: '#/components/schemas/QueryAccountHeight'
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 99
@@ -508,7 +512,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -621,7 +625,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -758,7 +762,7 @@

           content:
             application/json:
               schema:
-                $ref: ""#/components/schemas/Transaction""
+                $ref: ""#/components/schemas/IndexedTransaction""
         ""400"":
           description: Bad request
           content:
@@ -789,7 +793,7 @@

           content:
             application/json:
               schema:
-                $ref: ""#/components/schemas/Transaction""
+                $ref: ""#/components/schemas/IndexedTransaction""
         ""400"":
           description: Bad request
           content:
@@ -874,7 +878,7 @@

         content:
           application/json:
             schema:
-              $ref: ""#/components/schemas/QueryAddressHeight""
+              $ref: ""#/components/schemas/QueryAccountHeight""
             example:
               address: da034209758b78eaea06dd99c07909ab54c99b45
               height: 0
@@ -957,7 +961,7 @@

           type: string
         session_id:
           type: string
-    QueryAddressHeight:
+    QueryAccountHeight:
       type: object
       required:
         - height
@@ -968,7 +972,7 @@

           format: int64
         address:
           type: string
-    QueryAddressPaginated:
+    QueryAccountPaginated:
       type: object
       required:
         - address
@@ -1197,7 +1201,7 @@

         txs:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
         total_txs:
           type: integer
           format: int64
@@ -1317,7 +1321,7 @@

         transactions:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
         total_txs:
           type: integer
           format: int64
@@ -1459,7 +1463,7 @@

         transactions:
           type: array
           items:
-            $ref: ""#/components/schemas/Transaction""
+            $ref: ""#/components/schemas/IndexedTransaction""
     BlockHeader:
       type: object
       required:
@@ -1753,7 +1757,7 @@

           type: string
         signature:
           type: string
-    StdTx:
+    TxMessage:
       type: object
       required:
         - fee
@@ -1781,8 +1785,7 @@

         - hash
         - height
         - index
-        - txResult
-        - stdTx
+        - txMsg
       properties:
         hash:
           type: string
@@ -1792,14 +1795,11 @@

         index:
           type: integer
           format: int32
-        txResult:
-          $ref: ""#/components/schemas/TxResult""
-        stdTx:
-          $ref: ""#/components/schemas/StdTx""
-    TxResult:
-      type: object
-      required:
-        - tx
+        txMsg:
+          $ref: ""#/components/schemas/TxMessage""
+    IndexedTransaction:
+      type: object
+      required:
         - height
         - index
         - result_code
@@ -1807,9 +1807,8 @@

         - signer_addr
         - recipient_addr
         - message_type
-      properties:
-        tx:
-          type: string
+        - tx
+      properties:
         height:
           type: integer
           format: int64
@@ -1825,6 +1824,8 @@

           type: string
         message_type:
           type: string
+        tx:
+          $ref: ""#/components/schemas/Transaction""
     ThresholdSignature:
       type: object
       required:",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1201133126,https://github.com/pokt-network/pocket/pull/763#discussion_r1201133126,,356,a87e5cdb98f814c2886ab63dafc592ff594c9ba2,fba11c3f7cf66379028a9396e6eb4b66f0c43199,p2p/raintree/router.go,nan,"`s/appMsgData/pocketEnvelopeBz` ?

I realize you were staying consistent with what we had before but doesn't hurt to be more explicit seeing how we're already here.","+	}
+
+	// extract `PocketEnvelope` from `RainTreeMessage` (& continue propagation)
+	appMsgData, err := rtr.handleRainTreeMsg(data)","--- 

+++ 

@@ -325,49 +325,51 @@

 
 // readStream reads the incoming stream, extracts the serialized `PocketEnvelope`
 // data from the incoming `RainTreeMessage`, and passes it to the application by
-// calling the configured `rtr.handler`.
+// calling the configured `rtr.handler`. Intended to be called in a go routine.
 func (rtr *rainTreeRouter) readStream(stream libp2pNetwork.Stream) {
 	// Time out if no data is sent to free resources.
+	// NB: tests using libp2p's `mocknet` rely on this not returning an error.
 	if err := stream.SetReadDeadline(newReadStreamDeadline()); err != nil {
-		// NB: tests using libp2p's `mocknet` rely on this not returning an error.
 		// `SetReadDeadline` not supported by `mocknet` streams.
-		rtr.logger.Debug().Err(err).Msg(""setting stream read deadline"")
+		rtr.logger.Error().Err(err).Msg(""setting stream read deadline"")
 	}
 
 	// log incoming stream
 	rtr.logStream(stream)
 
 	// read stream
-	data, err := io.ReadAll(stream)
+	rainTreeMsgBz, err := io.ReadAll(stream)
 	if err != nil {
 		rtr.logger.Error().Err(err).Msg(""reading from stream"")
 		if err := stream.Reset(); err != nil {
-			rtr.logger.Debug().Err(err).Msg(""resetting stream (read-side)"")
+			rtr.logger.Error().Err(err).Msg(""resetting stream (read-side)"")
 		}
 		return
 	}
 
 	// done reading; reset to signal this to remote peer
+	// NB: failing to reset the stream can easily max out the number of available
+	// network connections on the receiver's side.
 	if err := stream.Reset(); err != nil {
-		rtr.logger.Debug().Err(err).Msg(""resetting stream (read-side)"")
+		rtr.logger.Error().Err(err).Msg(""resetting stream (read-side)"")
 	}
 
 	// extract `PocketEnvelope` from `RainTreeMessage` (& continue propagation)
-	appMsgData, err := rtr.handleRainTreeMsg(data)
-	if err != nil {
-		rtr.logger.Error().Err(err).Msg(""handling network data"")
+	poktEnvelopeBz, err := rtr.handleRainTreeMsg(rainTreeMsgBz)
+	if err != nil {
+		rtr.logger.Error().Err(err).Msg(""handling raintree message"")
 		return
 	}
 
 	// There was no error, but we don't need to forward this to the app-specific bus.
 	// For example, the message has already been handled by the application.
-	if appMsgData == nil {
+	if poktEnvelopeBz == nil {
 		return
 	}
 
 	// call configured handler to forward to app-specific bus
-	if err := rtr.handler(appMsgData); err != nil {
-		rtr.logger.Error().Err(err).Msg(""handling network data"")
+	if err := rtr.handler(poktEnvelopeBz); err != nil {
+		rtr.logger.Error().Err(err).Msg(""handling pocket envelope"")
 	}
 }
 ",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184416497,https://github.com/pokt-network/pocket/pull/684#discussion_r1184416497,53,60,42ae63c2ef0485cb92c5335efb978821b052d8ee,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/utils.go,nan,"Just a NIT that makes code easier to read. It's a general trick if you ever find yourself doing `if {very large block } `
I suggest changing
 ```
+	if height == 0 {
+		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
+		if currentHeight > 0 {
+			currentHeight -= 1
+		}
+		return currentHeight
+	}
+	return height
```
 to
```
+	if height != 0 {
+		return height
+	}
+	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
+	if currentHeight > 0 {
+		currentHeight -= 1
+	}
+	return currentHeight
```","+	if height == 0 {
+		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
+		if currentHeight > 0 {
+			currentHeight -= 1
+		}
+		return currentHeight
+	}
+	return height","--- 

+++ 

@@ -46,18 +46,18 @@

 }
 
 // getQueryHeight returns either the height supplied or if it is equal to 0
-// the most recent block height that has been commited. As the current consensus height
+// the most recent block height that has been committed. As the current consensus height
 // is one above this, and if used in certain queries will return an error as the height
-// has not been commited yet
+// has not been committed yet
 func (s *rpcServer) getQueryHeight(height int64) int64 {
-	if height == 0 {
-		currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-		if currentHeight > 0 {
-			currentHeight -= 1
-		}
-		return currentHeight
-	}
-	return height
+	if height != 0 {
+		return height
+	}
+	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
+	if currentHeight > 0 {
+		currentHeight -= 1
+	}
+	return currentHeight
 }
 
 // checkSortDesc takes a sort string and returns whether to sort descending or not",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1195874598,https://github.com/pokt-network/pocket/pull/737#discussion_r1195874598,,45,79d691fb741afb89ae2c361dfa95e49c4a15c297,470f4fea2cbd51f6f46dcf83750ad8d175f956fe,build/localnet/cluster-manager/sts_kill.go,nan,Let's log a warning here with details,"+		case watch.Added, watch.Modified:
+			pod, ok := event.Object.(*corev1.Pod)
+			if !ok {
+				continue",File_Deleted,,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1581104089,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/351#discussion_r1581104089,,61,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/conftest.py,nan,Can we drop the `return` statements in the fixtures that do not return anything?,"+    dj.config[""custom""][
+        ""database.prefix""
+    ] = f""u_{dj.config['database.user']}_testsuite_""
+    return","--- 

+++ 

@@ -58,7 +58,6 @@

     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_""
-    return
 
 
 def load_pipeline():
@@ -137,8 +136,6 @@

         }
     )
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def epoch_chunk_ingestion(test_params, pipeline, experiment_creation):
@@ -154,8 +151,6 @@

 
     acquisition.Chunk.ingest_chunks(experiment_name=test_params[""experiment_name""])
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def experimentlog_ingestion(pipeline):
@@ -166,20 +161,14 @@

     acquisition.SubjectEnterExit.populate(**_populate_settings)
     acquisition.SubjectWeight.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_qc_ingestion(pipeline, epoch_chunk_ingestion):
     qc = pipeline[""qc""]
     qc.CameraQC.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_tracking_ingestion(pipeline, camera_qc_ingestion):
     tracking = pipeline[""tracking""]
     tracking.CameraTracking.populate(**_populate_settings)
-
-    return",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1173565356,https://github.com/pokt-network/pocket/pull/683#discussion_r1173565356,,22,c723b33e617e0622e4431df83ec52a2c4ae4d958,fb245bc027ae55aaee936daf9f85ecb2d9ad8b40,runtime/test_artifacts/generator.go,nan,What do you think about multi-lining this signature for readability?,"+type GenesisOption func(*genesis.GenesisState)
+
+// IMPROVE: Extend the utilities here into a proper genesis suite in the future.
+func NewGenesisState(numValidators, numServicers, numApplications, numFisherman int, genesisOpts ...GenesisOption) (genesisState *genesis.GenesisState, validatorPrivateKeys []string) {","--- 

+++ 

@@ -6,6 +6,8 @@

 	""fmt""
 	""strconv""
 
+	""google.golang.org/protobuf/types/known/timestamppb""
+
 	""github.com/pokt-network/pocket/logger""
 	""github.com/pokt-network/pocket/runtime/configs""
 	""github.com/pokt-network/pocket/runtime/genesis""
@@ -13,13 +15,21 @@

 	""github.com/pokt-network/pocket/shared/core/types""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
-	""google.golang.org/protobuf/types/known/timestamppb""
 )
 
 type GenesisOption func(*genesis.GenesisState)
 
 // IMPROVE: Extend the utilities here into a proper genesis suite in the future.
-func NewGenesisState(numValidators, numServicers, numApplications, numFisherman int, genesisOpts ...GenesisOption) (genesisState *genesis.GenesisState, validatorPrivateKeys []string) {
+func NewGenesisState(
+	numValidators,
+	numServicers,
+	numApplications,
+	numFisherman int,
+	genesisOpts ...GenesisOption,
+) (
+	genesisState *genesis.GenesisState,
+	validatorPrivateKeys []string,
+) {
 	applications, appPrivateKeys := NewActors(coreTypes.ActorType_ACTOR_TYPE_APP, numApplications, DefaultChains)
 	validators, validatorPrivateKeys := NewActors(coreTypes.ActorType_ACTOR_TYPE_VAL, numValidators, nil)
 	servicers, servicerPrivateKeys := NewActors(coreTypes.ActorType_ACTOR_TYPE_SERVICER, numServicers, DefaultChains)
@@ -72,13 +82,20 @@

 
 func NewDefaultConfigs(privateKeys []string) (cfgs []*configs.Config) {
 	for i, pk := range privateKeys {
-		postgresSchema := ""node"" + strconv.Itoa(i+1)
 		cfgs = append(cfgs, configs.NewDefaultConfig(
 			configs.WithPK(pk),
-			configs.WithNodeSchema(postgresSchema),
+			configs.WithNodeSchema(getPostgresSchema(i+1)),
 		))
 	}
 	return cfgs
+}
+
+// TECHDEBT: This is used for the `node_schema` field in `PersistenceConfig` and enables
+// different nodes sharing the same database while being isolated from each other.
+// The naming convention should be changed to be more reflective of the node (e.g. <actor_type>_<address>),
+// which would require all related tooling and documentation to be updated as well.
+func getPostgresSchema(i int) string {
+	return ""node"" + strconv.Itoa(i)
 }
 
 func NewPools() (pools []*coreTypes.Account) {
@@ -116,7 +133,7 @@

 	return accounts
 }
 
-//nolint:unused
+//nolint:unused // useful if we want to generate accounts with random keys
 func newAccounts(numActors int) (accounts []*coreTypes.Account) {
 	for i := 0; i < numActors; i++ {
 		_, _, addr := keygen.GetInstance().Next()
@@ -133,7 +150,9 @@

 func NewActors(actorType coreTypes.ActorType, numActors int, chains []string) (actors []*coreTypes.Actor, privateKeys []string) {
 	// If the actor type is a validator, the chains must be nil since they are chain agnostic
 	if actorType == coreTypes.ActorType_ACTOR_TYPE_VAL {
-		logger.Global.Warn().Msgf(""validator actors should not have chains but a list was provided: %v"", chains)
+		logger.Global.Warn().
+			Array(""chains"", logger.StringLogArrayMarshaler{Strings: chains}).
+			Msg(""validator actors should not have chains but a list was provided."")
 		chains = nil
 	}
 	for i := 0; i < numActors; i++ {
@@ -145,7 +164,14 @@

 	return actors, privateKeys
 }
 
-func NewDefaultActor(actorType coreTypes.ActorType, serviceURL string, chains []string) (actor *coreTypes.Actor, privateKey string) {
+func NewDefaultActor(
+	actorType coreTypes.ActorType,
+	serviceURL string,
+	chains []string,
+) (
+	actor *coreTypes.Actor,
+	privateKey string,
+) {
 	privKey, pubKey, addr := keygen.GetInstance().Next()
 	return &coreTypes.Actor{
 		ActorType:       actorType,",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228800495,https://github.com/pokt-network/pocket/pull/803#discussion_r1228800495,,276,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/servicer/module.go,nan,"I suggest changing
 ```
+func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
```
 to
```
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
```","+// calculateServicerAppSessionTokens return the number of tokens the servicer has remaining for the Application in the session provided.
+//
+//	If nothing is cached, the maximum number of session tokens is computed.
+func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {","--- 

+++ 

@@ -13,6 +13,7 @@

 	""time""
 
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
@@ -24,11 +25,12 @@

 	""golang.org/x/exp/slices""
 )
 
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
 	errValidateServicer    = errors.New(""relay failed servicer validation"")
-	errValidateApplication = errors.New(""relay failed application validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
 
 	_ modules.ServicerModule = &servicer{}
 )
@@ -37,11 +39,12 @@

 	ServicerModuleName = ""servicer""
 )
 
-// sessionTokens is used to cache the original number of tokens available
+// sessionTokens is used to cache the starting number of tokens available
 // during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	sessionNumber          int64
-	originalCountAvailable *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -52,9 +55,10 @@

 	config *configs.ServicerConfig
 
 	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
 	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
-	// INVESTIGATE: considering the computational complexity, should we skip caching this value?
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
@@ -71,7 +75,9 @@

 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	s := &servicer{}
+	s := &servicer{
+		totalTokens: make(map[string]*sessionTokens),
+	}
 
 	for _, option := range options {
 		option(s)
@@ -120,7 +126,12 @@

 
 	// TODO(M6): Look into data integrity checks and response validation.
 
-	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(relay, response)
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -128,55 +139,50 @@

 		return response, nil
 	}
 
-	session, err := s.getSession(relay)
-	if err != nil {
-		return nil, err
-	}
-
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.StoreServiceRelay(session, relay.Meta.ApplicationAddress, relayDigest, relayReqResBytes); err != nil {
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
-	if err := localCtx.Release(); err != nil {
-		s.logger.Warn().Err(err).Msg(""failed to release local context"")
-	}
-
 	return response, nil
 }
 
 // isRelayVolumeApplicable returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) isRelayVolumeApplicable(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
 	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
 	if err != nil {
 		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
 	}
 
 	relayDigest := crypto.SHA3Hash(relayReqResBytes)
-
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.isRelayVolumeApplicableOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
 	}
 
 	return signedDigest, relayReqResBytes, collision, nil
 }
 
-// INCOMPLETE: implement this
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) isRelayVolumeApplicableOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
@@ -184,11 +190,11 @@

 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
 	switch payload := relay.RelayPayload.(type) {
 	case *coreTypes.Relay_JsonRpcPayload:
-		return s.executeHTTPRelay(relay.Meta, payload.JsonRpcPayload)
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
 	case *coreTypes.Relay_RestPayload:
-		return nil, fmt.Errorf(""Error executing relay on application %s: REST not supported"", relay.Meta.ApplicationAddress)
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
 	default:
-		return nil, fmt.Errorf(""Error exeucting relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
 	}
 }
 
@@ -235,15 +241,16 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	localCtx, err := s.GetBus().GetPersistenceModule().NewLocalContext()
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
 	if err != nil {
 		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -254,7 +261,7 @@

 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -263,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -270,13 +280,14 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer has remaining for the Application in the session provided.
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
 //
 //	If nothing is cached, the maximum number of session tokens is computed.
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.originalCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.originalCountAvailable), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
@@ -305,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -359,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
 	session, err := s.getSession(relay)
 	if err != nil {
 		return err
 	}
 
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -374,14 +379,14 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
 // of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
 func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
@@ -390,7 +395,7 @@

 		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
 	}
 
-	// DOCUMENT: find the right document to explain the following:
+	// TODO(M5): find the right document to explain the following:
 	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
 	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
 	//		matches the beginning of the session.
@@ -400,8 +405,7 @@

 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
 	if err != nil {
 		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
@@ -409,8 +413,8 @@

 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
 }
 
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func (s *servicer) executeHTTPRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JsonRpcPayload) (*coreTypes.RelayResponse, error) {
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
 	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
 		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
 	}
@@ -420,29 +424,48 @@

 		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
 	}
 
-	chainUrl, err := url.Parse(serviceConfig.Url)
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
 	if err != nil {
 		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
 	}
-	targetUrl := chainUrl.JoinPath(payload.HttpPath)
-
-	req, err := http.NewRequest(payload.Method, targetUrl.String(), bytes.NewBuffer([]byte(payload.Data)))
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
 	if err != nil {
 		return nil, err
 	}
-	if serviceConfig.BasicAuth != nil && serviceConfig.BasicAuth.UserName != """" {
-		req.SetBasicAuth(serviceConfig.BasicAuth.UserName, serviceConfig.BasicAuth.Password)
-	}
-
-	// DISCUSS: do we need a default user-agent for HTTP requests?
-	for k, v := range payload.Headers {
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
 		req.Header.Set(k, v)
 	}
-	if len(payload.Headers) == 0 {
+	if req.Header.Get(""Content-Type"") == """" {
 		req.Header.Set(""Content-Type"", ""application/json"")
 	}
 
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
 	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
 	if err != nil {
 		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220445042,https://github.com/pokt-network/pocket/pull/803#discussion_r1220445042,,28,f37b394454f45c27d058dac20af974a6ed273903,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/service/service.go,utility/servicer/module.go,"`UtilityConfig`

It's client configurable based on how much risk (opportunity?) they're willing to take to mine.

Let's also rename it to `RelayMiningVolumeAccuracy` and point to https://arxiv.org/abs/2305.10672","+	typesUtil ""github.com/pokt-network/pocket/utility/types""
 )
 
+// DISCUSS: where should the RelayAccracyParameter be defined?","--- 

+++ 

@@ -1,9 +1,8 @@

-package service
+package servicer
 
 import (
 	""bytes""
 	""encoding/hex""
-	""encoding/json""
 	""errors""
 	""fmt""
 	""io""
@@ -13,33 +12,39 @@

 	""sync""
 	""time""
 
-	""golang.org/x/exp/slices""
-
 	""github.com/pokt-network/pocket/logger""
+	""github.com/pokt-network/pocket/persistence""
 	""github.com/pokt-network/pocket/runtime/configs""
+	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	""github.com/pokt-network/pocket/shared/modules/base_modules""
 	""github.com/pokt-network/pocket/shared/utils""
 	typesUtil ""github.com/pokt-network/pocket/utility/types""
+	""golang.org/x/exp/slices""
 )
 
-// DISCUSS: where should the RelayAccracyParameter be defined?
-const RelayAccuracyParameter = 0.2
-
+// TECHDEBT(#519): Refactor error handling and consolidate with `shared/core/types/error.go`
 var (
 	errValidateBlockHeight = errors.New(""relay failed block height validation"")
 	errValidateRelayMeta   = errors.New(""relay failed metadata validation"")
-	errValidateServicer    = errors.New(""relay does not match the servicer"")
-	errValidateApplication = errors.New(""relay failed application validation"")
-
-	_ modules.Servicer = &servicer{}
+	errValidateServicer    = errors.New(""relay failed servicer validation"")
+	errShouldMineRelay     = errors.New(""relay failed validating available tokens"")
+
+	_ modules.ServicerModule = &servicer{}
 )
 
+const (
+	ServicerModuleName = ""servicer""
+)
+
+// sessionTokens is used to cache the starting number of tokens available
+// during a specific session: it is used as the value for a map with keys being applications' public keys
+// TODO: What if we have a servicer managing more than one session from the same app at once? We may/may not need to resolve this in the future.
 type sessionTokens struct {
-	SessionNumber int64
-	Count         *big.Int
+	sessionNumber               int64
+	startingTokenCountAvailable *big.Int
 }
 
 type servicer struct {
@@ -49,19 +54,29 @@

 	logger *modules.Logger
 	config *configs.ServicerConfig
 
+	// This lock is needed to allow multiple GO routines update the totalTokens cache as part of serving relays
+	// NB: per the description in pkg.go.dev/sync#Map, we have chosen explicitly not to use sync.Map
 	rwlock sync.RWMutex
-	// totalTokens holds the total number of tokens assigned to this servicer for the app in the current session
-	// DISCUSS: considering the computational complexity, should we skip caching this value?
+	// totalTokens is a mapping from application public keys to session metadata to keep track of session tokens
+	// OPTIMIZE: There is an opportunity to simplify the code through various means such as, but not limited to, avoiding extra math.big operations or excess GetParam calls
 	totalTokens map[string]*sessionTokens
 }
 
-func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
-	return new(servicer).Create(bus, options...)
+var (
+	_ modules.ServicerModule = &servicer{}
+)
+
+func CreateServicer(bus modules.Bus, options ...modules.ModuleOption) (modules.ServicerModule, error) {
+	m, err := new(servicer).Create(bus, options...)
+	if err != nil {
+		return nil, err
+	}
+	return m.(modules.ServicerModule), nil
 }
 
 func (*servicer) Create(bus modules.Bus, options ...modules.ModuleOption) (modules.Module, error) {
 	s := &servicer{
-		logger: logger.Global.CreateLoggerForModule(servicerModuleName),
+		totalTokens: make(map[string]*sessionTokens),
 	}
 
 	for _, option := range options {
@@ -70,19 +85,27 @@

 
 	bus.RegisterModule(s)
 
+	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
+
 	cfg := bus.GetRuntimeMgr().GetConfig()
-	s.config = cfg.Utility.ServicerConfig
+	s.config = cfg.Servicer
 
 	return s, nil
 }
 
+// TODO: implement this function
 func (s *servicer) Start() error {
-	s.logger = logger.Global.CreateLoggerForModule(s.GetModuleName())
-	return nil
-}
-
-func (*servicer) GetModuleName() string {
-	return servicerModuleName
+	s.logger.Info().Msg(""ðŸ§¬ Servicer module started ðŸ§¬"")
+	return nil
+}
+
+func (s *servicer) Stop() error {
+	s.logger.Info().Msg(""ðŸ§¬ Servicer module stopped ðŸ§¬"")
+	return nil
+}
+
+func (s *servicer) GetModuleName() string {
+	return ServicerModuleName
 }
 
 // HandleRelay processes a relay after performing validation.
@@ -101,8 +124,14 @@

 		return nil, fmt.Errorf(""Error executing relay: %w"", err)
 	}
 
-	// DISCUSS: should we validate the response from the node?
-	relayDigest, shouldStore, err := s.hasCollision(relay, response)
+	// TODO(M6): Look into data integrity checks and response validation.
+
+	session, err := s.getSession(relay)
+	if err != nil {
+		return nil, err
+	}
+
+	relayDigest, relayReqResBytes, shouldStore, err := s.isRelayVolumeApplicable(session, relay, response)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating relay service digest: %w"", err)
 	}
@@ -110,83 +139,63 @@

 		return response, nil
 	}
 
-	height := s.GetBus().GetConsensusModule().CurrentHeight()
-	writeCtx, err := s.GetBus().GetPersistenceModule().NewRWContext(int64(height))
-	if err != nil {
-		return nil, fmt.Errorf(""Error getting a write context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	defer writeCtx.Release()
-
-	// DISCUSS: should we extend/use UnitOfWork for updating/retrieving token usage?
-	if err := writeCtx.RecordRelayService(relay.Meta.ApplicationAddress, relayDigest, relay, response); err != nil {
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return nil, fmt.Errorf(""Error getting a local context to update token usage for application %s: %w"", relay.Meta.ApplicationAddress, err)
+	}
+
+	if err := localCtx.StoreServicedRelay(session, relayDigest, relayReqResBytes); err != nil {
 		return nil, fmt.Errorf(""Error recording service proof for application %s: %w"", relay.Meta.ApplicationAddress, err)
 	}
 
 	return response, nil
 }
 
-// hasCollision returns:
-//  1. The signed digest of a relay/response pair,
-//  2. Whether there was a collision for the specific chain (i.e. should the service proof be stored for claiming later)
-func (s *servicer) hasCollision(relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest []byte, collides bool, err error) {
-	relayBytes, err := marshal(relay, response)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
-	}
-
-	relayDigest := crypto.SHA3Hash(relayBytes)
-
+// isRelayVolumeApplicable returns:
+//  1. The signed digest of a relay/response pair
+//  2. Whether a legit relay eligible for claiming rewards
+//     Legit means satisfying at-least the following conditions: not-replay and having a proper signature,
+func (s *servicer) isRelayVolumeApplicable(session *coreTypes.Session, relay *coreTypes.Relay, response *coreTypes.RelayResponse) (digest, serializedRelayRes []byte, collides bool, err error) {
+	relayReqResBytes, err := codec.GetCodec().Marshal(&coreTypes.RelayReqRes{Relay: relay, Response: response})
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error marshalling relay and/or response: %w"", err)
+	}
+
+	relayDigest := crypto.SHA3Hash(relayReqResBytes)
 	signedDigest := s.sign(relayDigest)
 	response.ServicerSignature = hex.EncodeToString(signedDigest)
-	collision, err := s.hasCollisionOnChain(relayDigest, relay.Meta.RelayChain.Id)
-	if err != nil {
-		return nil, false, fmt.Errorf(""Error checking collision for chain %s: %w"", relay.Meta.RelayChain.Id, err)
-	}
-
-	return signedDigest, collision, nil
-}
-
-// INCOMPLETE: implement this
+	collision, err := s.isRelayVolumeApplicableOnChain(session, relayDigest)
+	if err != nil {
+		return nil, nil, false, fmt.Errorf(""Error checking for relay replay by app %s for chain %s during session number %d: %w"",
+			session.Application.Address, relay.Meta.RelayChain.Id, session.SessionNumber, err)
+	}
+
+	return signedDigest, relayReqResBytes, collision, nil
+}
+
+// INCOMPLETE(#832): provide a private key to the servicer and use it to sign all relays
 func (s *servicer) sign(bz []byte) []byte {
 	return bz
 }
 
-// INCOMPLETE: implement this
-func (s *servicer) hasCollisionOnChain(digest []byte, relayChainId string) (bool, error) {
+// INCOMPLETE: implement this according to the comment below
+// isRelayVolumeApplicableOnChain returns whether the serialized serviced relay and the response, provided as `digest`, is eligible for reward
+//
+//	on the service/chain corresponding to the provided session.
+func (s *servicer) isRelayVolumeApplicableOnChain(session *coreTypes.Session, digest []byte) (bool, error) {
 	return false, nil
 }
 
-func marshal(request *coreTypes.Relay, response *coreTypes.RelayResponse) ([]byte, error) {
-	if request == nil || response == nil {
-		return nil, fmt.Errorf(""error marshalling: got nil value as input"")
-	}
-
-	s := struct {
-		*coreTypes.Relay
-		*coreTypes.RelayResponse
-	}{
-		request,
-		response,
-	}
-	return json.Marshal(s)
-}
-
-// executeRelay performs the passed relay using an HTTP request to the chain-specific target URL
+// executeRelay performs the passed relay using the correct method depending on the relay payload type.
 func (s *servicer) executeRelay(relay *coreTypes.Relay) (*coreTypes.RelayResponse, error) {
-	if relay.Meta == nil || relay.Meta.RelayChain == nil || relay.Meta.RelayChain.Id == """" {
-		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", relay.Meta.ApplicationAddress)
-	}
-
-	chainConfig, ok := s.config.Chains[relay.Meta.RelayChain.Id]
-	if !ok {
-		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", relay.Meta.RelayChain.Id, errValidateRelayMeta)
-	}
-
-	res, err := executeHTTPRequest(chainConfig, relay.Payload)
-	if err != nil {
-		return nil, fmt.Errorf(""Error executing HTTP request for relay on application %s: %w"", relay.Meta.ApplicationAddress, err)
-	}
-	return res, nil
+	switch payload := relay.RelayPayload.(type) {
+	case *coreTypes.Relay_JsonRpcPayload:
+		return s.executeJsonRPCRelay(relay.Meta, payload.JsonRpcPayload)
+	case *coreTypes.Relay_RestPayload:
+		return s.executeRESTRelay(relay.Meta, payload.RestPayload)
+	default:
+		return nil, fmt.Errorf(""Error executing relay on application %s: Unsupported type on payload %s"", relay.Meta.ApplicationAddress, payload)
+	}
 }
 
 // validateRelayMeta ensures the relay metadata is valid for being handled by the servicer
@@ -208,8 +217,8 @@

 }
 
 func (s *servicer) validateRelayChainSupport(relayChain *coreTypes.Identifiable, currentHeight int64) error {
-	if _, ok := s.config.Chains[relayChain.Id]; !ok {
-		return fmt.Errorf(""chain %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
+	if _, ok := s.config.Services[relayChain.Id]; !ok {
+		return fmt.Errorf(""service %s not supported by servicer %s configuration"", relayChain.Id, s.config.Address)
 	}
 
 	// DISCUSS: either update NewReadContext to take a uint64, or the GetCurrentHeight to return an int64.
@@ -232,26 +241,27 @@

 	return nil
 }
 
-// validateApplication makes sure the application has not received more relays than allocated in the current session.
-func (s *servicer) validateApplication(session *coreTypes.Session, currentHeight int64) error {
-	// IMPROVE: use a function to get current height from the current session
-	servicerAppSessionTokens, err := s.calculateServicerAppSessionTokens(session, currentHeight)
+// ADDTEST: Need to add more unit tests to account for potential edge cases
+// shouldMineRelay makes sure the application has not received more relays than allocated in the current session.
+// returns nil if the servicer should attempt to mine another relay for the session provided
+func (s *servicer) shouldMineRelay(session *coreTypes.Session) error {
+	servicerAppSessionTokens, err := s.startingTokenCountAvailable(session)
 	if err != nil {
 		return fmt.Errorf(""Error calculating servicer tokens for application: %w"", err)
 	}
 
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return fmt.Errorf(""Error getting read context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
-	}
-
-	usedAppSessionTokens, err := readCtx.GetServicerTokenUsage(session)
+	localCtx, err := s.GetBus().GetPersistenceModule().GetLocalContext()
+	if err != nil {
+		return fmt.Errorf(""Error getting local persistence context: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
+	}
+
+	usedAppSessionTokens, err := localCtx.GetSessionTokensUsed(session)
 	if err != nil {
 		return fmt.Errorf(""Error getting servicer token usage: application %s session number %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
 
 	if usedAppSessionTokens == nil || usedAppSessionTokens.Cmp(servicerAppSessionTokens) < 0 {
-		return nil
+		return nil // should attempt to mine a relay
 	}
 
 	return fmt.Errorf(""application %s has exceeded its allocated relays %s for session %d"",
@@ -260,6 +270,9 @@

 		session.SessionNumber)
 }
 
+// cachedAppTokens returns the cached number of starting tokens for a session.
+//
+//	This caching is done to remove the need for getting the starting number of tokens for a session every time a relay is being served.
 func (s *servicer) cachedAppTokens(session *coreTypes.Session) *sessionTokens {
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
@@ -267,18 +280,21 @@

 	return s.totalTokens[session.Application.PublicKey]
 }
 
-// calculateServicerAppSessionTokens return the number of tokens the servicer can use for the application in the current session
-func (s *servicer) calculateServicerAppSessionTokens(session *coreTypes.Session, currentHeight int64) (*big.Int, error) {
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
+// startingTokenCountAvailable returns the total number of tokens the Application corresponding to the provided session has per servicer at the start of the session.
+//
+//	If nothing is cached, the maximum number of session tokens is computed.
+func (s *servicer) startingTokenCountAvailable(session *coreTypes.Session) (*big.Int, error) {
 	tokens := s.cachedAppTokens(session)
-	if tokens != nil && tokens.Count != nil && tokens.SessionNumber == session.SessionNumber {
-		return big.NewInt(1).Set(tokens.Count), nil
+	if tokens != nil && tokens.startingTokenCountAvailable != nil && tokens.sessionNumber == session.SessionNumber {
+		return big.NewInt(1).Set(tokens.startingTokenCountAvailable), nil
 	}
 
 	// Calculate this servicer's limit for the application in the current session.
 	//	This is distributed rate limiting (DRL): no need to know how many requests have
 	//		been performed for this application by other servicers. Instead, simply enforce
 	//		this servicer's share of the application's tokens for this session.
-	appSessionTokens, err := s.calculateAppSessionTokens(session.Application.StakedAmount, currentHeight)
+	appSessionTokens, err := s.calculateAppSessionTokens(session)
 	if err != nil {
 		return nil, fmt.Errorf(""Error calculating application %s total tokens for session %d: %w"", session.Application.PublicKey, session.SessionNumber, err)
 	}
@@ -290,7 +306,7 @@

 	// This multiplication is performed to minimize the chance of under-utilization of application's tokens,
 	//	while removing the overhead of communication between servicers which would be necessary otherwise.
 	// see https://arxiv.org/abs/2305.10672 for details on application and servicer distributed rate-limiting
-	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+RelayAccuracyParameter))
+	adjustedTokens := servicerTokens.Mul(servicerTokens, big.NewFloat(1+s.config.RelayMiningVolumeAccuracy))
 	roundedTokens, _ := adjustedTokens.Int(big.NewInt(1))
 
 	s.setAppSessionTokens(session, &sessionTokens{session.SessionNumber, roundedTokens})
@@ -300,10 +316,6 @@

 func (s *servicer) setAppSessionTokens(session *coreTypes.Session, tokens *sessionTokens) {
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
-
-	if len(s.totalTokens) == 0 {
-		s.totalTokens = make(map[string]*sessionTokens)
-	}
 
 	s.totalTokens[session.Application.PublicKey] = tokens
 }
@@ -329,6 +341,17 @@

 	return nil
 }
 
+// getSession returns a session for the current height and the passed relay
+func (s *servicer) getSession(relay *coreTypes.Relay) (*coreTypes.Session, error) {
+	height := s.GetBus().GetConsensusModule().CurrentHeight()
+	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
+	if err != nil {
+		return nil, fmt.Errorf(""failed to get a session for height %d for relay meta %s: %w"", height, relay.Meta, err)
+	}
+
+	return session, nil
+}
+
 // admitRelay decides whether the relay should be served
 func (s *servicer) admitRelay(relay *coreTypes.Relay) error {
 	// TODO: utility module should initialize the servicer (if this module instance is a servicer)
@@ -343,13 +366,11 @@

 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateRelayMeta)
 	}
 
-	// TODO: update the CLI to include ApplicationAddress(or Application Public Key) in the RelayMeta
-	session, err := s.GetBus().GetUtilityModule().GetSession(relay.Meta.ApplicationAddress, int64(height), relay.Meta.RelayChain.Id, relay.Meta.GeoZone.Id)
-	if err != nil {
-		return fmt.Errorf(""%s: failed to get a session for height %d for relay meta %s: %w"", errPrefix, height, relay.Meta, err)
-	}
-
-	// TODO: (REFACTOR) use a loop to run all validators: would also remove the need for passing the session around
+	session, err := s.getSession(relay)
+	if err != nil {
+		return err
+	}
+
 	if err := validateRelayBlockHeight(relay.Meta, session); err != nil {
 		return fmt.Errorf(""%s: %w"", err.Error(), errValidateBlockHeight)
 	}
@@ -358,35 +379,105 @@

 		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateServicer)
 	}
 
-	if err := s.validateApplication(session, int64(height)); err != nil {
-		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errValidateApplication)
-	}
-
-	return nil
-}
-
-// DISCUSS: do we need to export this functionality as part of the utility module?
+	if err := s.shouldMineRelay(session); err != nil {
+		return fmt.Errorf(""%s: %s: %w"", errPrefix, err.Error(), errShouldMineRelay)
+	}
+
+	return nil
+}
+
+// ADDTEST: Need to add more unit tests for the numerical portion of this functionality
 // calculateAppSessionTokens determines the number of ""session tokens"" an application gets at the beginning
-// of every session. Each servicer will serve a maximum of (Session Tokens / Number of Servicers in the Session) relays for the application
-func (s *servicer) calculateAppSessionTokens(appStakeStr string, currentHeight int64) (*big.Int, error) {
-	appStake, err := utils.StringToBigInt(appStakeStr)
-	if err != nil {
-		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", appStakeStr, coreTypes.ErrStringToBigInt(err))
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", currentHeight, err)
+// of every session. Each servicer will serve a maximum of ~(Session Tokens / Number of Servicers in the Session) relays for the application
+func (s *servicer) calculateAppSessionTokens(session *coreTypes.Session) (*big.Int, error) {
+	appStake, err := utils.StringToBigInt(session.Application.StakedAmount)
+	if err != nil {
+		return nil, fmt.Errorf(""Error processing application's staked amount %s: %w"", session.Application.StakedAmount, coreTypes.ErrStringToBigInt(err))
+	}
+
+	// TODO(M5): find the right document to explain the following:
+	//	We assume that the value of certain parameters only changes/takes effect at the start of a session.
+	//	In this specific case, the `AppSessionTokensMultiplierParamName` parameter is retrieved for the height that
+	//		matches the beginning of the session.
+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error getting persistence context at height %d: %w"", session.SessionHeight, err)
 	}
 	defer readCtx.Release() //nolint:errcheck // We only need to make sure the readCtx is released
 
-	// DISCUSS: using an interface for returning each defined parameter seems less error-prone: also could return e.g. int64 in this case to remove the type cast
-	appStakeTokensMultiplier, err := readCtx.GetIntParam(typesUtil.AppSessionTokensMultiplierParamName, currentHeight)
-	if err != nil {
-		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, currentHeight, err)
+	appStakeTokensMultiplier, err := persistence.GetParameter[int](readCtx, typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight)
+	if err != nil {
+		return nil, fmt.Errorf(""error reading parameter %s at height %d from persistence: %w"", typesUtil.AppSessionTokensMultiplierParamName, session.SessionHeight, err)
 	}
 
 	return appStake.Mul(appStake, big.NewInt(int64(appStakeTokensMultiplier))), nil
+}
+
+// executeJsonRPCRelay performs the relay for JSON-RPC payloads, sending them to the chain's/service's URL.
+func (s *servicer) executeJsonRPCRelay(meta *coreTypes.RelayMeta, payload *coreTypes.JSONRPCPayload) (*coreTypes.RelayResponse, error) {
+	if meta == nil || meta.RelayChain == nil || meta.RelayChain.Id == """" {
+		return nil, fmt.Errorf(""Relay for application %s does not specify relay chain"", meta.ApplicationAddress)
+	}
+
+	serviceConfig, ok := s.config.Services[meta.RelayChain.Id]
+	if !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+
+	relayBytes, err := codec.GetCodec().Marshal(payload)
+	if err != nil {
+		return nil, fmt.Errorf(""Error marshalling payload %s: %w"", payload.String(), err)
+	}
+
+	return s.executeHTTPRelay(serviceConfig, relayBytes, payload.Headers)
+}
+
+// executeRESTRelay performs the relay for REST payloads, sending them to the chain's/service's URL.
+// INCOMPLETE(#860): RESTful service relays: basic checks and execution through HTTP calls.
+func (s *servicer) executeRESTRelay(meta *coreTypes.RelayMeta, _ *coreTypes.RESTPayload) (*coreTypes.RelayResponse, error) {
+	if _, ok := s.config.Services[meta.RelayChain.Id]; !ok {
+		return nil, fmt.Errorf(""Chain %s not found in servicer configuration: %w"", meta.RelayChain.Id, errValidateRelayMeta)
+	}
+	return nil, nil
+}
+
+// executeHTTPRequest performs the HTTP request that sends the relay to the chain's/service's URL.
+func (s *servicer) executeHTTPRelay(serviceConfig *configs.ServiceConfig, payload []byte, headers map[string]string) (*coreTypes.RelayResponse, error) {
+	serviceUrl, err := url.Parse(serviceConfig.Url)
+	if err != nil {
+		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", serviceConfig.Url, err)
+	}
+
+	req, err := http.NewRequest(http.MethodPost, serviceUrl.String(), bytes.NewBuffer(payload))
+	if err != nil {
+		return nil, err
+	}
+
+	if auth := serviceConfig.BasicAuth; auth != nil && auth.UserName != """" {
+		req.SetBasicAuth(auth.UserName, auth.Password)
+	}
+
+	// INVESTIGATE: do we need a default user-agent for HTTP requests?
+	for k, v := range headers {
+		req.Header.Set(k, v)
+	}
+	if req.Header.Get(""Content-Type"") == """" {
+		req.Header.Set(""Content-Type"", ""application/json"")
+	}
+
+	// INCOMPLETE(#837): Optimize usage of HTTP client, e.g. connection reuse, depending on the volume of relays a servicer is expected to handle
+	resp, err := (&http.Client{Timeout: time.Duration(serviceConfig.TimeoutMsec) * time.Millisecond}).Do(req)
+	if err != nil {
+		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
+	}
+	defer resp.Body.Close()
+
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return nil, fmt.Errorf(""Error reading response body: %w"", err)
+	}
+
+	return &coreTypes.RelayResponse{Payload: string(body)}, nil
 }
 
 // IMPROVE: Add session height tolerance to account for session rollovers
@@ -404,188 +495,3 @@

 		sessionStartingBlock,
 		sessionLastBlock)
 }
-
-// TECHDEBT: These structures were copied as placeholders from v0 and need to be updated to reflect changes in v1
-// TODO: remove: use coreTypes.Relay instead
-type Relay interface {
-	RelayPayload
-	RelayMeta
-}
-
-type RelayPayload interface {
-	GetData() string               // the actual data string for the external chain
-	GetMethod() string             // the http CRUD method
-	GetHTTPPath() string           // the HTTP Path
-	GetHeaders() map[string]string // http headers
-}
-
-type RelayMeta interface {
-	GetBlockHeight() int64 // the block height when the request is made
-	GetServicerPublicKey() crypto.PublicKey
-	GetRelayChain() RelayChain
-	GetGeoZone() GeoZone
-	GetToken() AAT
-	GetSignature() string
-}
-
-type RelayResponse interface {
-	Payload() string
-	ServicerSignature() string
-}
-
-type RelayChain Identifiable
-type GeoZone Identifiable
-
-type AAT interface {
-	GetVersion() string              // confirm a valid AAT version
-	GetApplicationPublicKey() string // confirm the identity/signature of the app
-	GetClientPublicKey() string      // confirm the identity/signature of the client
-	GetApplicationSignature() string // confirm the application signed the token
-}
-
-type Identifiable interface {
-	Name() string
-	ID() string
-}
-
-var _ Relay = &relay{}
-
-type relay struct{}
-
-// Validate a submitted relay by a client before servicing
-func (r *relay) Validate() coreTypes.Error {
-
-	// validate payload
-
-	// validate the metadata
-
-	// ensure the RelayChain is supported locally
-
-	// ensure session block height is current
-
-	// get the session context
-
-	// get the application object from the r.AAT()
-
-	// get session node count from that session height
-
-	// get maximum possible relays for the application
-
-	// ensure not over serviced
-
-	// generate the session from seed data
-
-	// validate self against the session
-
-	return nil
-}
-
-// Store a submitted relay by a client for volume tracking
-func (r *relay) Store() coreTypes.Error {
-
-	// marshal relay object into protoBytes
-
-	// calculate the hashOf(protoBytes) <needed for volume tracking>
-
-	// persist relay object, indexing under session
-
-	return nil
-}
-
-// Execute a submitted relay by a client after validation
-func (r *relay) Execute() (RelayResponse, coreTypes.Error) {
-
-	// retrieve the RelayChain url from the servicer's local configuration file
-
-	// execute http request with the relay payload
-
-	// format and digitally sign the response
-
-	return nil, nil
-}
-
-// Get volume metric applicable relays from store
-func (r *relay) ReapStoreForHashCollision(sessionBlockHeight int64, hashEndWith string) ([]Relay, coreTypes.Error) {
-
-	// Pull all relays whose hash collides with the revealed secret key
-	// It's important to note, the secret key isn't revealed by the network until the session is over
-	// to prevent volume based bias. The secret key is usually a pseudorandom selection using the block hash as a seed.
-	// (See the session protocol)
-	//
-	// Demonstrable pseudocode below:
-	//   `SELECT * from RELAY where HashOf(relay) ends with hashEndWith AND sessionBlockHeight=sessionBlockHeight`
-
-	// This function also signifies deleting the non-volume-applicable Relays
-
-	return nil, nil
-}
-
-// Report volume metric applicable relays to Fisherman
-func (r *relay) ReportVolumeMetrics(fishermanServiceURL string, volumeRelays []Relay) coreTypes.Error {
-
-	// Send all volume applicable relays to the assigned trusted Fisherman for
-	// a proper verification of the volume completed. Send volumeRelays to fishermanServiceURL
-	// through http.
-
-	// NOTE: an alternative design is a 2 step, claim - proof lifecycle where the individual servicers
-	// build a merkle sum index tree from all the relays, submits a root and subsequent merkle proof to the
-	// network.
-	//
-	// Pros: Can report volume metrics directly to the chain in a trustless fashion
-	// Cons: Large chain bloat, non-trivial compute requirement for creation of claim/proof transactions and trees,
-	//       non-trivial compute requirement to process claim / proofs during ApplyBlock()
-
-	return nil
-}
-
-func (r *relay) GetData() string                        { return """" }
-func (r *relay) GetMethod() string                      { return """" }
-func (r *relay) GetHTTPPath() string                    { return """" }
-func (r *relay) GetHeaders() map[string]string          { return nil }
-func (r *relay) GetBlockHeight() int64                  { return 0 }
-func (r *relay) GetServicerPublicKey() crypto.PublicKey { return nil }
-func (r *relay) GetRelayChain() RelayChain              { return nil }
-func (r *relay) GetGeoZone() GeoZone                    { return nil }
-func (r *relay) GetToken() AAT                          { return nil }
-func (r *relay) GetSignature() string                   { return """" }
-
-// executeHTTPRequest performs the HTTP request that sends the relay to the chain's URL.
-func executeHTTPRequest(cfg *configs.ChainConfig, relay *coreTypes.RelayPayload) (*coreTypes.RelayResponse, error) {
-	chainUrl, err := url.Parse(cfg.Url)
-	if err != nil {
-		return nil, fmt.Errorf(""Error parsing chain URL %s: %w"", cfg.Url, err)
-	}
-	targetUrl := chainUrl.JoinPath(relay.HttpPath)
-
-	req, err := http.NewRequest(relay.Method, targetUrl.String(), bytes.NewBuffer([]byte(relay.Data)))
-	if err != nil {
-		return nil, err
-	}
-	if cfg.BasicAuth != nil && cfg.BasicAuth.UserName != """" {
-		req.SetBasicAuth(cfg.BasicAuth.UserName, cfg.BasicAuth.Password)
-	}
-	if cfg.UserAgent != """" {
-		req.Header.Set(""User-Agent"", cfg.UserAgent)
-	}
-
-	for k, v := range relay.Headers {
-		req.Header.Set(k, v)
-	}
-	if len(relay.Headers) == 0 {
-		req.Header.Set(""Content-Type"", ""application/json"")
-	}
-
-	// DISCUSS: we need to optimize usage of HTTP client, e.g. for connection reuse, considering the expected volume of relays
-	resp, err := (&http.Client{Timeout: time.Duration(cfg.TimeoutMilliseconds) * time.Millisecond}).Do(req)
-	if err != nil {
-		return nil, fmt.Errorf(""Error performing the HTTP request for relay: %w"", err)
-	}
-	defer resp.Body.Close()
-
-	body, err := io.ReadAll(resp.Body)
-	if err != nil {
-		return nil, fmt.Errorf(""Error reading response body: %w"", err)
-	}
-
-	return &coreTypes.RelayResponse{Payload: string(body)}, nil
-}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1251317366,https://github.com/pokt-network/pocket/pull/732#discussion_r1251317366,,492,4602283cbd676952b2a5c20f73758305cc833ce5,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/README.md,nan,"I suggest changing
 ```
+_DISCUSS(team): If you feel this needs a diagram, please reach out to the team for additional details._
```
 to
```
+_NOTE: If you (the reader) feel this needs a diagram, please reach out to the team for additional details._
```","-The `Network Module` is where [RainTree](https://github.com/pokt-network/pocket/files/9853354/raintree.pdf) (or the simpler basic approach) is implemented. See `raintree/network.go` for the specific implementation of RainTree, but please refer to the [specifications](https://github.com/pokt-network/pocket-network-protocol/tree/main/p2p) for more details.
+### Raintree Router Architecture
+
+_DISCUSS(team): If you feel this needs a diagram, please reach out to the team for additional details._","--- 

+++ 

@@ -7,6 +7,8 @@

 - [Definitions](#definitions)
 - [Interface & Integration](#interface--integration)
 - [Module Architecture](#module-architecture)
+  - [Architecture Design Language](#architecture-design-language)
+  - [Legends](#legends)
   - [P2P Module / Router Decoupling](#p2p-module--router-decoupling)
   - [Message Propagation & Handling](#message-propagation--handling)
   - [Message Deduplication](#message-deduplication)
@@ -71,7 +73,24 @@

 
 ## Module Architecture
 
+_(TODO: move ""arch. design lang."" & ""legends"" sections into `shared` to support common usage)_
+
+### Architecture Design Language
+
+The architecture design language expressed in this documentation is based on [UML](https://www.uml-diagrams.org/).
+Due to limitations in the current version of mermaid, class diagrams are much more adherant to the UML component specification.
+Component diagrams however are much more loosely inspired by their UML counterparts.
+
+Regardless, each architecture diagram should be accompanied by a legend which covers all the design language features used to provide disambiguation. 
+
+References:
+- [Class Diagrams](https://www.uml-diagrams.org/class-diagrams-overview.html)
+- [Component Diagrams](https://www.uml-diagrams.org/component-diagrams.html)
+
+  _NOTE: mermaid does not support ports, interfaces, ... in component diagrams (""flowcharts)._
+
 ### Legends
+
 ```mermaid
 flowchart
 subgraph Legend
@@ -89,13 +108,13 @@

 class ConcreteType {
   +ExportedField
   -unexportedField
-  +ExportedMethod(argType) returnType
-  -unexportedMethod()
+  +ExportedMethod(...argTypes) (...returnTypes)
+  -unexportedMethod(...argTypes) (...returnTypes)
 }
 
 class InterfaceType {
     <<interface>>
-    +Method(argType) (returnType1, returnType2)
+    +Method(...argTypes) (...returnTypes)
 }
 
 ConcreteType --|> InterfaceType : Interface realization
@@ -106,28 +125,82 @@

 ConcreteType ..*  ""(cardinality)"" OtherType : Indirect (via interface)
 ```
 
+#### Interface Realization
+
+_TL;DR An instance (i.e. client) implements the associated interface (i.e. supplierl)._
+
+> Realization is a specialized abstraction relationship between two sets of model elements, one representing a specification (the supplier) and the other represents an implementation of the latter (the client).
+
+> Realization can be used to model stepwise refinement, optimizations, transformations, templates, model synthesis, framework composition, etc.
+
+_(see: [UML Realization](https://www.uml-diagrams.org/realization.html))_
+
+#### Direct Usage
+
+_TL;DR one instance (i.e. client) is dependent the associated instance(s) (i.e. supplier) to function properly._
+
+> Dependency is a directed relationship which is used to show that some UML element or a set of elements requires, needs or depends on other model elements for specification or implementation. Because of this, dependency is called a supplier - client relationship, where supplier provides something to the client, and thus the client is in some sense incomplete while semantically or structurally dependent on the supplier element(s). Modification of the supplier may impact the client elements.
+
+> Usage is a dependency in which one named element (client) requires another named element (supplier) for its full definition or implementation.
+
+_(see: [UML Dependency](https://www.uml-diagrams.org/dependency.html))_
+
+#### Composition
+
+_TL;DR deleting an instance also deletes the associated instance(s)._
+
+> A ""strong"" form of aggregation
+
+> If a composite (whole) is deleted, all of its composite parts are ""normally"" deleted with it.
+
+_(see: [UML Shared composition](https://www.uml-diagrams.org/composition.html))_
+
+#### Aggregation
+
+
+_TL;DR deleting an instance does not necessarily delete the associated instance(s)._
+
+> A ""weak"" form of aggregation
+
+> Shared part could be included in several composites, and if some or all of the composites are deleted, shared part may still exist.
+
+_(see: [UML Shared aggregation](https://www.uml-diagrams.org/aggregation.html))_
+
+#### Cardinality
+
+_TL;DR indicates a number, or range of instances associated (i.e. supplier(s))_
+
+Cardinality indicates the number or range of simultaneous instances of supplier that are associated with the client.
+Applicable to multiple association types.
+Can be expressed arbitrarily (e.g. wildcards, variable, equation, etc.)
+
+_(see: [UML Association](https://www.uml-diagrams.org/association.html#association-end))_
+
+
 ### P2P Module / Router Decoupling
 
-The P2P module encapsulates the `RaiTreeRouter` and `BackgroundRouter` submodules.
+The P2P module encapsulates the `RainTreeRouter` and `BackgroundRouter` submodules.
 The P2P module internally refers to these as the `stakedActorRouter` and `unstakedActorRouter`, respectively.
 
 Depending on the necessary routing scheme (unicast / broadcast) and whether the peers involved are staked actors, a node will use one or both of these routers.
 
 **Unicast**
 
-| Sender         | Receiver       | Router          | Example Usage                                        |
-|----------------|----------------|-----------------|------------------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree only   | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Staked Actor   | Background only | Consensus (state sync) messages (to validators only) |
-| Unstaked Actor | Unstaked Actor | Background only | Consensus (state sync) & Debug (CLI) messages        |
+| Sender         | Receiver       | Router          | Example Usage                                                        |
+|----------------|----------------|-----------------|----------------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree only   | Consensus hotstuff messages (validators only) & state sync responses |
+| Staked Actor   | Untaked Actor  | Background only | Consensus state sync responses                                       |
+| Unstaked Actor | Staked Actor   | Background only | Consensus state sync responses, debug messages                       |
+| Unstaked Actor | Unstaked Actor | Background only | Consensus state sync responses, debug messages                       |
 
 **Broadcast**
 
-| Broadcaster    | Receiver       | Router                | Example Usage                              |
-|----------------|----------------|-----------------------|--------------------------------------------|
-| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages                        |
-| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (gossipsub redundancy) |
-| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages                        |
+| Broadcaster    | Receiver       | Router                | Example Usage                                                   |
+|----------------|----------------|-----------------------|-----------------------------------------------------------------|
+| Staked Actor   | Staked Actor   | Raintree + Background | Utility tx messages, consensus state sync requests              |
+| Staked Actor   | Untaked Actor  | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Staked Actor   | Background only       | Utility tx messages (redundancy), consensus state sync requests |
+| Unstaked Actor | Unstaked Actor | Background only       | Utility tx messages, consensus state sync requests              |
 
 Both router submodule implementations embed a `UnicastRouter` which enables them to send and receive messages directly to/from a single peer.
 
@@ -337,6 +410,8 @@

 
 The responsibility of deduplication is encapsulated by the P2P module, As such duplicate messages may come from multiple routers in some of these scenarios.
 
+The `NondeDeduper` state is not persisted outside of memory and therefore is cleared during node restarts.
+
 ```mermaid
 classDiagram
     class RainTreeMessage {
@@ -420,12 +495,17 @@

     p2pModule --* NonceDeduper
 ```
 
+#### Configuration
+
+The size of the `NonceDeduper` queue is configurable via the `P2PConfig.MaxNonces` field.
+
 ### Peer Discovery
+
 Peer discovery involves pairing peer IDs to their network addresses (multiaddr).
 This pairing always has an associated TTL (time-to-live), near the end of which it must
 be refreshed.
 
-In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves 7/8th through their TTL.
+In the background gossip overlay network (`backgroundRouter`), peers will re-advertise themselves every 3 hours through their TTL (see: [`RoutingDiscovery#Advertise()`](https://github.com/libp2p/go-libp2p/blob/87c2561238cb0340ddb182c61be8dbbc7a12a780/p2p/discovery/routing/routing.go#L34) and [`ProviderManager#AddProvider()`](https://github.com/libp2p/go-libp2p-kad-dht/blob/v0.24.2/providers/providers_manager.go#L255)).
 This refreshes the libp2p peerstore automatically.
 
 In the raintree gossip overlay network (`raintreeRouter`), the libp2p peerstore is **NOT** currently refreshed _(TODO: [#859](https://github.com/pokt-network/network/isues/859))_.
@@ -489,8 +569,7 @@

 
 ### Raintree Router Architecture
 
-_DISCUSS(team): If you feel this needs a diagram, please reach out to the team for additional details._
-_TODO(olshansky, BenVan): Link to RainTree visualizations once it is complete._
+_NOTE: If you (the reader) feel this needs a diagram, please reach out to the team for additional details._
 
 ### Code Organization
 ",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1184471728,https://github.com/pokt-network/pocket/pull/707#discussion_r1184471728,,350,4159d7a40f8d99835d17a00a052b6782cb3f795d,d7cc85d553cf9b0257f16c3a213ed4de96b8363a,p2p/background/router_test.go,nan,Can you reflect the same name of the wg as what you passed in. It'll help with the flow of the code to the reader.,"+func readSubscription(
+	t *testing.T,
+	ctx context.Context,
+	wg *sync.WaitGroup,","--- 

+++ 

@@ -17,6 +17,7 @@

 	""github.com/stretchr/testify/require""
 
 	""github.com/pokt-network/pocket/internal/testutil""
+	""github.com/pokt-network/pocket/p2p/config""
 	typesP2P ""github.com/pokt-network/pocket/p2p/types""
 	mock_types ""github.com/pokt-network/pocket/p2p/types/mocks""
 	""github.com/pokt-network/pocket/p2p/utils""
@@ -123,7 +124,8 @@

 
 	var (
 		ctx = context.Background()
-		mu  sync.Mutex
+		// mutex preventing concurrent writes to `seenMessages`
+		seenMessagesMutext sync.Mutex
 		// map used as a set to collect IDs of peers which have received a message
 		seenMessages       = make(map[string]struct{})
 		bootstrapWaitgroup = sync.WaitGroup{}
@@ -147,7 +149,7 @@

 		testHosts = append(testHosts, host)
 		expectedPeerIDs[i] = host.ID().String()
 		rtr := newRouterWithSelfPeerAndHost(t, selfPeer, host)
-		go readSubscription(t, ctx, &broadcastWaitgroup, rtr, &mu, seenMessages)
+		go readSubscription(t, ctx, &broadcastWaitgroup, rtr, &seenMessagesMutext, seenMessages)
 	}
 
 	// bootstrap off of arbitrary testHost
@@ -174,6 +176,8 @@

 
 	bootstrap(t, ctx, testHosts)
 
+	// broadcasting in a go routine so that we can wait for bootstrapping to
+	// complete before broadcasting.
 	go func() {
 		// wait for hosts to listen and peer discovery
 		bootstrapWaitgroup.Wait()
@@ -188,10 +192,8 @@

 		t.Log(""broadcasting..."")
 		err := testRouter.Broadcast([]byte(testMsg))
 		require.NoError(t, err)
-	}()
-
-	// wait concurrently
-	go func() {
+
+		// wait for broadcast to be received by all peers
 		broadcastWaitgroup.Wait()
 		broadcastDone <- struct{}{}
 	}()
@@ -200,7 +202,7 @@

 	select {
 	case <-testTimeout:
 		t.Fatalf(
-			""timed out waiting for message: got %d; wanted %d"",
+			""timed out waiting for all expected messages: got %d; wanted %d"",
 			len(seenMessages),
 			numPeers,
 		)
@@ -217,6 +219,7 @@

 
 	t.Log(""bootstrapping..."")
 	bootstrapHost := testHosts[0]
+	bootstrapAddr := bootstrapHost.Addrs()[0]
 	for _, h := range testHosts {
 		if h.ID() == bootstrapHost.ID() {
 			continue
@@ -228,7 +231,7 @@

 		addrInfo := libp2pPeer.AddrInfo{
 			ID: bootstrapHost.ID(),
 			Addrs: []multiaddr.Multiaddr{
-				bootstrapHost.Addrs()[0].Encapsulate(p2pAddr),
+				bootstrapAddr.Encapsulate(p2pAddr),
 			},
 		}
 
@@ -282,7 +285,7 @@

 	err := pstore.AddPeer(selfPeer)
 	require.NoError(t, err)
 
-	router, err := NewBackgroundRouter(busMock, &utils.RouterConfig{
+	router, err := NewBackgroundRouter(busMock, &config.BackgroundConfig{
 		Addr:                  selfPeer.GetAddress(),
 		PeerstoreProvider:     pstoreProviderMock,
 		CurrentHeightProvider: consensusMock,
@@ -296,7 +299,7 @@

 	return libp2pNet
 }
 
-// TECHDEBT: move & de-dup
+// TECHDEBT(#609): move & de-duplicate
 func newTestPeer(t *testing.T) (cryptoPocket.PrivateKey, *typesP2P.NetworkPeer) {
 	t.Helper()
 
@@ -347,7 +350,7 @@

 func readSubscription(
 	t *testing.T,
 	ctx context.Context,
-	wg *sync.WaitGroup,
+	broadcastWaitGroup *sync.WaitGroup,
 	rtr *backgroundRouter,
 	mu *sync.Mutex,
 	seenMsgs map[string]struct{},
@@ -366,7 +369,7 @@

 		require.NoError(t, err)
 
 		mu.Lock()
-		wg.Done()
+		broadcastWaitGroup.Done()
 		seenMsgs[rtr.host.ID().String()] = struct{}{}
 		mu.Unlock()
 	}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1206007090,https://github.com/pokt-network/pocket/pull/778#discussion_r1206007090,,66,b93c50312e4967daff683ac9fd577e4e5edc2a3b,a516c51f2d000d25f1469d28e6586ef7875d5a37,app/client/cli/servicer.go,nan,"Thoughts on decoupling ""getSession"" and ""getServicer"" or getting the session in place and passing that to `fetchServicer`?

Feels weird to get back a session from a function called `fetchServicer`","+					return fmt.Errorf(""error getting application's private key: %w"", err)
+				}
+
+				session, servicer, err := fetchServicer(cmd.Context(), applicationAddr, chain, servicerAddr)","--- 

+++ 

@@ -2,7 +2,6 @@

 
 import (
 	""context""
-	sha ""crypto""
 	""encoding/hex""
 	""encoding/json""
 	""fmt""
@@ -10,6 +9,7 @@

 
 	""github.com/spf13/cobra""
 
+	""github.com/pokt-network/pocket/app/client/cli/flags""
 	""github.com/pokt-network/pocket/rpc""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
@@ -19,7 +19,6 @@

 	rootCmd.AddCommand(NewServicerCommand())
 }
 
-// TECHDEBT: (unittest) unit test the command: e.g. on number of arguments
 func NewServicerCommand() *cobra.Command {
 	cmd := &cobra.Command{
 		Use:     ""Servicer"",
@@ -45,25 +44,36 @@

 		newUnstakeCmd(cmdDef),
 		newUnpauseCmd(cmdDef),
 		{
-			Use:   ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Short: ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Long: `Sends a trustless relay using <payload> as contents, to the specified active <servicer> in the the <application>'s session.
+			// IMPROVE: allow reading the relay payload from a file with the serialized protobuf via [--input_file]
+			Use:   ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Short: ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Long: `Sends a trustless relay using <relayPayload> as contents, to the specified active <servicerAddrHex> in the the <applicationAddrHex>'s session.
 Will prompt the user for the *application* account passphrase`,
 			Aliases: []string{},
 			Args:    cobra.ExactArgs(4),
 			RunE: func(cmd *cobra.Command, args []string) error {
-				servicerAddr := args[0]
-				applicationAddr := args[1]
+				applicationAddr := args[0]
+				servicerAddr := args[1]
 				chain := args[2]
 				relayPayload := args[3]
 
-				// TODO: (SUGGESTION) refactor to decouple the client logic from the CLI/command
-				pk, err := getPrivateKey(applicationAddr)
+				// REFACTOR: decouple the client logic from the CLI
+				//	The client will: send the trustless relay and return the response (using a single function as entrypoint)
+				//	The CLI will:
+				//		1) extract the required input from the command arguments
+				//		2) call the client function (with the inputs above) that performs the trustless relay
+				pk, err := getPrivateKeyFromKeybase(applicationAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting application's private key: %w"", err)
 				}
 
-				session, servicer, err := fetchServicer(cmd.Context(), applicationAddr, chain, servicerAddr)
+				// TECHDEBT(#791): cache session data
+				session, err := getCurrentSession(cmd.Context(), applicationAddr, chain)
+				if err != nil {
+					return fmt.Errorf(""Error getting current session: %w"", err)
+				}
+
+				servicer, err := validateServicer(session, servicerAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting servicer for the relay: %w"", err)
 				}
@@ -73,7 +83,7 @@

 					return fmt.Errorf(""error building relay from payload: %w"", err)
 				}
 
-				fmt.Printf(""sending trustless relay for %s to %v with payload: %s\n"", applicationAddr, servicer, relayPayload)
+				fmt.Printf(""sending trustless relay for %s to %s with payload: %s\n"", applicationAddr, servicerAddr, relayPayload)
 
 				resp, err := sendTrustlessRelay(cmd.Context(), servicer.ServiceUrl, relay)
 				if err != nil {
@@ -91,59 +101,22 @@

 	return cmds
 }
 
-// TODO: (QUESTION): do we need/want a cli subcommand for fetching servicers?
-
-// fetchServicer returns the servicer specified by the <servicer> argument.
-// It validates the following conditions:
-//
-//	A. The <application> argument is the address of an active application
-//	B. The <servicer> is the address of a servicer that is active in the application's current session.
-//
-// TODO: (SUGGESTION) use a package-internal interface for servicer and application?
-// TODO: (SUGGESTION) use a struct as input to combine all fields (same for output)
-func fetchServicer(ctx context.Context, appAddress, chain, servicerAddress string) (rpc.Session, rpc.ProtocolActor, error) {
-	// TECHDEBT: cache session data
-	session, err := getCurrentSession(ctx, appAddress, chain)
-	if err != nil {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: %w"", err)
-	}
-
-	var (
-		servicer rpc.ProtocolActor
-		found    bool
-	)
-	// TODO: a map may be a better choice for storing servicers
-	for _, s := range session.Servicers {
-		if s.Address == servicerAddress {
-			servicer = s
-			found = true
-			break
+// TODO: add a cli command for fetching sessions
+// validateServicer returns the servicer specified by the <servicer> argument.
+// It validates that the <servicer> is the address of a servicer that is active in the current session.
+func validateServicer(session *rpc.Session, servicerAddress string) (*rpc.ProtocolActor, error) {
+	for i := range session.Servicers {
+		if session.Servicers[i].Address == servicerAddress {
+			return &session.Servicers[i], nil
 		}
 	}
 
-	// TODO: cover with unit tests
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session"", servicerAddress)
-	}
-
-	// TODO: cover with unit tests
-	found = false
-	for _, ch := range servicer.Chains {
-		if ch == chain {
-			found = true
-			break
-		}
-	}
-
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: service %s does not support chain %s"", servicerAddress, chain)
-	}
-
-	return *session, servicer, nil
+	// ADDTEST: cover with gherkin tests
+	return nil, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session %d"", servicerAddress, session.SessionNumber)
 }
 
 func getCurrentSession(ctx context.Context, appAddress, chain string) (*rpc.Session, error) {
-	// TODO: passing 0 as the height value to get the current session seems more optimal than this.
+	// CONSIDERATION: passing 0 as the height value to get the current session seems more optimal than this.
 	currentHeight, err := getCurrentHeight(ctx)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session: %w"", err)
@@ -152,11 +125,11 @@

 	req := rpc.SessionRequest{
 		AppAddress: appAddress,
 		Chain:      chain,
-		// TODO: Geozone
+		// TODO(#697): Geozone
 		SessionHeight: currentHeight,
 	}
 
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session for app/chain/height: %s/%s/%d: %w"", appAddress, chain, currentHeight, err)
 	}
@@ -165,7 +138,8 @@

 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session with request %v: %w"", req, err)
 	}
-	// TODO: refactor boiler-plate code
+
+	// CLEANUP: move the HTTP response processing code to a separate function to enable reuse.
 	if resp.HTTPResponse.StatusCode != http.StatusOK {
 		return nil, fmt.Errorf(""Error getting current session: Unexpected status code %d for request %v"", resp.HTTPResponse.StatusCode, req)
 	}
@@ -177,9 +151,9 @@

 	return resp.JSON200, nil
 }
 
-// TODO: reuse this function in the query commands
+// REFACTOR: reuse this function in all the query commands
 func getCurrentHeight(ctx context.Context) (int64, error) {
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return 0, fmt.Errorf(""Error getting current height: %w"", err)
 	}
@@ -198,51 +172,46 @@

 	return resp.JSON200.Height, nil
 }
 
-// TODO: (localnet) Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
-// TODO: (REFACTOR) should we move package-level variables (e.g. remoteCLIURL) to a cli object?
-func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
+// IMPROVE(#823): [K8s][LocalNet] Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
+// CONSIDERATION: move package-level variables (e.g. RemoteCLIURL) to a cli object and consider storing it in the context
+func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay *rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
 	client, err := rpc.NewClientWithResponses(servicerUrl)
 	if err != nil {
 		return nil, err
 	}
 
-	return client.PostV1ClientRelayWithResponse(ctx, relay)
-}
-
-// TODO: (NICE) allow reading the relay request from the command line arguments AND from a file
-func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session rpc.Session, servicer rpc.ProtocolActor) (rpc.RelayRequest, error) {
+	return client.PostV1ClientRelayWithResponse(ctx, *relay)
+}
+
+func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session *rpc.Session, servicer *rpc.ProtocolActor) (*rpc.RelayRequest, error) {
 	// TECHDEBT: This is mostly COPIED from pocket-go: we should refactor pocket-go code and import this functionality from there instead.
 	relayPayload := rpc.Payload{
-		Data:   payload,
-		Method: ""POST"",
-		// TODO: Path: load Path field from the corresponding Blockchain (e.g. database)
-		// TODO: set Headers
+		// INCOMPLETE(#803): need to unmarshal into JSONRPC and other supported relay formats once proto-generated custom types are added.
+		Jsonrpc: ""2.0"",
+		Method:  payload,
+		// INCOMPLETE: set Headers for HTTP relays
 	}
 
 	relayMeta := rpc.RelayRequestMeta{
 		BlockHeight: session.SessionHeight,
-		// TODO: use Identifiable for Chain in Session (or string for Chain in Relay Meta)
+		// TODO: Make Chain Identifier type consistent in Session and Meta use Identifiable for Chain in Session (or string for Chain in Relay Meta)
 		Chain: rpc.Identifiable{
 			Id: session.Chain,
 		},
 		ServicerPubKey: servicer.PublicKey,
-		// TODO: Geozone
-		// TODO: Token
-	}
-
-	relay := rpc.RelayRequest{
+		// TODO(#697): Geozone
+	}
+
+	relay := &rpc.RelayRequest{
 		Payload: relayPayload,
 		Meta:    relayMeta,
-		// TODO: (QUESTION) why is there no Proof field in v1 struct?
-	}
+	}
+	// TECHDEBT: Evaluate which fields we should and shouldn't marshall when signing the payload
 	reqBytes, err := json.Marshal(relay)
 	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
-	}
-	hashedReq, err := hash(reqBytes)
-	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error hashing relay request bytes %s: %w"", string(reqBytes), err)
-	}
+		return nil, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
+	}
+	hashedReq := crypto.SHA3Hash(reqBytes)
 	signature, err := appPrivateKey.Sign(hashedReq)
 	if err != nil {
 		return relay, fmt.Errorf(""Error signing relay: %w"", err)
@@ -252,23 +221,14 @@

 	return relay, nil
 }
 
-func hash(data []byte) ([]byte, error) {
-	hasher := sha.SHA3_256.New()
-	if _, err := hasher.Write(data); err != nil {
-		return nil, fmt.Errorf(""Error hashing data: %w"", err)
-	}
-
-	return hasher.Sum(nil), nil
-}
-
-// TODO: remove use of package-level variables
-func getPrivateKey(address string) (crypto.PrivateKey, error) {
+// TECHDEBT: remove use of package-level variables, such as NonInteractive, RemoteCLIURL, pwd, etc.
+func getPrivateKeyFromKeybase(address string) (crypto.PrivateKey, error) {
 	kb, err := keybaseForCLI()
 	if err != nil {
 		return nil, err
 	}
 
-	if !nonInteractive {
+	if !flags.NonInteractive {
 		pwd = readPassphrase(pwd)
 	}
 ",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820652338,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1820652338,,176,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,nan,Refactor: Updated the `read` method in the `JsonList` class within `aeon/io/reader.py.`,"         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x: x[column])
+            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -70,29 +69,22 @@

         payloadtype = _payloadtypes[data[4] & ~0x10]
         elementsize = payloadtype.itemsize
         payloadshape = (length, payloadsize // elementsize)
-        seconds = np.ndarray(
-            length, dtype=np.uint32, buffer=data, offset=5, strides=stride
-        )
-        ticks = np.ndarray(
-            length, dtype=np.uint16, buffer=data, offset=9, strides=stride
-        )
+        seconds = np.ndarray(length, dtype=np.uint32, buffer=data, offset=5, strides=stride)
+        ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
-            data = pd.DataFrame(
-                payload, index=seconds, columns=self.columns[: payloadshape[1]]
-            )
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -117,17 +109,13 @@

 
     def __init__(self, pattern=""Metadata""):
         """"""Initialize the object with the specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml""
-        )
+        super().__init__(pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml"")
 
     def read(self, file):
         """"""Returns metadata for the specified epoch.""""""
         epoch_str = file.parts[-2]
         date_str, time_str = epoch_str.split(""T"")
-        time = datetime.datetime.fromisoformat(
-            date_str + ""T"" + time_str.replace(""-"", "":"")
-        )
+        time = datetime.datetime.fromisoformat(date_str + ""T"" + time_str.replace(""-"", "":""))
         with open(file) as fp:
             metadata = json.load(fp)
         workflow = metadata.pop(""Workflow"")
@@ -173,7 +161,7 @@

             df = pd.read_json(f, lines=True)
         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])
+            df[column] = df[self.root_key].apply(lambda x: x[column])  # noqa B023
         return df
 
 
@@ -181,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -196,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -211,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -223,60 +214,43 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
-        super().__init__(
-            pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""]
-        )
+        super().__init__(pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""])
 
 
 class BitmaskEvent(Harp):
     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -300,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -322,15 +297,14 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""]
-        )
+        super().__init__(pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""])
         self._rawcolumns = [""time""] + self.columns[0:2]
 
     def read(self, file):
@@ -347,29 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
-    """"""
-
-    def __init__(
-        self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""
-    ):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
+    """"""
+
+    def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -396,53 +388,34 @@

         # Drop any repeat parts.
         unique_parts, unique_idxs = np.unique(parts, return_index=True)
         repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
-        if (
-            repeat_idxs
-        ):  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
             init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
-            rep_part_col_idxs = np.concatenate(
-                [np.arange(i, i + 3) for i in init_rep_part_col_idx]
-            )
-            keep_part_col_idxs = np.setdiff1d(
-                np.arange(len(data.columns)), rep_part_col_idxs
-            )
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
             data = data.iloc[:, keep_part_col_idxs]
             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = pd.Series(
-            [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
-        )
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
-                columns[0 : (len(identities) + 1)]
-                if bonsai_sleap_v == BONSAI_SLEAP_V3
-                else columns[0:2]
+                columns[0 : (len(identities) + 1)] if bonsai_sleap_v == BONSAI_SLEAP_V3 else columns[0:2]
             )
             part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             part_data = pd.DataFrame(data[part_columns])
             if bonsai_sleap_v == BONSAI_SLEAP_V3:
                 # combine all identity_likelihood cols into a single col as dict
                 part_data[""identity_likelihood""] = part_data.apply(
-                    lambda row: {
-                        identity: row[f""{identity}_likelihood""]
-                        for identity in identities
-                    },
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
                     axis=1,
                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -481,29 +454,19 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod
-    def get_config_file(
-        cls, config_file_dir: Path, config_file_names: None | list[str] = None
-    ) -> Path:
+    def get_config_file(cls, config_file_dir: Path, config_file_names: None | list[str] = None) -> Path:
         """"""Returns the config file from a model's config directory.""""""
         if config_file_names is None:
-            config_file_names = [
-                ""confmap_config.json""
-            ]  # SLEAP (add for other trackers to this list)
+            config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
         config_file = None
         for f in config_file_names:
             if (config_file_dir / f).exists():
@@ -522,21 +485,14 @@

         return globals()[reader_type](pattern=pattern, **kwargs)
 
     return DotMap(
-        {
-            k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k)
-            for k, v in data.items()
-        }
+        {k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k) for k, v in data.items()}
     )
 
 
 def to_dict(dotmap):
     """"""Converts a DotMap object to a dictionary.""""""
     if isinstance(dotmap, Reader):
-        kwargs = {
-            k: v
-            for k, v in vars(dotmap).items()
-            if k not in [""pattern""] and not k.startswith(""_"")
-        }
+        kwargs = {k: v for k, v in vars(dotmap).items() if k not in [""pattern""] and not k.startswith(""_"")}
         kwargs[""type""] = type(dotmap).__name__
         return kwargs
     return {k: to_dict(v) for k, v in dotmap.items()}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1174221617,https://github.com/pokt-network/pocket/pull/684#discussion_r1174221617,,216,33fefdfbfb6dbfb6107ab93e63c3abca7951a012,9359cdf1837fa6bf84632f6ccc8dfb05de0f7296,rpc/handlers.go,nan,"I suggest changing
 ```
+	}
```
 to
```
+	}
+	defer readCtx.Release()
```

Ditto everywhere else","+	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
+	if err != nil {
+		return ctx.String(http.StatusInternalServerError, err.Error())
+	}","--- 

+++ 

@@ -2,17 +2,11 @@

 
 import (
 	""encoding/hex""
-	""errors""
-	""fmt""
-	""math/big""
 	""net/http""
-	""strconv""
 
 	""github.com/labstack/echo/v4""
 	""github.com/pokt-network/pocket/app""
-	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
-	""github.com/pokt-network/pocket/shared/utils""
 )
 
 // CONSIDER: Remove all the V1 prefixes from the RPC module
@@ -36,7 +30,7 @@

 		return ctx.String(http.StatusBadRequest, ""cannot decode tx bytes"")
 	}
 
-	if err = s.GetBus().GetUtilityModule().HandleTransaction(txBz); err != nil {
+	if err := s.GetBus().GetUtilityModule().HandleTransaction(txBz); err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
 
@@ -47,9 +41,8 @@

 	return nil
 }
 
-// DISCUSSION: This may need to be changed when the GetSession function is actually implemented
-func (s *rpcServer) PostV1ClientDispatch(ctx echo.Context) error {
-	var body DispatchRequest
+func (s *rpcServer) PostV1ClientGetSession(ctx echo.Context) error {
+	var body SessionRequest
 	if err := ctx.Bind(&body); err != nil {
 		return ctx.String(http.StatusBadRequest, ""bad request"")
 	}
@@ -63,29 +56,32 @@

 	rpcApp := protocolActorToRPCProtocolActor(application)
 
 	rpcServicers := make([]ProtocolActor, 0)
-	for _, serv := range session.GetServicers() {
-		actor := protocolActorToRPCProtocolActor(serv)
+	for _, servicer := range session.GetServicers() {
+		actor := protocolActorToRPCProtocolActor(servicer)
 		rpcServicers = append(rpcServicers, actor)
 	}
 
 	rpcFishermen := make([]ProtocolActor, 0)
-	for _, fm := range session.GetFishermen() {
-		actor := protocolActorToRPCProtocolActor(fm)
+	for _, fisher := range session.GetFishermen() {
+		actor := protocolActorToRPCProtocolActor(fisher)
 		rpcFishermen = append(rpcFishermen, actor)
 	}
 
 	return ctx.JSON(http.StatusOK, Session{
-		SessionId:   session.GetId(),
-		Height:      session.GetHeight(),
-		Chain:       string(session.GetRelayChain()),
-		Geozone:     string(session.GetGeoZone()),
-		Application: rpcApp,
-		Servicers:   rpcServicers,
-		Fishermen:   rpcFishermen,
-	})
-}
-
-// DISCUSSION: This may need to be changed when the SendRelay function is actually implemented
+		SessionId:        session.GetId(),
+		SessionNumber:    session.GetSessionNumber(),
+		SessionHeight:    session.GetSessionHeight(),
+		NumSessionBlocks: session.GetNumSessionBlocks(),
+		Chain:            string(session.GetRelayChain()),
+		Geozone:          string(session.GetGeoZone()),
+		Application:      rpcApp,
+		Servicers:        rpcServicers,
+		Fishermen:        rpcFishermen,
+	})
+}
+
+// TECHDEBT: This will need to be changed when the HandleRelay function is actually implemented
+// because it copies data structures from v0. For example, AATs are no longer necessary in v1.
 func (s *rpcServer) PostV1ClientRelay(ctx echo.Context) error {
 	var body RelayRequest
 	if err := ctx.Bind(&body); err != nil {
@@ -133,7 +129,7 @@

 		Meta:    relayMeta,
 	}
 
-	relayResponse, err := s.GetBus().GetUtilityModule().SendRelay(relayRequest)
+	relayResponse, err := s.GetBus().GetUtilityModule().HandleRelay(relayRequest)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -144,7 +140,8 @@

 	})
 }
 
-// DISCUSSION: This may need to be changed when the SendRelay function is actually implemented
+// TECHDEBT: This will need to be changed when the HandleChallenge function is actually implemented
+// because it copies data structures from v0
 func (s *rpcServer) PostV1ClientChallenge(ctx echo.Context) error {
 	var body ChallengeRequest
 	if err := ctx.Bind(&body); err != nil {
@@ -193,830 +190,6 @@

 	})
 }
 
-// Queries
-
-func (s *rpcServer) PostV1QueryAccount(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	accBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	amount, err := readCtx.GetAccountAmount(accBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, Account{
-		Address: body.Address,
-		Coins:   []Coin{{Amount: amount, Denom: ""upokt""}},
-	})
-}
-
-func (s *rpcServer) PostV1QueryAccounts(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allAccounts, err := readCtx.GetAllAccounts(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allAccounts), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAccountsResponse{})
-	}
-
-	accounts := make([]Account, 0)
-	for _, account := range allAccounts[start : end+1] {
-		accounts = append(accounts, Account{
-			Address: account.Address,
-			Coins:   []Coin{{Amount: account.Amount, Denom: ""upokt""}},
-		})
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAccountsResponse{
-		Result:     accounts,
-		Page:       body.Page,
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryAccounttxs(ctx echo.Context) error {
-	var body QueryAddressPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-	sort := checkSort(*body.Sort)
-	sortDesc := true
-	if sort == ""asc"" {
-		sortDesc = false
-	}
-
-	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResults, err := txIndexer.GetBySender(body.Address, sortDesc)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(txResults), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{})
-	}
-
-	pageTxs := make([]Transaction, 0)
-	for _, txResult := range txResults[start : end+1] {
-		rpcTx, err := s.txResultToRPCTransaction(txResult)
-		if err != nil {
-			return ctx.String(http.StatusInternalServerError, err.Error())
-		}
-		pageTxs = append(pageTxs, *rpcTx)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAccountTxsResponse{
-		Txs:        pageTxs,
-		Page:       body.Page,
-		TotalTxs:   int64(len(txResults)),
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) GetV1QueryAllChainParams(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	paramSlice, err := readCtx.GetAllParams()
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	resp := make([]Parameter, 0)
-	for i := 0; i < len(paramSlice); i++ {
-		resp = append(resp, Parameter{
-			ParameterName:  paramSlice[i][0],
-			ParameterValue: paramSlice[i][1],
-		})
-	}
-	return ctx.JSON(200, resp)
-}
-
-func (s *rpcServer) PostV1QueryApp(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	application, err := readCtx.GetApp(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(application)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryApps(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allApps, err := readCtx.GetAllApps(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allApps), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryAppsResponse{})
-	}
-
-	rpcApps := make([]ProtocolActor, 0)
-	for _, app := range allApps[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(app)
-		rpcApps = append(rpcApps, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryAppsResponse{
-		Apps:       rpcApps,
-		TotalApps:  int64(len(allApps)),
-		Page:       body.Page,
-		TotalPages: int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryBalance(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	accBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	amountStr, err := readCtx.GetAccountAmount(accBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	amount, err := strconv.ParseInt(amountStr, 10, 64)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryBalanceResponse{
-		Balance: amount,
-	})
-}
-
-func (s *rpcServer) PostV1QueryBlock(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := uint64(body.Height)
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
-	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	block := new(coreTypes.Block)
-	if err := codec.GetCodec().Unmarshal(blockBz, block); err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	rpcBlock, err := s.blockToRPCBlock(block)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcBlock)
-}
-
-func (s *rpcServer) PostV1QueryBlocktxs(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-	sort := checkSort(*body.Sort)
-	sortDesc := true
-	if sort == ""asc"" {
-		sortDesc = false
-	}
-
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := uint64(body.Height)
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	blockStore := s.GetBus().GetPersistenceModule().GetBlockStore()
-	blockBz, err := blockStore.Get(utils.HeightToBytes(height))
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	block := new(coreTypes.Block)
-	if err := codec.GetCodec().Unmarshal(blockBz, block); err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	rpcBlock, err := s.blockToRPCBlock(block)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allTxs := rpcBlock.Transactions
-	if sortDesc {
-		for i, j := 0, len(allTxs)-1; i < j; i, j = i+1, j-1 {
-			allTxs[i], allTxs[j] = allTxs[j], allTxs[i]
-		}
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allTxs), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryTxsResponse{})
-	}
-
-	return ctx.JSON(http.StatusOK, QueryTxsResponse{
-		Transactions: allTxs[start : end+1],
-		TotalTxs:     int64(len(allTxs)),
-		Page:         body.Page,
-		TotalPages:   int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryFisherman(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	fisherman, err := readCtx.GetFisherman(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(fisherman)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryFishermen(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allFishermen, err := readCtx.GetAllFishermen(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allFishermen), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryFishermenResponse{})
-	}
-
-	rpcFishermen := make([]ProtocolActor, 0)
-	for _, fm := range allFishermen[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(fm)
-		rpcFishermen = append(rpcFishermen, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryFishermenResponse{
-		Fishermen:      rpcFishermen,
-		TotalFishermen: int64(len(allFishermen)),
-		Page:           body.Page,
-		TotalPages:     int64(totalPages),
-	})
-}
-
-func (s *rpcServer) GetV1QueryHeight(ctx echo.Context) error {
-	// Get latest stored block height
-	currentHeight := s.GetBus().GetConsensusModule().CurrentHeight()
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-
-	return ctx.JSON(http.StatusOK, QueryHeight{
-		Height: int64(currentHeight),
-	})
-}
-
-func (s *rpcServer) PostV1QueryParam(ctx echo.Context) error {
-	var body QueryParameter
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	paramValue, err := readCtx.GetStringParam(body.ParamName, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, Parameter{
-		ParameterName:  body.ParamName,
-		ParameterValue: paramValue,
-	})
-}
-
-func (s *rpcServer) PostV1QueryServicer(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	servicer, err := readCtx.GetServicer(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(servicer)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryServicers(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allServicers, err := readCtx.GetAllServicers(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allServicers), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryServicersResponse{})
-	}
-
-	rpcServicers := make([]ProtocolActor, 0)
-	for _, serv := range allServicers[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(serv)
-		rpcServicers = append(rpcServicers, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryServicersResponse{
-		Servicers:      rpcServicers,
-		TotalServicers: int64(len(allServicers)),
-		Page:           body.Page,
-		TotalPages:     int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QuerySupply(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	pools, err := readCtx.GetAllPools(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	rpcPools := make([]Pool, 0)
-	total := new(big.Int)
-	for _, pool := range pools {
-		name := coreTypes.PoolAddressToFriendlyName(pool.Address)
-		amount, success := new(big.Int).SetString(pool.Amount, 10)
-		if !success {
-			return ctx.String(http.StatusInternalServerError, ""failed to convert amount to big.Int"")
-		}
-		total = total.Add(total, amount)
-		rpcPools = append(rpcPools, Pool{
-			Address: pool.Address,
-			Name:    name,
-			Amount:  pool.Amount,
-			Denom:   ""upokt"",
-		})
-	}
-
-	return ctx.JSON(http.StatusOK, QuerySupplyResponse{
-		Pools: rpcPools,
-		Total: Coin{
-			Amount: total.String(),
-			Denom:  ""upokt"",
-		},
-	})
-}
-
-func (s *rpcServer) PostV1QuerySupportedchains(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	chains, err := readCtx.GetSupportedChains(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QuerySupportedChainsResponse{
-		SupportedChains: chains,
-	})
-}
-
-func (s *rpcServer) PostV1QueryTx(ctx echo.Context) error {
-	var body QueryHash
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	hashBz, err := hex.DecodeString(body.Hash)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	txIndexer := s.GetBus().GetPersistenceModule().GetTxIndexer()
-	txResult, err := txIndexer.GetByHash(hashBz)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	rpcTx, err := s.txResultToRPCTransaction(txResult)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcTx)
-}
-
-func (s *rpcServer) PostV1QueryUnconfirmedtx(ctx echo.Context) error {
-	var body QueryHash
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	mempool := s.GetBus().GetUtilityModule().GetMempool()
-	uncTx := mempool.Get(body.Hash)
-	if uncTx == nil {
-		return ctx.String(http.StatusBadRequest, fmt.Sprintf(""hash not found in mempool: %s"", body.Hash))
-	}
-
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions([][]byte{uncTx})
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, rpcUncTxs[0])
-}
-
-func (s *rpcServer) PostV1QueryUnconfirmedtxs(ctx echo.Context) error {
-	var body QueryPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	mempool := s.GetBus().GetUtilityModule().GetMempool()
-	uncTxs := mempool.GetAll()
-
-	start, end, totalPages, err := getPageIndexes(len(uncTxs), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryTxsResponse{})
-	}
-
-	rpcUncTxs, err := s.txProtoBytesToRPCTransactions(uncTxs[start : end+1])
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryTxsResponse{
-		Transactions: rpcUncTxs,
-		TotalTxs:     int64(len(uncTxs)),
-		Page:         body.Page,
-		TotalPages:   int64(totalPages),
-	})
-}
-
-func (s *rpcServer) PostV1QueryUpgrade(ctx echo.Context) error {
-	var body QueryHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 || height > currentHeight {
-		height = currentHeight
-	}
-	reatCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	version, err := reatCtx.GetVersionAtHeight(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	return ctx.JSON(http.StatusOK, QueryUpgradeResponse{
-		Height:  height,
-		Version: version,
-	})
-}
-
-func (s *rpcServer) PostV1QueryValidator(ctx echo.Context) error {
-	var body QueryAddressHeight
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	addrBz, err := hex.DecodeString(body.Address)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	validator, err := readCtx.GetValidator(addrBz, height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	actor := protocolActorToRPCProtocolActor(validator)
-	return ctx.JSON(http.StatusOK, actor)
-}
-
-func (s *rpcServer) PostV1QueryValidators(ctx echo.Context) error {
-	var body QueryHeightPaginated
-	if err := ctx.Bind(&body); err != nil {
-		return ctx.String(http.StatusBadRequest, ""bad request"")
-	}
-
-	// Get latest stored block height
-	currentHeight := int64(s.GetBus().GetConsensusModule().CurrentHeight())
-	if currentHeight > 0 {
-		currentHeight -= 1
-	}
-	height := body.Height
-	if height == 0 {
-		height = currentHeight
-	}
-	readCtx, err := s.GetBus().GetPersistenceModule().NewReadContext(currentHeight)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-	allValidators, err := readCtx.GetAllValidators(height)
-	if err != nil {
-		return ctx.String(http.StatusInternalServerError, err.Error())
-	}
-
-	start, end, totalPages, err := getPageIndexes(len(allValidators), int(body.Page), int(body.PerPage))
-	if err != nil && !errors.Is(err, errNoItems) {
-		return ctx.String(http.StatusBadRequest, err.Error())
-	}
-	if totalPages == 0 || errors.Is(err, errNoItems) {
-		return ctx.JSON(http.StatusOK, QueryValidatorsResponse{})
-	}
-
-	rpcValidators := make([]ProtocolActor, 0)
-	for _, val := range allValidators[start : end+1] {
-		actor := protocolActorToRPCProtocolActor(val)
-		rpcValidators = append(rpcValidators, actor)
-	}
-
-	return ctx.JSON(http.StatusOK, QueryValidatorsResponse{
-		Validators:      rpcValidators,
-		TotalValidators: int64(len(allValidators)),
-		Page:            body.Page,
-		TotalPages:      int64(totalPages),
-	})
-}
-
 func (s *rpcServer) GetV1P2pStakedActorsAddressBook(ctx echo.Context, params GetV1P2pStakedActorsAddressBookParams) error {
 	var height int64
 	var actors []Actor",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1251330088,https://github.com/pokt-network/pocket/pull/732#discussion_r1251330088,,123,4602283cbd676952b2a5c20f73758305cc833ce5,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/background/router.go,nan,Great usage of multierr everywhere!," 	}
 
-	return rtr, nil
+	return multierr.Append(","--- 

+++ 

@@ -49,6 +49,7 @@

 	host libp2pHost.Host
 	// cancelReadSubscription is the cancel function for the context which is
 	// monitored in the `#readSubscription()` go routine. Call to terminate it.
+	// only one read subscription exists per router at any point in time
 	cancelReadSubscription context.CancelFunc
 
 	// Fields below are assigned during creation via `#setupDependencies()`.
@@ -215,6 +216,7 @@

 }
 
 func (rtr *backgroundRouter) setupDependencies(ctx context.Context, cfg *config.BackgroundConfig) error {
+	// NB: The order in which the internal components are setup below is important
 	if err := rtr.setupUnicastRouter(); err != nil {
 		return err
 	}
@@ -237,7 +239,9 @@

 
 	if err := rtr.setupPeerstore(
 		ctx,
-		cfg.PeerstoreProvider, cfg.CurrentHeightProvider); err != nil {
+		cfg.PeerstoreProvider,
+		cfg.CurrentHeightProvider,
+	); err != nil {
 		return fmt.Errorf(""setting up peerstore: %w"", err)
 	}
 	return nil
@@ -263,6 +267,7 @@

 	return nil
 }
 
+// setupPeerDiscovery sets up the Kademlia Distributed Hash Table (DHT)
 func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {
 	dhtMode := dht.ModeAutoServer
 	// NB: don't act as a bootstrap node in peer discovery in client debug mode
@@ -274,6 +279,7 @@

 	return err
 }
 
+// setupPubsub sets up a new gossip sub topic using libp2p
 func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {
 	// TECHDEBT(#730): integrate libp2p tracing via `pubsub.WithEventTracer()`.
 
@@ -313,8 +319,8 @@

 }
 
 func (rtr *backgroundRouter) bootstrap(ctx context.Context) error {
-	// CONSIDERATION: add `GetPeers` method to `PeerstoreProvider` interface
-	// to avoid this loop.
+	// CONSIDERATION: add `GetPeers` method, which returns a map,
+	// to the `PeerstoreProvider` interface to simplify this loop.
 	for _, peer := range rtr.pstore.GetPeerList() {
 		if err := utils.AddPeerToLibp2pHost(rtr.host, peer); err != nil {
 			return err
@@ -352,22 +358,26 @@

 func (rtr *backgroundRouter) topicValidator(_ context.Context, _ libp2pPeer.ID, msg *pubsub.Message) bool {
 	var backgroundMsg typesP2P.BackgroundMessage
 	if err := proto.Unmarshal(msg.Data, &backgroundMsg); err != nil {
+		rtr.logger.Error().Err(err).Msg(""unmarshalling Background message"")
 		return false
 	}
 
 	if backgroundMsg.Data == nil {
+		rtr.logger.Debug().Msg(""no data in Background message"")
 		return false
 	}
 
-	networkMessage := messaging.PocketEnvelope{}
-	if err := proto.Unmarshal(backgroundMsg.Data, &networkMessage); err != nil {
-		rtr.logger.Error().Err(err).Msg(""Error decoding network message"")
+	poktEnvelope := messaging.PocketEnvelope{}
+	if err := proto.Unmarshal(backgroundMsg.Data, &poktEnvelope); err != nil {
+		rtr.logger.Error().Err(err).Msg(""Error decoding Background message"")
 		return false
 	}
 
 	return true
 }
 
+// readSubscription is a while loop for receiving and handling messages from the
+// subscription. It is intended to be called as a goroutine.
 func (rtr *backgroundRouter) readSubscription(ctx context.Context) {
 	for {
 		if err := ctx.Err(); err != nil {",,,,
mattzcarey/code-review-gpt,https://api.github.com/repos/pass-culture/data-gcp/pulls/comments/1880340949,https://github.com/pass-culture/data-gcp/pull/3585#discussion_r1880340949,,103,bf8deef12fda58e360f2198987a26c2b6c92e3e5,17b1a6dcc9852af137b9918ce5c1e73b5454d97d,orchestration/dags/jobs/ml/algo_default_deployment.py,nan,typo,"-
-        deploy_model.set_upstream(seq_task)
-        seq_task = deploy_model
+    deploy_model_taks = {}","--- 

+++ 

@@ -100,8 +100,6 @@

         retries=2,
     )
 
-    deploy_model_taks = {}
-    seq_task = fetch_install_code
     with TaskGroup(""deploy_models"", dag=dag) as deploy_models:
         for model_params in models_to_deploy:
             experiment_name = model_params[""experiment_name""]
@@ -121,7 +119,7 @@

                     --max-nodes {max_nodes}
             """"""
 
-            deploy_model_taks[endpoint_name] = SSHGCEOperator(
+            SSHGCEOperator(
                 task_id=f""deploy_model_{experiment_name}_{endpoint_name}"",
                 instance_name=GCE_INSTANCE,
                 base_dir=BASE_DIR,",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1210858477,https://github.com/pokt-network/pocket/pull/778#discussion_r1210858477,,234,3f7c8f63ff1d05e64f8672113a70545632501fca,a516c51f2d000d25f1469d28e6586ef7875d5a37,rpc/handlers.go,nan,Why not assign to `payload.JsonRpcPayload.Headers[header.Name]` directly? I believe it's auto initialized to an empty map.,"+	if body.Payload.Headers != nil {
+		headers := make(map[string]string)
+		for _, header := range *body.Payload.Headers {
+			headers[header.Name] = header.Value","--- 

+++ 

@@ -30,10 +30,12 @@

 		return ctx.String(http.StatusBadRequest, ""cannot decode tx bytes"")
 	}
 
+	// Validate the transaction and add it to the mempool
 	if err := s.GetBus().GetUtilityModule().HandleTransaction(txBz); err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
 
+	// Broadcast the transaction to the rest of the network if it passed the basic validation above
 	if err := s.broadcastMessage(txBz); err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -81,6 +83,13 @@

 }
 
 func (s *rpcServer) PostV1ClientRelay(ctx echo.Context) error {
+	utility := s.GetBus().GetUtilityModule()
+	_, err := utility.GetServicerModule()
+
+	if err != nil {
+		return ctx.String(http.StatusInternalServerError, ""node is not a servicer"")
+	}
+
 	var body RelayRequest
 	if err := ctx.Bind(&body); err != nil {
 		return ctx.String(http.StatusBadRequest, ""bad request"")
@@ -103,10 +112,10 @@

 		Signature:         body.Meta.Signature,
 	}
 
-	relayRequest := buildRelayPayload(body)
+	relayRequest := buildJsonRPCRelayPayload(&body)
 	relayRequest.Meta = relayMeta
 
-	relayResponse, err := s.GetBus().GetUtilityModule().HandleRelay(relayRequest)
+	relayResponse, err := utility.HandleRelay(relayRequest)
 	if err != nil {
 		return ctx.String(http.StatusInternalServerError, err.Error())
 	}
@@ -207,8 +216,8 @@

 	return ctx.JSON(http.StatusOK, response)
 }
 
-// TECHDEBT: handle other relay payload types
-func buildRelayPayload(body RelayRequest) *coreTypes.Relay {
+// TECHDEBT: handle other relay payload types, e.g. JSON, GRPC, etc.
+func buildJsonRPCRelayPayload(body *RelayRequest) *coreTypes.Relay {
 	payload := &coreTypes.Relay_JsonRpcPayload{
 		JsonRpcPayload: &coreTypes.JSONRPCPayload{
 			JsonRpc: body.Payload.Jsonrpc,
@@ -218,10 +227,6 @@

 
 	if body.Payload.Id != nil {
 		payload.JsonRpcPayload.Id = []byte(*body.Payload.Id)
-	}
-
-	if body.Payload.Path != nil {
-		payload.JsonRpcPayload.HttpPath = *body.Payload.Path
 	}
 
 	if body.Payload.Parameters != nil {",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1906709003,https://github.com/parkpow/deep-license-plate-recognition/pull/225#discussion_r1906709003,,51,95d04ed7e2449ec3732881a6cff35e91ea579dd5,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/main.py,nan,nit: isn't datetime.strptime better for this?,"+        #  ""11:49:22"", Format HH/MM/SS
+        hour, minute, second = data[""TimeUtc""].split("":"")
+        lgr.debug([hour, minute, second])
+        created_date = datetime(","--- 

+++ 

@@ -42,21 +42,12 @@

     def enqueue(self, data, params):
         camera_id = data[""CameraName""]
         image_base_64 = data[""ContextImage""]
-        # ""10/01/2022"", Format DD/MM/YYYY
-        month, day, year = data[""DateUtc""].split(""/"")
-        lgr.debug([month, day, year])
-        #  ""11:49:22"", Format HH/MM/SS
-        hour, minute, second = data[""TimeUtc""].split("":"")
-        lgr.debug([hour, minute, second])
-        created_date = datetime(
-            int(year),
-            int(month),
-            int(day),
-            int(hour),
-            int(minute),
-            int(second),
-            tzinfo=timezone.utc,
-        )
+        date_utc = data[""DateUtc""]
+        time_utc = data[""TimeUtc""]
+        created_date = datetime.strptime(
+            f""{date_utc} {time_utc}"", ""%d/%m/%Y %H:%M:%S""
+        ).replace(tzinfo=timezone.utc)
+
         # created_date = datetime.now() uncomment for testing
         if isinstance(self.api, ParkPowApi):
             v_attrs = data[""Attributes""]",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228700393,https://github.com/pokt-network/pocket/pull/803#discussion_r1228700393,,19,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,persistence/local.go,nan,"Because we have a few `INCOMPLETE` that are related and are a multi-day effort, I think it's worth creating a ticket  an adding it in cracked: `INCOMPLETE(#XXX): ... `","+	return nil
+}
+
+// INCOMPLETE: implement this",File_Deleted,,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/hyperskill/mobile-app/pulls/comments/1483859108,https://github.com/hyperskill/mobile-app/pull/888#discussion_r1483859108,,427,b45cae7067fb467277485d8111361e44ba29f83a,e8910814a28e62ea4e588eb466fdc3921ad02be5,shared/src/commonMain/resources/MR/base/strings.xml,nan,"I suggest changing
 ```
+    <string name=""problems_limit_reached_modal_unlock_unlimited_problems_description"">You\'ve solved %d problems today. Great job! Unlock unlimited problems with Mobile only plan</string>
```
 to
```
+    <string name=""problems_limit_reached_modal_unlock_unlimited_problems_description"">You\'ve solved %d problems today. Great job! Unlock unlimited problems with Mobile only plan.</string>
```","+    <!--Problems limit reached modal-->
     <string name=""problems_limit_reached_modal_title"">You\'ve reached your daily limit</string>
     <string name=""problems_limit_reached_modal_description"">You\'ve solved %d problems today. Great job! Tomorrow new problems will be available to you.</string>
+    <string name=""problems_limit_reached_modal_unlock_unlimited_problems_description"">You\'ve solved %d problems today. Great job! Unlock unlimited problems with Mobile only plan</string>","--- 

+++ 

@@ -424,8 +424,8 @@

     <!--Problems limit reached modal-->
     <string name=""problems_limit_reached_modal_title"">You\'ve reached your daily limit</string>
     <string name=""problems_limit_reached_modal_description"">You\'ve solved %d problems today. Great job! Tomorrow new problems will be available to you.</string>
-    <string name=""problems_limit_reached_modal_unlock_unlimited_problems_description"">You\'ve solved %d problems today. Great job! Unlock unlimited problems with Mobile only plan</string>
-    <string name=""problems_limit_reached_buy_subscription_button"">Unlock unlimited problems</string>
+    <string name=""problems_limit_reached_modal_unlock_unlimited_problems_description"">You\'ve solved %d problems today. Great job! Unlock unlimited problems with Mobile only plan.</string>
+    <string name=""problems_limit_reached_modal_buy_subscription_button"">Unlock unlimited problems</string>
 
     <!--  Problems limit widget  -->
     <string name=""problems_limit_widget_problems_limit"">%d/%d problems left</string>",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1180838572,https://github.com/pokt-network/pocket/pull/683#discussion_r1180838572,,513,d06787e78227b872e61af15b05f8414b4d4be31a,fb245bc027ae55aaee936daf9f85ecb2d9ad8b40,utility/session_test.go,nan,"See: https://pkg.go.dev/testing#T.Helper

I suggest changing
 ```
+func assertActorsDifference(t *testing.T, actors1, actors2 []*coreTypes.Actor, maxSimilarityThreshold float64) {
```
 to
```
+func assertActorsDifference(t *testing.T, actors1, actors2 []*coreTypes.Actor, maxSimilarityThreshold float64) {
+  t.Helper()
```","+	// until the new parameter takes effect. There are open design questions that need to be made.
+}
+
+func assertActorsDifference(t *testing.T, actors1, actors2 []*coreTypes.Actor, maxSimilarityThreshold float64) {","--- 

+++ 

@@ -511,8 +511,10 @@

 }
 
 func assertActorsDifference(t *testing.T, actors1, actors2 []*coreTypes.Actor, maxSimilarityThreshold float64) {
-	slice1 := actorsToAddrs(actors1)
-	slice2 := actorsToAddrs(actors2)
+	t.Helper()
+
+	slice1 := actorsToAddrs(t, actors1)
+	slice2 := actorsToAddrs(t, actors2)
 	var commonCount float64
 	for _, s1 := range slice1 {
 		for _, s2 := range slice2 {
@@ -526,7 +528,9 @@

 	assert.LessOrEqual(t, commonCount, maxCommonCount, ""Slices have more similarity than expected: %v vs max %v"", slice1, slice2)
 }
 
-func actorsToAddrs(actors []*coreTypes.Actor) []string {
+func actorsToAddrs(t *testing.T, actors []*coreTypes.Actor) []string {
+	t.Helper()
+
 	addresses := make([]string, len(actors))
 	for i, actor := range actors {
 		addresses[i] = actor.Address",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1243115416,https://github.com/pokt-network/pocket/pull/732#discussion_r1243115416,,29,8467f3a832ae26b48ed9ec481a8b99355c849c09,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/event_handler.go,nan,"OPTIONAL PERSONAL STYLE NIT:  Reduce indent for readability

```go
isStaked, err := m.isStakedActor()
if err != nil {
   return err
}
if !isStaked {
   return nil
}
// business logic
```
","-		added, removed := oldPeerList.Delta(updatedPeerstore.GetPeerList())
-		for _, add := range added {
-			if err := m.router.AddPeer(add); err != nil {
+		} else if isStaked {","--- 

+++ 

@@ -26,23 +26,25 @@

 
 		if isStaked, err := m.isStakedActor(); err != nil {
 			return err
-		} else if isStaked {
-			oldPeerList := m.stakedActorRouter.GetPeerstore().GetPeerList()
-			updatedPeerstore, err := m.pstoreProvider.GetStakedPeerstoreAtHeight(consensusNewHeightEvent.Height)
-			if err != nil {
+		} else if !isStaked {
+			return nil // unstaked actors do not use RainTree and therefore do not need to update this router
+		}
+
+		oldPeerList := m.stakedActorRouter.GetPeerstore().GetPeerList()
+		updatedPeerstore, err := m.pstoreProvider.GetStakedPeerstoreAtHeight(consensusNewHeightEvent.Height)
+		if err != nil {
+			return err
+		}
+
+		added, removed := oldPeerList.Delta(updatedPeerstore.GetPeerList())
+		for _, add := range added {
+			if err := m.stakedActorRouter.AddPeer(add); err != nil {
 				return err
 			}
-
-			added, removed := oldPeerList.Delta(updatedPeerstore.GetPeerList())
-			for _, add := range added {
-				if err := m.stakedActorRouter.AddPeer(add); err != nil {
-					return err
-				}
-			}
-			for _, rm := range removed {
-				if err := m.stakedActorRouter.RemovePeer(rm); err != nil {
-					return err
-				}
+		}
+		for _, rm := range removed {
+			if err := m.stakedActorRouter.RemovePeer(rm); err != nil {
+				return err
 			}
 		}
 
@@ -55,13 +57,25 @@

 		m.logger.Debug().Fields(messaging.TransitionEventToMap(stateMachineTransitionEvent)).Msg(""Received state machine transition event"")
 
 		if stateMachineTransitionEvent.NewState == string(coreTypes.StateMachineState_P2P_Bootstrapping) {
-			if m.stakedActorRouter.GetPeerstore().Size() == 0 {
-				m.logger.Warn().Msg(""No peers in addrbook, bootstrapping"")
+			staked, err := m.isStakedActor()
+			if err != nil {
+				return err
+			}
+			if staked {
+				// TECHDEBT(#859): this will never happen as the peerstore is
+				// seeded from consensus during P2P module construction.
+				if m.stakedActorRouter.GetPeerstore().Size() == 0 {
+					m.logger.Warn().Msg(""No peers in peerstore, bootstrapping"")
 
-				if err := m.bootstrap(); err != nil {
-					return err
+					if err := m.bootstrap(); err != nil {
+						return err
+					}
 				}
 			}
+
+			// TECHDEBT(#859): for unstaked actors, unstaked actor (background)
+			// router bootstrapping SHOULD complete before the event below is sent.
+
 			m.logger.Info().Bool(""TODO"", true).Msg(""Advertise self to network"")
 			if err := m.GetBus().GetStateMachineModule().SendEvent(coreTypes.StateMachineEvent_P2P_IsBootstrapped); err != nil {
 				return err",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1220421661,https://github.com/pokt-network/pocket/pull/756#discussion_r1220421661,,1,3c65f9c895487df8f1b6109168200bba6ac7875f,70a1a0e2fe2c238fce8c2019e13e9167629a2639,shared/modules/gomock_reflect_1203808572/prog.go,nan,Intended to be merged in?,"@@ -0,0 +1,66 @@
+",File_Deleted,,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658854,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1820658854,,51,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_pipeline_instantiation.py,nan,S101: Replaced assertions with exceptions,"-    exp_subjects = (acquisition.Experiment.Subject & {""experiment_name"": experiment_name}).fetch(""subject"")
-    assert len(exp_subjects) == test_params[""subject_count""]
-    assert ""BAA-1100701"" in exp_subjects
+    if raw_dir != test_params[""raw_dir""]:","--- 

+++ 

@@ -1,36 +1,16 @@

 """"""Tests for pipeline instantiation and experiment creation.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.instantiation
 def test_pipeline_instantiation(pipeline):
-    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):
-        raise AssertionError(
-            ""Pipeline acquisition does not have 'FoodPatchEvent' attribute.""
-        )
-
-    if not hasattr(pipeline[""lab""], ""Arena""):
-        raise AssertionError(""Pipeline lab does not have 'Arena' attribute."")
-
-    if not hasattr(pipeline[""qc""], ""CameraQC""):
-        raise AssertionError(""Pipeline qc does not have 'CameraQC' attribute."")
-
-    if not hasattr(pipeline[""report""], ""InArenaSummaryPlot""):
-        raise AssertionError(
-            ""Pipeline report does not have 'InArenaSummaryPlot' attribute.""
-        )
-
-    if not hasattr(pipeline[""subject""], ""Subject""):
-        raise AssertionError(""Pipeline subject does not have 'Subject' attribute."")
-
-    if not hasattr(pipeline[""tracking""], ""CameraTracking""):
-        raise AssertionError(
-            ""Pipeline tracking does not have 'CameraTracking' attribute.""
-        )
+    assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
+    assert hasattr(pipeline[""lab""], ""Arena"")
+    assert hasattr(pipeline[""qc""], ""CameraQC"")
+    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
+    assert hasattr(pipeline[""subject""], ""Subject"")
+    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
 
 
 @pytest.mark.instantiation
@@ -38,30 +18,14 @@

     acquisition = pipeline[""acquisition""]
 
     experiment_name = test_params[""experiment_name""]
-    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")
-    if fetched_experiment_name != experiment_name:
-        raise AssertionError(
-            f""Expected experiment name '{experiment_name}', but got '{fetched_experiment_name}'.""
-        )
-
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
         acquisition.Experiment.Directory
         & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
-    if raw_dir != test_params[""raw_dir""]:
-        raise AssertionError(
-            f""Expected raw directory '{test_params['raw_dir']}', but got '{raw_dir}'.""
-        )
-
+    assert raw_dir == test_params[""raw_dir""]
     exp_subjects = (
         acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
     ).fetch(""subject"")
-    if len(exp_subjects) != test_params[""subject_count""]:
-        raise AssertionError(
-            f""Expected subject count {test_params['subject_count']}, but got {len(exp_subjects)}.""
-        )
-
-    if ""BAA-1100701"" not in exp_subjects:
-        raise AssertionError(
-            ""Expected subject 'BAA-1100701' not found in experiment subjects.""
-        )
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1235980764,https://github.com/pokt-network/pocket/pull/803#discussion_r1235980764,,229,f3bfe1387700a38296d89201c5e7cce3f78a63eb,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/servicer/module_test.go,nan,"Should we update the test to getting pocket height? Otherwise, this is confusing of using `POKT-UnitTest` with `eth_blockNumber`.

My suggestion:
1. POKT-UnitTest with REST payload getting `/v1/height`
2. ETH Goerli with `eth_blockNumber` and json RPC","-		Payload: &coreTypes.RelayPayload{
-			Method: ""POST"",
-			Data:   `{""id"": 1, ""jsonrpc"": ""2.0"", method: ""eth_blockNumber""}`,
+		RelayPayload: &coreTypes.Relay_JsonRpcPayload{","--- 

+++ 

@@ -171,6 +171,10 @@

 			name:  ""Relay for accepted service is executed"",
 			relay: testRelay(),
 		},
+		{
+			name:  ""JSONRPC Relay is executed"",
+			relay: testRelay(testEthGoerliRelay()),
+		},
 	}
 
 	for _, testCase := range testCases {
@@ -210,6 +214,19 @@

 func testRelayHeight(height int64) relayEditor {
 	return func(relay *coreTypes.Relay) {
 		relay.Meta.BlockHeight = height
+	}
+}
+
+func testEthGoerliRelay() relayEditor {
+	return func(relay *coreTypes.Relay) {
+		relay.Meta.RelayChain.Id = ""ETH-Goerli""
+		relay.RelayPayload = &coreTypes.Relay_JsonRpcPayload{
+			JsonRpcPayload: &coreTypes.JSONRPCPayload{
+				Id:      []byte(""1""),
+				JsonRpc: ""2.0"",
+				Method:  ""eth_blockNumber"",
+			},
+		}
 	}
 }
 
@@ -226,11 +243,10 @@

 				Id: ""geozone"",
 			},
 		},
-		RelayPayload: &coreTypes.Relay_JsonRpcPayload{
-			JsonRpcPayload: &coreTypes.JsonRpcPayload{
-				Id:      []byte(""1""),
-				JsonRpc: ""2.0"",
-				Method:  ""eth_blockNumber"",
+		RelayPayload: &coreTypes.Relay_RestPayload{
+			RestPayload: &coreTypes.RESTPayload{
+				HttpPath:    ""/v1/height"",
+				RequestType: coreTypes.RESTRequestType_RESTRequestTypeGET,
 			},
 		},
 	}
@@ -248,6 +264,7 @@

 		Address:   testServicer1.Address,
 		Services: map[string]*configs.ServiceConfig{
 			""POKT-UnitTestNet"": testServiceConfig1,
+			""ETH-Goerli"":       testServiceConfig1,
 		},
 	}
 }",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/QuickSwap/interface-v2/pulls/comments/1188377629,https://github.com/QuickSwap/interface-v2/pull/798#discussion_r1188377629,,14,699e03aa920964b9a20f3af9a5be2f7646d61fdb,3a854bec2ebd84162ad6fe1be7b4fbc567dd0682,src/lib/src/swapRouter.ts,nan,remove it," import { Trade } from './trade';
 import { PermitOptions, SelfPermit } from './selfPermit';
 import { MethodParameters, toHex } from './utils/calldata';
-// import { abi } from '@uniswap/v3-periphery/artifacts/contracts/SwapRouter.sol/SwapRouter.json'","--- 

+++ 

@@ -14,8 +14,6 @@

 import abi from 'constants/abis/v3/swap-router.json';
 import { ADDRESS_ZERO } from 'v3lib/utils/v3constants';
 import { encodeRouteToPath } from './utils/encodeRouteToPath';
-
-// import abi from './swapRouterTestABI.json'
 
 export interface FeeOptions {
   /**",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828128411,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1828128411,102,105,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/subject.py,nan,"I suggest changing
 ```
+            {
+                ""strain_id"": animal_resp[""strain_id""],
+                ""strain_name"": animal_resp[""strain_id""],
+            },
```
 to
```
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
```
Revert black

Likewise the following dicts can fit in a single line
https://github.com/SainsburyWellcomeCentre/aeon_mecha/blob/48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b/aeon/dj_pipeline/subject.py#L74-L85","+            {
+                ""strain_id"": animal_resp[""strain_id""],
+                ""strain_name"": animal_resp[""strain_id""],
+            },","--- 

+++ 

@@ -3,7 +3,7 @@

 import json
 import os
 import time
-from datetime import datetime, timedelta, timezone
+from datetime import UTC, datetime, timedelta
 
 import datajoint as dj
 import requests
@@ -67,6 +67,7 @@

             ""o"": 0,
             ""l"": 10,
             ""eartag"": eartag_or_id,
+            ""state"": [""live"", ""sacrificed"", ""exported""],
         }
         animal_resp = get_pyrat_data(endpoint=""animals"", params=params)
         if len(animal_resp) == 0:
@@ -99,10 +100,7 @@

             }
         )
         Strain.insert1(
-            {
-                ""strain_id"": animal_resp[""strain_id""],
-                ""strain_name"": animal_resp[""strain_id""],
-            },
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
             skip_duplicates=True,
         )
         entry = {
@@ -111,13 +109,11 @@

             ""strain_id"": animal_resp[""strain_id""],
             ""cage_number"": animal_resp[""cagenumber""],
             ""lab_id"": animal_resp[""labid""],
+            ""available"": animal_resp.get(""state"", """") == ""live"",
         }
         if animal_resp[""gen_bg_id""] is not None:
             GeneticBackground.insert1(
-                {
-                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
-                    ""gen_bg"": animal_resp[""gen_bg""],
-                },
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
                 skip_duplicates=True,
             )
             entry[""gen_bg_id""] = animal_resp[""gen_bg_id""]
@@ -191,7 +187,7 @@

                 0
             ]
         else:
-            ref_date = datetime.now(timezone.utc).date()
+            ref_date = datetime.now(UTC).date()
 
         weight_query = SubjectWeight & subj_key & f""weight_time < '{ref_date}'""
         ref_weight = (
@@ -201,7 +197,7 @@

         entry = {
             ""subject"": subject_name,
             ""reference_weight"": ref_weight,
-            ""last_updated_time"": datetime.now(timezone.utc),
+            ""last_updated_time"": datetime.now(UTC),
         }
         cls.update1(entry) if cls & {""subject"": subject_name} else cls.insert1(entry)
 
@@ -244,7 +240,7 @@

 
     def _auto_schedule(self):
         """"""Automatically schedule the next task.""""""
-        utc_now = datetime.now(timezone.utc)
+        utc_now = datetime.now(UTC)
 
         next_task_schedule_time = utc_now + timedelta(hours=self.schedule_interval)
         if (
@@ -257,11 +253,14 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the Subject table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         new_eartags = []
         for responsible_id in lab.User.fetch(""responsible_id""):
             # 1 - retrieve all animals from this user
-            animal_resp = get_pyrat_data(endpoint=""animals"", params={""responsible_id"": responsible_id})
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""responsible_id"": responsible_id, ""state"": [""live"", ""sacrificed"", ""exported""]}
+            )
             for animal_entry in animal_resp:
                 # 2 - find animal with comment - Project Aeon
                 eartag_or_id = animal_entry[""eartag_or_id""]
@@ -289,7 +288,7 @@

             new_entry_count += 1
 
         logger.info(f""Inserting {new_entry_count} new subject(s) from Pyrat"")
-        completion_time = datetime.now(timezone.utc)
+        completion_time = datetime.now(UTC)
         self.insert1(
             {
                 **key,
@@ -320,7 +319,7 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the PyratCommentWeightProcedure table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         logger.info(""Extracting weights/comments/procedures"")
 
         eartag_or_id = key[""subject""]
@@ -330,7 +329,7 @@

             if e.args[0].endswith(""response code: 404""):
                 SubjectDetail.update1(
                     {
-                        **key,
+                        ""subject"": key[""subject""],
                         ""available"": False,
                     }
                 )
@@ -359,7 +358,21 @@

             # compute/update reference weight
             SubjectReferenceWeight.get_reference_weight(eartag_or_id)
         finally:
-            completion_time = datetime.now(timezone.utc)
+            # recheck for ""state"" to see if the animal is still available
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""k"": [""labid"", ""state""],
+                        ""eartag"": eartag_or_id,
+                        ""state"": [""live"", ""sacrificed"", ""exported""]})
+            animal_resp = animal_resp[0]
+            SubjectDetail.update1(
+                {
+                    ""subject"": key[""subject""],
+                    ""available"": animal_resp.get(""state"", """") == ""live"",
+                    ""lab_id"": animal_resp[""labid""],
+                }
+            )
+            completion_time = datetime.now(UTC)
             self.insert1(
                 {
                     **key,
@@ -377,7 +390,7 @@

 
     def make(self, key):
         """"""Create one new PyratIngestionTask for every newly added users.""""""
-        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(timezone.utc)})
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(UTC)})
         time.sleep(1)
         self.insert1(key)
 
@@ -447,7 +460,10 @@

 
 
 def get_pyrat_data(endpoint: str, params: dict = None, **kwargs):
-    """"""Get data from PyRat API.""""""
+    """"""Get data from PyRat API.
+
+    See docs at: https://swc.pyrat.cloud/api/v3/docs (production)
+    """"""
     base_url = ""https://swc.pyrat.cloud/api/v3/""
     pyrat_system_token = os.getenv(""PYRAT_SYSTEM_TOKEN"")
     pyrat_user_token = os.getenv(""PYRAT_USER_TOKEN"")",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/Codrux2200/Rtype/pulls/comments/1378933843,https://github.com/Codrux2200/Rtype/pull/33#discussion_r1378933843,,41,38cd97c4b49d39ae3cf6650b7770c140a427880e,524778dd7723bc4edb9e544905d3a6d42fe37c8f,server/ServerCore.cpp,nan,same here," 
-    std::cout << ""At creation : x: "" << enemy->getComponent<PositionComponent>()->x << ""; y: "" << enemy->getComponent<PositionComponent>()->y << std::endl;
-
+    //std::cout << ""At creation : x: "" << enemy->getComponent<PositionComponent>()->x << ""; y: "" << enemy->getComponent<PositionComponent>()->y << std::endl;","--- 

+++ 

@@ -7,6 +7,8 @@

 
 #include ""ServerCore.hpp""
 #include <thread>
+#include ""BossEntity.hpp""
+#include ""BossShootComponent.hpp""
 #include ""CollisionSystem.hpp""
 #include ""EnemyComponent.hpp""
 #include ""EnemyEntity.hpp""
@@ -14,11 +16,14 @@

 #include ""GameSystem.hpp""
 #include ""HitboxComponent.hpp""
 #include ""PlayerBullet.hpp""
+#include ""PlayerComponent.hpp""
 #include ""PlayerEntity.hpp""
 #include ""PlayersPos.hpp""
 #include ""PositionComponent.hpp""
 #include ""Server.hpp""
 #include ""WaveSystem.hpp""
+#include ""VelocityComponent.hpp""
+#include ""BossShootEntity.hpp""
 
 ECS::ServerCore::ServerCore(RType::Server &server) : _server(server)
 {
@@ -35,13 +40,14 @@

 void ECS::ServerCore::_initEntities()
 {
     std::shared_ptr<ECS::Entity> player = std::make_shared<PlayerEntity>();
-    //std::shared_ptr<ECS::Entity> enemy = std::make_shared<EnemyEntity>(0);
     std::shared_ptr<ECS::Entity> playerBullet = std::make_shared<PlayerBullet>(0);
-
-    //std::cout << ""At creation : x: "" << enemy->getComponent<PositionComponent>()->x << ""; y: "" << enemy->getComponent<PositionComponent>()->y << std::endl;
+    std::shared_ptr<ECS::Entity> boss = std::make_shared<BossEntity>([this] { _bossShoot(); }, 0);
+    std::shared_ptr<ECS::Entity> bossBullet = std::make_shared<BossShootEntity>(0, 0, 0);
+
     _entityFactory.registerEntity(player, ""player"");
-    //_entityFactory.registerEntity(enemy, ""entity"" + std::to_string(ECS::Entity::ENEMY_CLASSIC));
     _entityFactory.registerEntity(playerBullet, ""entity"" + std::to_string(ECS::Entity::PLAYER_BULLET));
+    _entityFactory.registerEntity(boss, ""entity"" + std::to_string(ECS::Entity::BOSS));
+    _entityFactory.registerEntity(bossBullet, ""entity"" + std::to_string(ECS::Entity::BOSS_BULLET));
 }
 
 std::shared_ptr<ECS::Scene> ECS::ServerCore::_initMainMenuScene()
@@ -57,10 +63,13 @@

 
     for (int i = 0; i < 4; i++) {
         std::shared_ptr<ECS::Entity> player = _entityFactory.createEntity(""player"", i);
+        std::cout << ""Player "" << player->getId() << "" created"" << std::endl;
         scene->addEntity(player);
     }
     std::shared_ptr<EnemyEntity> enemy = std::make_shared<EnemyEntity>(_entityFactory.ids++);
     scene->addEntity(enemy);
+    std::shared_ptr<ECS::Entity> boss = _entityFactory.createEntity(""entity"" + std::to_string(ECS::Entity::BOSS), _entityFactory.ids++);
+    scene->addEntity(boss);
     return scene;
 }
 
@@ -87,7 +96,7 @@

         }
         _server.sendPackets();
 
-        sceneManager.getCurrentScene()->removeEntitiesToDestroy();
+        sceneManager.getCurrentScene()->removeEntitiesToDestroy(_deltaTime);
 
         waitTime = std::chrono::milliseconds(TICK_TIME_MILLIS - std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::high_resolution_clock::now() - lastFrameTime).count());
         if (waitTime.count() > 0)
@@ -112,8 +121,10 @@

         return;
     auto clientID = _server.clientManager.getClientId(endpoint);
 
-    if (clientID == -1 || clientID > 3)
-        return;
+    if (clientID == -1 || clientID > 3) {
+        std::cerr << ""Invalid client ID: "" << clientID << std::endl;
+        return;
+    }
 
     auto scene = sceneManager.getScene(SceneType::GAME);
 
@@ -123,9 +134,14 @@

         return;
 
     auto playerPositionComponent = playerEntity->getComponent<ECS::PositionComponent>();
-
-    if (playerPositionComponent == nullptr)
-        return;
+    auto playerComponent = playerEntity->getComponent<ECS::PlayerComponent>();
+
+    if (playerPositionComponent == nullptr || playerComponent == nullptr)
+        return;
+
+    if (playerComponent->getLastFire() < playerComponent->fireRate) {
+        return;
+    }
 
     auto playerPosition = playerPositionComponent->getValue();
 
@@ -135,9 +151,12 @@

         return;
 
     auto bulletPosComponent = bulletEntity->getComponent<ECS::PositionComponent>();
-
-    if (bulletPosComponent == nullptr)
-        return;
+    auto velocityComponent = bulletEntity->getComponent<ECS::VelocityComponent>();
+
+    if (bulletPosComponent == nullptr || velocityComponent == nullptr)
+        return;
+
+    playerComponent->resetLastFire();
 
     bulletPosComponent->x = playerPosition[0] + 50;
     bulletPosComponent->y = playerPosition[1] + 50;
@@ -148,6 +167,8 @@

 
     data.x = playerPosition[0];
     data.y = playerPosition[1];
+    data.vx = velocityComponent->vx;
+    data.vy = velocityComponent->vy;
     data.type = ECS::Entity::PLAYER_BULLET;
     data.id = bulletEntity->getId();
 
@@ -163,11 +184,6 @@

 void ECS::ServerCore::_handlerStartGame(Network::Packet &packet, const udp::endpoint &endpoint)
 {
     if (sceneManager.getSceneType() != MAIN_MENU)
-        return;
-    auto client = _server.clientManager.getClientByEndpoint(endpoint);
-    auto leader = _server.clientManager.getLeader();
-
-    if (leader == nullptr || client == nullptr || leader->getEndpoint() != client->getEndpoint())
         return;
 
     sceneManager.setCurrentScene(SceneType::GAME);
@@ -193,12 +209,47 @@

             continue;
         std::vector<int> values = positionComponent->getValue();
 
-        data.x = positionComponent->x;
-        data.y = positionComponent->y;
+        data.x = static_cast<int>(positionComponent->x);
+        data.y = static_cast<int>(positionComponent->y);
         data.type = ECS::Entity::ENEMY_CLASSIC;
         data.id = enemy->getId();
 
         std::cout << ""Sending enemy spawn packet"" << std::endl;
+        std::cout << ""x: "" << positionComponent->x << std::endl;
+        std::cout << ""y: "" << positionComponent->y << std::endl;
+        std::cout << ""type: "" << (int) data.type << std::endl;
+        std::cout << ""id: "" << data.id << std::endl;
+        std::cout << ""---------------------"" << std::endl;
+
+        auto packetToSend = Network::PacketManager::createPacket(Network::ENTITY_SPAWN, &data);
+
+        for (const auto& cli : _server.clientManager.getClients()) {
+            if (cli == nullptr)
+                continue;
+            _server.sendPacketsQueue.emplace_back(cli, *packetToSend);
+        }
+    }
+
+    std::vector<std::shared_ptr<ECS::Entity>> bosses =
+    gameScene->getEntitiesWithComponent<BossComponent>();
+
+    // Sends all present bosses to the players
+    for (auto &boss : bosses) {
+        if (boss == nullptr)
+            continue;
+        Network::data::EntitySpawnData data{};
+        auto positionComponent = boss->getComponent<PositionComponent>();
+
+        if (positionComponent == nullptr)
+            continue;
+        std::vector<int> values = positionComponent->getValue();
+
+        data.x = static_cast<int>(positionComponent->x);
+        data.y = static_cast<int>(positionComponent->y);
+        data.type = ECS::Entity::BOSS;
+        data.id = boss->getId();
+
+        std::cout << ""Sending boss spawn packet"" << std::endl;
         std::cout << ""x: "" << positionComponent->x << std::endl;
         std::cout << ""y: "" << positionComponent->y << std::endl;
         std::cout << ""type: "" << (int) data.type << std::endl;
@@ -328,3 +379,60 @@

 
     _server.broadcastNewLeader();
 }
+
+void ECS::ServerCore::_bossShoot()
+{
+    auto gameScene = sceneManager.getScene(ECS::SceneType::GAME);
+    auto bossEntities = gameScene->getEntitiesWithComponent<ECS::BossComponent>();
+
+    if (bossEntities.empty())
+        return;
+
+    auto bossEntity = bossEntities[0];
+
+    if (bossEntity == nullptr)
+        return;
+
+    auto bossPosComponent = bossEntity->getComponent<ECS::PositionComponent>();
+
+    if (bossPosComponent == nullptr)
+        return;
+
+    // Summon 10 boss bullets in a half circle
+    for (int i = 0; i < 10; i++) {
+        auto bulletEntity = _entityFactory.createEntity(""entity"" + std::to_string(ECS::Entity::BOSS_BULLET), _entityFactory.ids++);
+
+        if (bulletEntity == nullptr)
+            return;
+
+        auto bulletPosComponent = bulletEntity->getComponent<ECS::PositionComponent>();
+        auto bulletComponent = bulletEntity->getComponent<ECS::BossShootComponent>();
+        auto bulletVelocityComponent = bulletEntity->getComponent<ECS::VelocityComponent>();
+
+        if (bulletPosComponent == nullptr || bulletComponent == nullptr || bulletVelocityComponent == nullptr)
+            return;
+
+        bulletPosComponent->x = bossPosComponent->x;
+        bulletPosComponent->y = bossPosComponent->y + 200;
+
+        bulletVelocityComponent->vx = static_cast<float>(-std::cos((i * 18) * M_PI / 180) * 500);
+        bulletVelocityComponent->vy = static_cast<float>(std::sin((i * 18) * M_PI / 180) * 500);
+
+        gameScene->addEntity(bulletEntity);
+
+        Network::data::EntitySpawnData data{};
+
+        data.x = static_cast<int>(bulletPosComponent->x);
+        data.y = static_cast<int>(bulletPosComponent->y);
+        data.vx = bulletVelocityComponent->vx;
+        data.vy = bulletVelocityComponent->vy;
+        data.type = ECS::Entity::BOSS_BULLET;
+        data.id = bulletEntity->getId();
+
+        auto packetToSend = Network::PacketManager::createPacket(Network::ENTITY_SPAWN, &data);
+
+        for (const auto& cli : _server.clientManager.getClients())
+            if (cli != nullptr)
+                _server.sendPacketsQueue.emplace_back(cli, *packetToSend);
+    }
+}",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1228792764,https://github.com/pokt-network/pocket/pull/803#discussion_r1228792764,,30,a98595f48fc016025fce2373b14d8a3829308d2e,208ffbb29eee2f2d603aacf373b1a0fa7d8c9daa,utility/servicer/module_test.go,nan,"How did you generate these?

We have utilities to generate keys that are guaranteed to be compatible.

```go
	operatorKey, err := crypto.GeneratePublicKey()
	if err != nil {
		return nil, err
	}

	outputAddr, err := crypto.GenerateAddress()
	if err != nil {
		return nil, err
	}
```	","+	testServicer1 = &coreTypes.Actor{
+		ActorType: coreTypes.ActorType_ACTOR_TYPE_SERVICER,
+		Address:   ""a3d9ea9d9ad9c58bb96ec41340f83cb2cabb6496"",
+		PublicKey: ""a6cd0a304c38d76271f74dd3c90325144425d904ef1b9a6fbab9b201d75a998b"",","--- 

+++ 

@@ -1,11 +1,12 @@

 package servicer
 
 import (
-	""errors""
 	""fmt""
+	""log""
 	""math/big""
 	""net/http""
 	""net/http/httptest""
+	""os""
 	""testing""
 
 	""github.com/golang/mock/gomock""
@@ -13,6 +14,7 @@

 
 	""github.com/pokt-network/pocket/runtime/configs""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
+	""github.com/pokt-network/pocket/shared/crypto""
 	""github.com/pokt-network/pocket/shared/modules""
 	mockModules ""github.com/pokt-network/pocket/shared/modules/mocks""
 	typesUtil ""github.com/pokt-network/pocket/utility/types""
@@ -23,17 +25,45 @@

 	testCurrentHeight        = int64(9)
 )
 
+// INCOMPLETE(#833) add e2e testing on servicer's features
+
 var (
+	// Initialized in TestMain
+	testServicer1 *coreTypes.Actor
+
+	// Initialized in TestMain
+	testApp1 *coreTypes.Actor
+
+	// Initialized in TestMain
+	testServiceConfig1 *configs.ServiceConfig
+)
+
+// testPublicKey is a helper that returns a public key and its corresponding address
+func testPublicKey() (publicKey, address string) {
+	pk, err := crypto.GeneratePublicKey()
+	if err != nil {
+		log.Fatalf(""Error creating public key: %s"", err)
+	}
+
+	return pk.String(), pk.Address().String()
+}
+
+// TestMain initialized the test fixtures for all the unit tests in the servicer package
+func TestMain(m *testing.M) {
+	servicerPublicKey, servicerAddr := testPublicKey()
 	testServicer1 = &coreTypes.Actor{
-		ActorType: coreTypes.ActorType_ACTOR_TYPE_SERVICER,
-		Address:   ""a3d9ea9d9ad9c58bb96ec41340f83cb2cabb6496"",
-		PublicKey: ""a6cd0a304c38d76271f74dd3c90325144425d904ef1b9a6fbab9b201d75a998b"",
-		Chains:    []string{""0021""},
-	}
-
+		ActorType:    coreTypes.ActorType_ACTOR_TYPE_SERVICER,
+		Address:      servicerAddr,
+		PublicKey:    servicerPublicKey,
+		Chains:       []string{""POKT-UnitTestNet""},
+		StakedAmount: ""1000"",
+	}
+
+	appPublicKey, appAddr := testPublicKey()
 	testApp1 = &coreTypes.Actor{
-		Address:      ""98a792b7aca673620132ef01f50e62caa58eca83"",
-		PublicKey:    ""b5cd0a304c38d76271f74dd3c90325144425d904ef1b9a6fbab9b201d86b009c"",
+		ActorType:    coreTypes.ActorType_ACTOR_TYPE_APP,
+		Address:      appAddr,
+		PublicKey:    appPublicKey,
 		StakedAmount: ""1000"",
 	}
 
@@ -45,9 +75,11 @@

 			Password: ""password1"",
 		},
 	}
-)
-
-func TestAdmitRelay(t *testing.T) {
+
+	os.Exit(m.Run())
+}
+
+func TestRelay_Admit(t *testing.T) {
 	const (
 		currentSessionNumber      = 2
 		testSessionStartingHeight = 8
@@ -74,7 +106,7 @@

 			expected: errValidateRelayMeta,
 		},
 		{
-			name:     ""Relay for unsupported chain is rejected"",
+			name:     ""Relay for unsupported service is rejected"",
 			relay:    testRelay(testRelayChain(""foo"")),
 			expected: errValidateRelayMeta,
 		},
@@ -97,7 +129,7 @@

 			name:              ""Relay for app out of quota is rejected"",
 			relay:             testRelay(),
 			usedSessionTokens: 999999,
-			expected:          errValidateApplication,
+			expected:          errShouldMineRelay,
 		},
 	}
 
@@ -119,27 +151,29 @@

 			require.True(t, ok)
 
 			err = servicer.admitRelay(testCase.relay)
-			if !errors.Is(err, testCase.expected) {
-				t.Fatalf(""Expected error %v got: %v"", testCase.expected, err)
-			}
+			require.ErrorIs(t, err, testCase.expected)
 		})
 	}
 }
 
-func TestExecuteRelay(t *testing.T) {
+func TestRelay_Execute(t *testing.T) {
 	testCases := []struct {
 		name        string
 		relay       *coreTypes.Relay
 		expectedErr error
 	}{
 		{
-			name:        ""relay is rejected if chain is not specified in the config"",
+			name:        ""relay is rejected if service is not specified in the config"",
 			relay:       testRelay(testRelayChain(""foo"")),
 			expectedErr: errValidateRelayMeta,
 		},
 		{
-			name:  ""Relay for accepted chain is executed"",
+			name:  ""Relay for accepted service is executed"",
 			relay: testRelay(),
+		},
+		{
+			name:  ""JSONRPC Relay is executed"",
+			relay: testRelay(testEthGoerliRelay()),
 		},
 	}
 
@@ -151,16 +185,14 @@

 			defer ts.Close()
 
 			config := testServicerConfig()
-			for ch := range config.Services {
-				config.Services[ch].Url = ts.URL
+			for svc := range config.Services {
+				config.Services[svc].Url = ts.URL
 			}
 
 			servicer := &servicer{config: &config}
 			_, err := servicer.executeRelay(testCase.relay)
-			if !errors.Is(err, testCase.expectedErr) {
-				t.Fatalf(""Expected error %v got: %v"", testCase.expectedErr, err)
-			}
-			// INCOMPLETE: verify HTTP request properties: payload/headers/user-agent/etc.
+			require.ErrorIs(t, err, testCase.expectedErr)
+			// INCOMPLETE(@adshmh): verify HTTP request properties: payload/headers/user-agent/etc.
 		})
 	}
 }
@@ -182,6 +214,19 @@

 func testRelayHeight(height int64) relayEditor {
 	return func(relay *coreTypes.Relay) {
 		relay.Meta.BlockHeight = height
+	}
+}
+
+func testEthGoerliRelay() relayEditor {
+	return func(relay *coreTypes.Relay) {
+		relay.Meta.RelayChain.Id = ""ETH-Goerli""
+		relay.RelayPayload = &coreTypes.Relay_JsonRpcPayload{
+			JsonRpcPayload: &coreTypes.JSONRPCPayload{
+				Id:      []byte(""1""),
+				JsonRpc: ""2.0"",
+				Method:  ""eth_blockNumber"",
+			},
+		}
 	}
 }
 
@@ -192,16 +237,16 @@

 			ApplicationAddress: testApp1.Address,
 			BlockHeight:        testCurrentHeight,
 			RelayChain: &coreTypes.Identifiable{
-				Id: ""0021"",
+				Id: ""POKT-UnitTestNet"",
 			},
 			GeoZone: &coreTypes.Identifiable{
 				Id: ""geozone"",
 			},
 		},
-		RelayPayload: &coreTypes.Relay_JsonRpcPayload{
-			JsonRpcPayload: &coreTypes.JsonRpcPayload{
-				Method: ""POST"",
-				Data:   []byte(`{""id"": 1, ""jsonrpc"": ""2.0"", method: ""eth_blockNumber""}`),
+		RelayPayload: &coreTypes.Relay_RestPayload{
+			RestPayload: &coreTypes.RESTPayload{
+				HttpPath:    ""/v1/height"",
+				RequestType: coreTypes.RESTRequestType_RESTRequestTypeGET,
 			},
 		},
 	}
@@ -218,7 +263,8 @@

 		PublicKey: testServicer1.PublicKey,
 		Address:   testServicer1.Address,
 		Services: map[string]*configs.ServiceConfig{
-			""0021"": testServiceConfig1,
+			""POKT-UnitTestNet"": testServiceConfig1,
+			""ETH-Goerli"":       testServiceConfig1,
 		},
 	}
 }
@@ -276,16 +322,15 @@

 		Return(testAppsTokensMultiplier, nil).AnyTimes()
 
 	persistenceLocalContextMock := mockModules.NewMockPersistenceLocalContext(ctrl)
-	persistenceLocalContextMock.EXPECT().StoreServiceRelay(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Return(nil).AnyTimes()
+	persistenceLocalContextMock.EXPECT().StoreServicedRelay(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil).AnyTimes()
 	persistenceLocalContextMock.EXPECT().GetSessionTokensUsed(gomock.Any()).Return(big.NewInt(usedSessionTokens), nil).AnyTimes()
-	persistenceLocalContextMock.EXPECT().Release().Return(nil).AnyTimes()
 
 	persistenceMock := mockModules.NewMockPersistenceModule(ctrl)
 	persistenceMock.EXPECT().GetModuleName().Return(modules.PersistenceModuleName).AnyTimes()
 	persistenceMock.EXPECT().Start().Return(nil).AnyTimes()
 	persistenceMock.EXPECT().SetBus(gomock.Any()).Return().AnyTimes()
 	persistenceMock.EXPECT().NewReadContext(gomock.Any()).Return(persistenceReadContextMock, nil).AnyTimes()
-	persistenceMock.EXPECT().NewLocalContext().Return(persistenceLocalContextMock, nil).AnyTimes()
+	persistenceMock.EXPECT().GetLocalContext().Return(persistenceLocalContextMock, nil).AnyTimes()
 
 	busMock := mockModules.NewMockBus(ctrl)
 	busMock.EXPECT().GetRuntimeMgr().Return(runtimeMgrMock).AnyTimes()",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1205997356,https://github.com/pokt-network/pocket/pull/778#discussion_r1205997356,,255,b93c50312e4967daff683ac9fd577e4e5edc2a3b,a516c51f2d000d25f1469d28e6586ef7875d5a37,app/client/cli/servicer.go,nan,We have a helper in `shared/crypto/sha3.go` you might be able to use. Can you see if it can be leveraged?,"+	return relay, nil
+}
+
+func hash(data []byte) ([]byte, error) {","--- 

+++ 

@@ -2,7 +2,6 @@

 
 import (
 	""context""
-	sha ""crypto""
 	""encoding/hex""
 	""encoding/json""
 	""fmt""
@@ -10,6 +9,7 @@

 
 	""github.com/spf13/cobra""
 
+	""github.com/pokt-network/pocket/app/client/cli/flags""
 	""github.com/pokt-network/pocket/rpc""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 	""github.com/pokt-network/pocket/shared/crypto""
@@ -19,7 +19,6 @@

 	rootCmd.AddCommand(NewServicerCommand())
 }
 
-// TECHDEBT: (unittest) unit test the command: e.g. on number of arguments
 func NewServicerCommand() *cobra.Command {
 	cmd := &cobra.Command{
 		Use:     ""Servicer"",
@@ -45,25 +44,36 @@

 		newUnstakeCmd(cmdDef),
 		newUnpauseCmd(cmdDef),
 		{
-			Use:   ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Short: ""Relay <servicer> <application> <relayChainID> <payload>"",
-			Long: `Sends a trustless relay using <payload> as contents, to the specified active <servicer> in the the <application>'s session.
+			// IMPROVE: allow reading the relay payload from a file with the serialized protobuf via [--input_file]
+			Use:   ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Short: ""Relay <applicationAddrHex> <servicerAddrHex> <relayChainID> <relayPayload>"",
+			Long: `Sends a trustless relay using <relayPayload> as contents, to the specified active <servicerAddrHex> in the the <applicationAddrHex>'s session.
 Will prompt the user for the *application* account passphrase`,
 			Aliases: []string{},
 			Args:    cobra.ExactArgs(4),
 			RunE: func(cmd *cobra.Command, args []string) error {
-				servicerAddr := args[0]
-				applicationAddr := args[1]
+				applicationAddr := args[0]
+				servicerAddr := args[1]
 				chain := args[2]
 				relayPayload := args[3]
 
-				// TODO: (SUGGESTION) refactor to decouple the client logic from the CLI/command
-				pk, err := getPrivateKey(applicationAddr)
+				// REFACTOR: decouple the client logic from the CLI
+				//	The client will: send the trustless relay and return the response (using a single function as entrypoint)
+				//	The CLI will:
+				//		1) extract the required input from the command arguments
+				//		2) call the client function (with the inputs above) that performs the trustless relay
+				pk, err := getPrivateKeyFromKeybase(applicationAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting application's private key: %w"", err)
 				}
 
-				session, servicer, err := fetchServicer(cmd.Context(), applicationAddr, chain, servicerAddr)
+				// TECHDEBT(#791): cache session data
+				session, err := getCurrentSession(cmd.Context(), applicationAddr, chain)
+				if err != nil {
+					return fmt.Errorf(""Error getting current session: %w"", err)
+				}
+
+				servicer, err := validateServicer(session, servicerAddr)
 				if err != nil {
 					return fmt.Errorf(""error getting servicer for the relay: %w"", err)
 				}
@@ -73,7 +83,7 @@

 					return fmt.Errorf(""error building relay from payload: %w"", err)
 				}
 
-				fmt.Printf(""sending trustless relay for %s to %v with payload: %s\n"", applicationAddr, servicer, relayPayload)
+				fmt.Printf(""sending trustless relay for %s to %s with payload: %s\n"", applicationAddr, servicerAddr, relayPayload)
 
 				resp, err := sendTrustlessRelay(cmd.Context(), servicer.ServiceUrl, relay)
 				if err != nil {
@@ -91,59 +101,22 @@

 	return cmds
 }
 
-// TODO: (QUESTION): do we need/want a cli subcommand for fetching servicers?
-
-// fetchServicer returns the servicer specified by the <servicer> argument.
-// It validates the following conditions:
-//
-//	A. The <application> argument is the address of an active application
-//	B. The <servicer> is the address of a servicer that is active in the application's current session.
-//
-// TODO: (SUGGESTION) use a package-internal interface for servicer and application?
-// TODO: (SUGGESTION) use a struct as input to combine all fields (same for output)
-func fetchServicer(ctx context.Context, appAddress, chain, servicerAddress string) (rpc.Session, rpc.ProtocolActor, error) {
-	// TECHDEBT: cache session data
-	session, err := getCurrentSession(ctx, appAddress, chain)
-	if err != nil {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: %w"", err)
-	}
-
-	var (
-		servicer rpc.ProtocolActor
-		found    bool
-	)
-	// TODO: a map may be a better choice for storing servicers
-	for _, s := range session.Servicers {
-		if s.Address == servicerAddress {
-			servicer = s
-			found = true
-			break
+// TODO: add a cli command for fetching sessions
+// validateServicer returns the servicer specified by the <servicer> argument.
+// It validates that the <servicer> is the address of a servicer that is active in the current session.
+func validateServicer(session *rpc.Session, servicerAddress string) (*rpc.ProtocolActor, error) {
+	for i := range session.Servicers {
+		if session.Servicers[i].Address == servicerAddress {
+			return &session.Servicers[i], nil
 		}
 	}
 
-	// TODO: cover with unit tests
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session"", servicerAddress)
-	}
-
-	// TODO: cover with unit tests
-	found = false
-	for _, ch := range servicer.Chains {
-		if ch == chain {
-			found = true
-			break
-		}
-	}
-
-	if !found {
-		return rpc.Session{}, rpc.ProtocolActor{}, fmt.Errorf(""Error getting servicer: service %s does not support chain %s"", servicerAddress, chain)
-	}
-
-	return *session, servicer, nil
+	// ADDTEST: cover with gherkin tests
+	return nil, fmt.Errorf(""Error getting servicer: address %s does not match any servicers in the session %d"", servicerAddress, session.SessionNumber)
 }
 
 func getCurrentSession(ctx context.Context, appAddress, chain string) (*rpc.Session, error) {
-	// TODO: passing 0 as the height value to get the current session seems more optimal than this.
+	// CONSIDERATION: passing 0 as the height value to get the current session seems more optimal than this.
 	currentHeight, err := getCurrentHeight(ctx)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session: %w"", err)
@@ -152,11 +125,11 @@

 	req := rpc.SessionRequest{
 		AppAddress: appAddress,
 		Chain:      chain,
-		// TODO: Geozone
+		// TODO(#697): Geozone
 		SessionHeight: currentHeight,
 	}
 
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session for app/chain/height: %s/%s/%d: %w"", appAddress, chain, currentHeight, err)
 	}
@@ -165,7 +138,8 @@

 	if err != nil {
 		return nil, fmt.Errorf(""Error getting current session with request %v: %w"", req, err)
 	}
-	// TODO: refactor boiler-plate code
+
+	// CLEANUP: move the HTTP response processing code to a separate function to enable reuse.
 	if resp.HTTPResponse.StatusCode != http.StatusOK {
 		return nil, fmt.Errorf(""Error getting current session: Unexpected status code %d for request %v"", resp.HTTPResponse.StatusCode, req)
 	}
@@ -177,9 +151,9 @@

 	return resp.JSON200, nil
 }
 
-// TODO: reuse this function in the query commands
+// REFACTOR: reuse this function in all the query commands
 func getCurrentHeight(ctx context.Context) (int64, error) {
-	client, err := rpc.NewClientWithResponses(remoteCLIURL)
+	client, err := rpc.NewClientWithResponses(flags.RemoteCLIURL)
 	if err != nil {
 		return 0, fmt.Errorf(""Error getting current height: %w"", err)
 	}
@@ -198,51 +172,46 @@

 	return resp.JSON200.Height, nil
 }
 
-// TODO: (localnet) Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
-// TODO: (REFACTOR) should we move package-level variables (e.g. remoteCLIURL) to a cli object?
-func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
+// IMPROVE(#823): [K8s][LocalNet] Publish Servicer(s) Host and Port as env. vars in K8s: similar to Validators
+// CONSIDERATION: move package-level variables (e.g. RemoteCLIURL) to a cli object and consider storing it in the context
+func sendTrustlessRelay(ctx context.Context, servicerUrl string, relay *rpc.RelayRequest) (*rpc.PostV1ClientRelayResponse, error) {
 	client, err := rpc.NewClientWithResponses(servicerUrl)
 	if err != nil {
 		return nil, err
 	}
 
-	return client.PostV1ClientRelayWithResponse(ctx, relay)
-}
-
-// TODO: (NICE) allow reading the relay request from the command line arguments AND from a file
-func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session rpc.Session, servicer rpc.ProtocolActor) (rpc.RelayRequest, error) {
+	return client.PostV1ClientRelayWithResponse(ctx, *relay)
+}
+
+func buildRelay(payload string, appPrivateKey crypto.PrivateKey, session *rpc.Session, servicer *rpc.ProtocolActor) (*rpc.RelayRequest, error) {
 	// TECHDEBT: This is mostly COPIED from pocket-go: we should refactor pocket-go code and import this functionality from there instead.
 	relayPayload := rpc.Payload{
-		Data:   payload,
-		Method: ""POST"",
-		// TODO: Path: load Path field from the corresponding Blockchain (e.g. database)
-		// TODO: set Headers
+		// INCOMPLETE(#803): need to unmarshal into JSONRPC and other supported relay formats once proto-generated custom types are added.
+		Jsonrpc: ""2.0"",
+		Method:  payload,
+		// INCOMPLETE: set Headers for HTTP relays
 	}
 
 	relayMeta := rpc.RelayRequestMeta{
 		BlockHeight: session.SessionHeight,
-		// TODO: use Identifiable for Chain in Session (or string for Chain in Relay Meta)
+		// TODO: Make Chain Identifier type consistent in Session and Meta use Identifiable for Chain in Session (or string for Chain in Relay Meta)
 		Chain: rpc.Identifiable{
 			Id: session.Chain,
 		},
 		ServicerPubKey: servicer.PublicKey,
-		// TODO: Geozone
-		// TODO: Token
-	}
-
-	relay := rpc.RelayRequest{
+		// TODO(#697): Geozone
+	}
+
+	relay := &rpc.RelayRequest{
 		Payload: relayPayload,
 		Meta:    relayMeta,
-		// TODO: (QUESTION) why is there no Proof field in v1 struct?
-	}
+	}
+	// TECHDEBT: Evaluate which fields we should and shouldn't marshall when signing the payload
 	reqBytes, err := json.Marshal(relay)
 	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
-	}
-	hashedReq, err := hash(reqBytes)
-	if err != nil {
-		return rpc.RelayRequest{}, fmt.Errorf(""Error hashing relay request bytes %s: %w"", string(reqBytes), err)
-	}
+		return nil, fmt.Errorf(""Error marshalling relay request %v: %w"", relay, err)
+	}
+	hashedReq := crypto.SHA3Hash(reqBytes)
 	signature, err := appPrivateKey.Sign(hashedReq)
 	if err != nil {
 		return relay, fmt.Errorf(""Error signing relay: %w"", err)
@@ -252,23 +221,14 @@

 	return relay, nil
 }
 
-func hash(data []byte) ([]byte, error) {
-	hasher := sha.SHA3_256.New()
-	if _, err := hasher.Write(data); err != nil {
-		return nil, fmt.Errorf(""Error hashing data: %w"", err)
-	}
-
-	return hasher.Sum(nil), nil
-}
-
-// TODO: remove use of package-level variables
-func getPrivateKey(address string) (crypto.PrivateKey, error) {
+// TECHDEBT: remove use of package-level variables, such as NonInteractive, RemoteCLIURL, pwd, etc.
+func getPrivateKeyFromKeybase(address string) (crypto.PrivateKey, error) {
 	kb, err := keybaseForCLI()
 	if err != nil {
 		return nil, err
 	}
 
-	if !nonInteractive {
+	if !flags.NonInteractive {
 		pwd = readPassphrase(pwd)
 	}
 ",,,,
coderabbitai/ai-pr-reviewer,https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829515344,https://github.com/SainsburyWellcomeCentre/aeon_mecha/pull/443#discussion_r1829515344,,105,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,pyproject.toml,nan,Remove this as project requires python >= 3.11 we can use the `datetime.UTC` alias,"+  ""PLR0912"", 
   ""PLR0913"",
   ""PLR0915"",
+  ""UP017""  # skip `datetime.UTC` alias","--- 

+++ 

@@ -102,7 +102,6 @@

   ""PLR0912"", 
   ""PLR0913"",
   ""PLR0915"",
-  ""UP017""  # skip `datetime.UTC` alias
 ]
 extend-exclude = [
   "".git"",",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1251331098,https://github.com/pokt-network/pocket/pull/732#discussion_r1251331098,,277,4602283cbd676952b2a5c20f73758305cc833ce5,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/background/router.go,nan,"I suggest changing
 ```
+func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {
```
 to
```
+// setupPubsub sets up a new gossip sub topic using libp2p
+func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {
```","+	return err
+}
+
+func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {","--- 

+++ 

@@ -49,6 +49,7 @@

 	host libp2pHost.Host
 	// cancelReadSubscription is the cancel function for the context which is
 	// monitored in the `#readSubscription()` go routine. Call to terminate it.
+	// only one read subscription exists per router at any point in time
 	cancelReadSubscription context.CancelFunc
 
 	// Fields below are assigned during creation via `#setupDependencies()`.
@@ -215,6 +216,7 @@

 }
 
 func (rtr *backgroundRouter) setupDependencies(ctx context.Context, cfg *config.BackgroundConfig) error {
+	// NB: The order in which the internal components are setup below is important
 	if err := rtr.setupUnicastRouter(); err != nil {
 		return err
 	}
@@ -237,7 +239,9 @@

 
 	if err := rtr.setupPeerstore(
 		ctx,
-		cfg.PeerstoreProvider, cfg.CurrentHeightProvider); err != nil {
+		cfg.PeerstoreProvider,
+		cfg.CurrentHeightProvider,
+	); err != nil {
 		return fmt.Errorf(""setting up peerstore: %w"", err)
 	}
 	return nil
@@ -263,6 +267,7 @@

 	return nil
 }
 
+// setupPeerDiscovery sets up the Kademlia Distributed Hash Table (DHT)
 func (rtr *backgroundRouter) setupPeerDiscovery(ctx context.Context) (err error) {
 	dhtMode := dht.ModeAutoServer
 	// NB: don't act as a bootstrap node in peer discovery in client debug mode
@@ -274,6 +279,7 @@

 	return err
 }
 
+// setupPubsub sets up a new gossip sub topic using libp2p
 func (rtr *backgroundRouter) setupPubsub(ctx context.Context) (err error) {
 	// TECHDEBT(#730): integrate libp2p tracing via `pubsub.WithEventTracer()`.
 
@@ -313,8 +319,8 @@

 }
 
 func (rtr *backgroundRouter) bootstrap(ctx context.Context) error {
-	// CONSIDERATION: add `GetPeers` method to `PeerstoreProvider` interface
-	// to avoid this loop.
+	// CONSIDERATION: add `GetPeers` method, which returns a map,
+	// to the `PeerstoreProvider` interface to simplify this loop.
 	for _, peer := range rtr.pstore.GetPeerList() {
 		if err := utils.AddPeerToLibp2pHost(rtr.host, peer); err != nil {
 			return err
@@ -352,22 +358,26 @@

 func (rtr *backgroundRouter) topicValidator(_ context.Context, _ libp2pPeer.ID, msg *pubsub.Message) bool {
 	var backgroundMsg typesP2P.BackgroundMessage
 	if err := proto.Unmarshal(msg.Data, &backgroundMsg); err != nil {
+		rtr.logger.Error().Err(err).Msg(""unmarshalling Background message"")
 		return false
 	}
 
 	if backgroundMsg.Data == nil {
+		rtr.logger.Debug().Msg(""no data in Background message"")
 		return false
 	}
 
-	networkMessage := messaging.PocketEnvelope{}
-	if err := proto.Unmarshal(backgroundMsg.Data, &networkMessage); err != nil {
-		rtr.logger.Error().Err(err).Msg(""Error decoding network message"")
+	poktEnvelope := messaging.PocketEnvelope{}
+	if err := proto.Unmarshal(backgroundMsg.Data, &poktEnvelope); err != nil {
+		rtr.logger.Error().Err(err).Msg(""Error decoding Background message"")
 		return false
 	}
 
 	return true
 }
 
+// readSubscription is a while loop for receiving and handling messages from the
+// subscription. It is intended to be called as a goroutine.
 func (rtr *backgroundRouter) readSubscription(ctx context.Context) {
 	for {
 		if err := ctx.Err(); err != nil {",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1243081568,https://github.com/pokt-network/pocket/pull/732#discussion_r1243081568,,222,8467f3a832ae26b48ed9ec481a8b99355c849c09,6b875eee002f49c4b1abe5ac9faa61f59daaa4cf,p2p/utils_test.go,nan,"Have you ever looked at `waitForEventsInternal` in consensus?

Don't think we need to change anything here, but could be an alternative approach in the future. Maybe just update the TODO you have below?"," 	mockModulesRegistry.EXPECT().GetModule(peerstore_provider.ModuleName).Return(nil, runtime.ErrModuleNotRegistered(peerstore_provider.ModuleName)).AnyTimes()
 	mockModulesRegistry.EXPECT().GetModule(current_height_provider.ModuleName).Return(nil, runtime.ErrModuleNotRegistered(current_height_provider.ModuleName)).AnyTimes()
 	mockBus.EXPECT().GetModulesRegistry().Return(mockModulesRegistry).AnyTimes()
+	mockBus.EXPECT().PublishEventToBus(gomock.AssignableToTypeOf(&messaging.PocketEnvelope{})).","--- 

+++ 

@@ -110,9 +110,9 @@

 	peerIDs := setupMockNetPeers(t, netMock, len(busMocks))
 
 	ctrl := gomock.NewController(t)
-	backgroundRouterMock := mock_types.NewMockRouter(ctrl)
-	backgroundRouterMock.EXPECT().Broadcast(gomock.Any()).Times(1)
-	backgroundRouterMock.EXPECT().Close().Times(len(busMocks))
+	noopBackgroundRouterMock := mock_types.NewMockRouter(ctrl)
+	noopBackgroundRouterMock.EXPECT().Broadcast(gomock.Any()).Times(1)
+	noopBackgroundRouterMock.EXPECT().Close().Times(len(busMocks))
 
 	p2pModules = make(map[string]*p2pModule, len(busMocks))
 	for i := range busMocks {
@@ -120,8 +120,8 @@

 		p2pMod, err := Create(
 			busMocks[i],
 			WithHost(host),
-			// mock background router to prevent background message propagation.
-			WithUnstakedActorRouter(backgroundRouterMock),
+			// mock background router to prevent & ignore background message propagation.
+			WithUnstakedActorRouter(noopBackgroundRouterMock),
 		)
 		require.NoError(t, err)
 		p2pModules[validatorId(i+1)] = p2pMod.(*p2pModule)
@@ -226,7 +226,7 @@

 			if readWriteWaitGroup != nil {
 				readWriteWaitGroup.Done()
 			}
-		}).AnyTimes() // TODO: specific times
+		}).AnyTimes() // TECHDEBT: assert number of times. Consider `waitForEventsInternal` or similar as in consensus.
 	mockBus.EXPECT().PublishEventToBus(gomock.Any()).AnyTimes()
 	return mockBus
 }
@@ -341,7 +341,7 @@

 	ctrl := gomock.NewController(t)
 	eventMetricsAgentMock := mockModules.NewMockEventMetricsAgent(ctrl)
 
-	// DISCUSS_THIS_COMMIT: The number of times each telemetry event is expected
+	// TECHDEBT(#886): The number of times each telemetry event is expected
 	// (below) is dependent on the number of redundant messages all validators see,
 	// which is a function of the network size. Until this function is derived and
 	// implemented, we cannot predict the number of times each event is expected.",,,,
anc95/ChatGPT-CodeReview,https://api.github.com/repos/pokt-network/pocket/pulls/comments/1209947205,https://github.com/pokt-network/pocket/pull/771#discussion_r1209947205,,42,eda74276723ac0b0ff412bed31e9802e09bf6a51,10a931a06f81902518a919ce33b9642f8388949c,utility/transaction.go,nan,I think it was a by-product of not being able to use the hash of the proto bytes that I originally used the hydrate function - now with the `TxIndexer` working on the same hash for both proto and indexed txs what is stopping us from using the txIndexer alone for this call?," }
+
+// GetIndexedTransaction implements the exposed functionality of the shared utilityModule interface.
+func (u *utilityModule) GetIndexedTransaction(txProtoBytes []byte) (*coreTypes.IndexedTransaction, error) {","--- 

+++ 

@@ -2,7 +2,9 @@

 
 import (
 	""encoding/hex""
+	""errors""
 
+	""github.com/dgraph-io/badger/v3""
 	""github.com/pokt-network/pocket/shared/codec""
 	coreTypes ""github.com/pokt-network/pocket/shared/core/types""
 )
@@ -41,15 +43,6 @@

 // GetIndexedTransaction implements the exposed functionality of the shared utilityModule interface.
 func (u *utilityModule) GetIndexedTransaction(txProtoBytes []byte) (*coreTypes.IndexedTransaction, error) {
 	txHash := coreTypes.TxHash(txProtoBytes)
-	persistenceModule := u.GetBus().GetPersistenceModule()
-
-	txExists, err := persistenceModule.TransactionExists(txHash)
-	if err != nil {
-		return nil, err
-	}
-	if !txExists {
-		return nil, coreTypes.ErrTransactionNotCommitted()
-	}
 
 	// TECHDEBT: Note the inconsistency between referencing tx hash as a string vs. byte slice in different places. Need to pick
 	// one and consolidate throughout the codebase
@@ -57,8 +50,11 @@

 	if err != nil {
 		return nil, err
 	}
-	idTx, err := persistenceModule.GetTxIndexer().GetByHash(hash)
+	idTx, err := u.GetBus().GetPersistenceModule().GetTxIndexer().GetByHash(hash)
 	if err != nil {
+		if errors.Is(err, badger.ErrKeyNotFound) {
+			return nil, coreTypes.ErrTransactionNotCommitted()
+		}
 		return nil, err
 	}
 ",,,,
